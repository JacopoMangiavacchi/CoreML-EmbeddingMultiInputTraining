{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "// %install-swiftpm-flags -c release\n",
    "// %install '.package(url: \"https://github.com/JacopoMangiavacchi/SwiftCoreMLTools.git\", from: \"0.0.5\")' SwiftCoreMLTools\n",
    "// %install '.package(url: \"https://github.com/dduan/Just.git\", from: \"0.8.0\")' Just"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TO4FT9bUohHx"
   },
   "outputs": [],
   "source": [
    "import Foundation\n",
    "import TensorFlow\n",
    "// import SwiftCoreMLTools\n",
    "// import Just"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Download\n",
    "\n",
    "Boston house prices dataset\n",
    "---------------------------\n",
    "\n",
    "**Data Set Characteristics:**  \n",
    "\n",
    "    :Number of Instances: 506 \n",
    "\n",
    "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
    "\n",
    "    :Attribute Information (in order):\n",
    "        - CRIM     per capita crime rate by town\n",
    "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "        - INDUS    proportion of non-retail business acres per town\n",
    "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "        - NOX      nitric oxides concentration (parts per 10 million)\n",
    "        - RM       average number of rooms per dwelling\n",
    "        - AGE      proportion of owner-occupied units built prior to 1940\n",
    "        - DIS      weighted distances to five Boston employment centres\n",
    "        - RAD      index of accessibility to radial highways\n",
    "        - TAX      full-value property-tax rate per ten thousand dollars\n",
    "        - PTRATIO  pupil-teacher ratio by town\n",
    "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "        - LSTAT    % lower status of the population\n",
    "        - MEDV     Median value of owner-occupied homes in a thousand dollar\n",
    "\n",
    "    :Missing Attribute Values: None\n",
    "\n",
    "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
    "\n",
    "This is a copy of UCI ML housing dataset.\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// if let cts = Just.get(URL(string: \"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\")!).content {\n",
    "//     try! cts.write(to: URL(fileURLWithPath:\"../data/housing.csv\"))\n",
    "// }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "let data = try String(contentsOfFile:\"./data/housing.csv\", encoding: String.Encoding.utf8)\n",
    "let dataRecords: [[Float]] = data.split(separator: \"\\n\").map{ String($0).split(separator: \" \").compactMap{ Float(String($0)) } }\n",
    "\n",
    "let numRecords = dataRecords.count\n",
    "let numColumns = dataRecords[0].count\n",
    "\n",
    "let dataFeatures = dataRecords.map{ Array($0[0..<numColumns-1]) }\n",
    "let dataLabels = dataRecords.map{ Array($0[(numColumns-1)...]) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Numerical Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "let categoricalColumns = [3, 8]\n",
    "let numericalColumns = [0, 1, 2, 4, 5, 6, 7, 9, 10, 11, 12]\n",
    "let numCategoricalFeatures = categoricalColumns.count\n",
    "let numNumericalFeatures = numericalColumns.count\n",
    "let numLabels = 1\n",
    "\n",
    "assert(numColumns == numCategoricalFeatures + numNumericalFeatures + 1)\n",
    "\n",
    "// Get Categorical Features\n",
    "let allCategoriesValues = dataFeatures.map{ row in categoricalColumns.map{ Int32(row[$0]) } }\n",
    "                                .reduce(into: Array(repeating: [Int32](), count: 2)){ total, value in\n",
    "                                    total[0].append(value[0])\n",
    "                                    total[1].append(value[1]) }\n",
    "                                .map{ Set($0).sorted() }\n",
    "\n",
    "let embeddingSizes = allCategoriesValues.map{ $0.count }\n",
    "\n",
    "let categoricalFeatures = dataFeatures.map{ row in categoricalColumns.map{ Int32(row[$0]) } }\n",
    "let oneHotCategoricalFeatures:[[[Int32]]] = categoricalFeatures.map{ catArray in\n",
    "    var oneHotArray = [[Int32]]()\n",
    "    \n",
    "    for i in 0..<catArray.count {\n",
    "        var oneHot = Array(repeating: Int32(0), count: allCategoriesValues[i].count)\n",
    "        if let pos = allCategoriesValues[i].firstIndex(where: { $0 == catArray[i] }){\n",
    "            oneHot[pos] = 1\n",
    "        }\n",
    "        oneHotArray.append(oneHot)\n",
    "    }\n",
    "    \n",
    "    return oneHotArray\n",
    "}\n",
    "\n",
    "// Get Numerical Features\n",
    "let numericalFeatures = dataFeatures.map{ row in numericalColumns.map{ row[$0] } }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "let trainPercentage:Float = 0.8\n",
    "let numTrainRecords = Int(ceil(Float(numRecords) * trainPercentage))\n",
    "let numTestRecords = numRecords - numTrainRecords\n",
    "\n",
    "func matrixTranspose<T>(_ matrix: [[T]]) -> [[T]] {\n",
    "    if matrix.isEmpty {return matrix}\n",
    "    var result = [[T]]()\n",
    "    for index in 0..<matrix.first!.count {\n",
    "        result.append(matrix.map{$0[index]})\n",
    "    }\n",
    "    return result\n",
    "}\n",
    "\n",
    "let xCategoricalAllTrain = matrixTranspose(Array(oneHotCategoricalFeatures[0..<numTrainRecords])).map{ Array($0.joined()) }\n",
    "let xCategoricalAllTest = matrixTranspose(Array(oneHotCategoricalFeatures[numTrainRecords...])).map{ Array($0.joined()) }\n",
    "let xNumericalAllTrain = Array(Array(numericalFeatures[0..<numTrainRecords]).joined())\n",
    "let xNumericalAllTest = Array(Array(numericalFeatures[numTrainRecords...]).joined())\n",
    "let yAllTrain = Array(Array(dataLabels[0..<numTrainRecords]).joined())\n",
    "let yAllTest = Array(Array(dataLabels[numTrainRecords...]).joined())\n",
    "\n",
    "let XCategoricalTrain = xCategoricalAllTrain.enumerated().map{ (offset, element) in \n",
    "    Tensor<Int32>(element).reshaped(to: TensorShape([numTrainRecords, embeddingSizes[offset]]))\n",
    "}\n",
    "let XCategoricalTest = xCategoricalAllTest.enumerated().map{ (offset, element) in \n",
    "    Tensor<Int32>(element).reshaped(to: TensorShape([numTestRecords, embeddingSizes[offset]]))\n",
    "}\n",
    "let XNumericalTrainDeNorm = Tensor<Float>(xNumericalAllTrain).reshaped(to: TensorShape([numTrainRecords, numNumericalFeatures]))\n",
    "let XNumericalTestDeNorm = Tensor<Float>(xNumericalAllTest).reshaped(to: TensorShape([numTestRecords, numNumericalFeatures]))\n",
    "let YTrain = Tensor<Float>(yAllTrain).reshaped(to: TensorShape([numTrainRecords, numLabels]))\n",
    "let YTest = Tensor<Float>(yAllTest).reshaped(to: TensorShape([numTestRecords, numLabels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.0137098,  14.197531,   9.523555, 0.53213036,  6.3311296,   64.47929,  4.1678762,  353.68396,\r\n",
      "    18.03163,  379.84735,  11.394517]] [[ 6.5076075,  25.258776,   6.534038, 0.11449408,  0.7311985,  29.000755,  2.1797554,  132.14561,\r\n",
      "    2.217345,  40.494495,   6.852825]]\r\n"
     ]
    }
   ],
   "source": [
    "let mean = XNumericalTrainDeNorm.mean(alongAxes: 0)\n",
    "let std = XNumericalTrainDeNorm.standardDeviation(alongAxes: 0)\n",
    "\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "let XNumericalTrain = (XNumericalTrainDeNorm - mean)/std\n",
    "let XNumericalTest = (XNumericalTestDeNorm - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shapes [405, 11] [405, 2] [405, 9] [405, 1]\r\n",
      "Testing shapes  [101, 11] [101, 2] [101, 9] [101, 1]\r\n"
     ]
    }
   ],
   "source": [
    "print(\"Training shapes \\(XNumericalTrain.shape) \\(XCategoricalTrain[0].shape) \\(XCategoricalTrain[1].shape) \\(YTrain.shape)\")\n",
    "print(\"Testing shapes  \\(XNumericalTest.shape) \\(XCategoricalTest[0].shape) \\(XCategoricalTest[1].shape) \\(YTest.shape)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HDJZCCgqohIC"
   },
   "outputs": [],
   "source": [
    "struct MultiInputs<N: Differentiable, C>: Differentiable {\n",
    "  var numerical: N\n",
    "  \n",
    "  @noDerivative\n",
    "  var categorical: C\n",
    "\n",
    "  @differentiable\n",
    "  init(numerical: N, categorical: C) {\n",
    "    self.numerical = numerical\n",
    "    self.categorical = categorical\n",
    "  }\n",
    "}\n",
    "\n",
    "struct RegressionModel: Module {\n",
    "//     var numericalLayer = Dense<Float>(inputSize: 11, outputSize: 32, activation: relu)\n",
    "    var embedding1 = Embedding<Float>(vocabularySize: 2, embeddingSize: 2)\n",
    "    var embedding2 = Embedding<Float>(vocabularySize: 9, embeddingSize: 5)\n",
    "//     var embeddingLayer = Dense<Float>(inputSize: (4 + 45), outputSize: 64, activation: relu)\n",
    "//     var allInputConcatLayer = Dense<Float>(inputSize: (32 + 64), outputSize: 128, activation: relu)\n",
    "    var allInputConcatLayer = Dense<Float>(inputSize: (11 + 4 + 45), outputSize: 128, activation: relu)\n",
    "    var hiddenLayer = Dense<Float>(inputSize: 128, outputSize: 32, activation: relu)\n",
    "    var outputLayer = Dense<Float>(inputSize: 32, outputSize: 1)\n",
    "    \n",
    "    @differentiable\n",
    "    func callAsFunction(_ input: MultiInputs<[Tensor<Float>], [Tensor<Int32>]>) -> Tensor<Float> {\n",
    "//         let numericalInput = numericalLayer(input.numerical[0])\n",
    "        let embeddingOutput1 = embedding1(input.categorical[0])\n",
    "        let embeddingOutput1Reshaped = embeddingOutput1.reshaped(to: \n",
    "            TensorShape([embeddingOutput1.shape[0], embeddingOutput1.shape[1] * embeddingOutput1.shape[2]]))\n",
    "        let embeddingOutput2 = embedding2(input.categorical[1])\n",
    "        let embeddingOutput2Reshaped = embeddingOutput2.reshaped(to: \n",
    "            TensorShape([embeddingOutput2.shape[0], embeddingOutput2.shape[1] * embeddingOutput2.shape[2]]))\n",
    "//         let embeddingConcat = Tensor<Float>(concatenating: [embeddingOutput1Reshaped, embeddingOutput2Reshaped], alongAxis: 1)\n",
    "//         let embeddingInput = embeddingLayer(embeddingConcat)\n",
    "//         let allConcat = Tensor<Float>(concatenating: [numericalInput, embeddingInput], alongAxis: 1)\n",
    "        let allConcat = Tensor<Float>(concatenating: [input.numerical[0], embeddingOutput1Reshaped, embeddingOutput2Reshaped], alongAxis: 1)\n",
    "        return allConcat.sequenced(through: allInputConcatLayer, hiddenLayer, outputLayer)\n",
    "    }\n",
    "}\n",
    "\n",
    "var model = RegressionModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JK0Vj7bSohIF"
   },
   "outputs": [],
   "source": [
    "let optimizer = RMSProp(for: model, learningRate: 0.001)\n",
    "Context.local.learningPhase = .training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gY8C7yHJohIH"
   },
   "outputs": [],
   "source": [
    "let epochCount = 500\n",
    "let batchSize = 32\n",
    "let numberOfBatch = Int(ceil(Double(numTrainRecords) / Double(batchSize)))\n",
    "let shuffle = true\n",
    "\n",
    "func mae(predictions: Tensor<Float>, truths: Tensor<Float>) -> Float {\n",
    "    return abs(Tensor<Float>(predictions - truths)).mean().scalarized()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "L9bU9HsdohIK",
    "outputId": "692b81c5-3286-4e7c-9246-eb56e6a3eaee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: MSE: 502.0456, MAE: 19.801373\n",
      "Epoch 2: MSE: 234.11732, MAE: 11.567073\n",
      "Epoch 3: MSE: 92.90376, MAE: 6.2636995\n",
      "Epoch 4: MSE: 65.79809, MAE: 5.104972\n",
      "Epoch 5: MSE: 55.88944, MAE: 4.8351364\n",
      "Epoch 6: MSE: 51.318428, MAE: 4.2889614\n",
      "Epoch 7: MSE: 45.5352, MAE: 4.097953\n",
      "Epoch 8: MSE: 40.751522, MAE: 3.595095\n",
      "Epoch 9: MSE: 38.35515, MAE: 3.5714211\n",
      "Epoch 10: MSE: 34.48596, MAE: 3.397207\n",
      "Epoch 11: MSE: 29.352333, MAE: 3.2578378\n",
      "Epoch 12: MSE: 29.699827, MAE: 3.0485804\n",
      "Epoch 13: MSE: 27.916904, MAE: 3.0622892\n",
      "Epoch 14: MSE: 28.149239, MAE: 3.0177972\n",
      "Epoch 15: MSE: 25.466373, MAE: 2.9360785\n",
      "Epoch 16: MSE: 25.76882, MAE: 2.9714775\n",
      "Epoch 17: MSE: 23.245653, MAE: 2.8534093\n",
      "Epoch 18: MSE: 25.04813, MAE: 2.8597603\n",
      "Epoch 19: MSE: 24.774015, MAE: 2.858098\n",
      "Epoch 20: MSE: 23.56269, MAE: 2.854866\n",
      "Epoch 21: MSE: 23.217655, MAE: 2.803007\n",
      "Epoch 22: MSE: 22.488848, MAE: 2.8177464\n",
      "Epoch 23: MSE: 22.359364, MAE: 2.7257943\n",
      "Epoch 24: MSE: 22.495338, MAE: 2.7968454\n",
      "Epoch 25: MSE: 22.431416, MAE: 2.7096899\n",
      "Epoch 26: MSE: 22.082245, MAE: 2.703756\n",
      "Epoch 27: MSE: 22.533394, MAE: 2.663689\n",
      "Epoch 28: MSE: 22.267363, MAE: 2.6973813\n",
      "Epoch 29: MSE: 21.233036, MAE: 2.7037175\n",
      "Epoch 30: MSE: 22.042057, MAE: 2.6610322\n",
      "Epoch 31: MSE: 19.718248, MAE: 2.6687188\n",
      "Epoch 32: MSE: 20.194899, MAE: 2.612185\n",
      "Epoch 33: MSE: 20.131521, MAE: 2.571679\n",
      "Epoch 34: MSE: 20.602322, MAE: 2.5842779\n",
      "Epoch 35: MSE: 20.473192, MAE: 2.5569818\n",
      "Epoch 36: MSE: 19.435446, MAE: 2.5553818\n",
      "Epoch 37: MSE: 19.413738, MAE: 2.5338695\n",
      "Epoch 38: MSE: 19.871555, MAE: 2.4789968\n",
      "Epoch 39: MSE: 18.0521, MAE: 2.492863\n",
      "Epoch 40: MSE: 19.051914, MAE: 2.4728084\n",
      "Epoch 41: MSE: 19.445827, MAE: 2.4394343\n",
      "Epoch 42: MSE: 18.352495, MAE: 2.4605474\n",
      "Epoch 43: MSE: 18.763582, MAE: 2.461745\n",
      "Epoch 44: MSE: 17.28063, MAE: 2.3837152\n",
      "Epoch 45: MSE: 18.24482, MAE: 2.4163694\n",
      "Epoch 46: MSE: 17.894432, MAE: 2.3456933\n",
      "Epoch 47: MSE: 17.458174, MAE: 2.3567717\n",
      "Epoch 48: MSE: 17.410143, MAE: 2.343374\n",
      "Epoch 49: MSE: 16.606752, MAE: 2.380531\n",
      "Epoch 50: MSE: 16.405771, MAE: 2.291466\n",
      "Epoch 51: MSE: 17.188679, MAE: 2.3177824\n",
      "Epoch 52: MSE: 16.293745, MAE: 2.304129\n",
      "Epoch 53: MSE: 16.261166, MAE: 2.2682707\n",
      "Epoch 54: MSE: 16.130255, MAE: 2.2720308\n",
      "Epoch 55: MSE: 15.827086, MAE: 2.2574255\n",
      "Epoch 56: MSE: 15.41895, MAE: 2.2121546\n",
      "Epoch 57: MSE: 16.134617, MAE: 2.2450356\n",
      "Epoch 58: MSE: 15.063269, MAE: 2.2339094\n",
      "Epoch 59: MSE: 15.089553, MAE: 2.237606\n",
      "Epoch 60: MSE: 14.626784, MAE: 2.1974852\n",
      "Epoch 61: MSE: 15.117102, MAE: 2.1786497\n",
      "Epoch 62: MSE: 14.803477, MAE: 2.1265996\n",
      "Epoch 63: MSE: 13.677891, MAE: 2.154013\n",
      "Epoch 64: MSE: 14.804524, MAE: 2.106738\n",
      "Epoch 65: MSE: 14.659659, MAE: 2.1345232\n",
      "Epoch 66: MSE: 14.247143, MAE: 2.1202881\n",
      "Epoch 67: MSE: 14.548192, MAE: 2.1004965\n",
      "Epoch 68: MSE: 13.688523, MAE: 2.055556\n",
      "Epoch 69: MSE: 12.684475, MAE: 2.0805666\n",
      "Epoch 70: MSE: 13.406368, MAE: 2.1178327\n",
      "Epoch 71: MSE: 12.753516, MAE: 2.0477808\n",
      "Epoch 72: MSE: 13.640346, MAE: 2.0445929\n",
      "Epoch 73: MSE: 13.610659, MAE: 2.0737777\n",
      "Epoch 74: MSE: 12.534662, MAE: 2.016878\n",
      "Epoch 75: MSE: 12.254043, MAE: 2.0097368\n",
      "Epoch 76: MSE: 12.37181, MAE: 2.0096617\n",
      "Epoch 77: MSE: 13.623526, MAE: 2.0207307\n",
      "Epoch 78: MSE: 12.468953, MAE: 1.9862273\n",
      "Epoch 79: MSE: 11.162929, MAE: 1.9780123\n",
      "Epoch 80: MSE: 12.571235, MAE: 1.9327893\n",
      "Epoch 81: MSE: 12.412754, MAE: 1.959055\n",
      "Epoch 82: MSE: 11.838932, MAE: 1.9517057\n",
      "Epoch 83: MSE: 11.151488, MAE: 1.9263055\n",
      "Epoch 84: MSE: 11.233821, MAE: 1.9799832\n",
      "Epoch 85: MSE: 11.777454, MAE: 1.9231198\n",
      "Epoch 86: MSE: 11.409102, MAE: 1.939773\n",
      "Epoch 87: MSE: 10.758793, MAE: 1.887246\n",
      "Epoch 88: MSE: 10.753641, MAE: 1.9106038\n",
      "Epoch 89: MSE: 11.053972, MAE: 1.9067107\n",
      "Epoch 90: MSE: 11.568547, MAE: 1.9351757\n",
      "Epoch 91: MSE: 11.091593, MAE: 1.9102908\n",
      "Epoch 92: MSE: 11.111501, MAE: 1.9118642\n",
      "Epoch 93: MSE: 11.0555935, MAE: 1.8706216\n",
      "Epoch 94: MSE: 10.224677, MAE: 1.9188493\n",
      "Epoch 95: MSE: 10.204324, MAE: 1.8815498\n",
      "Epoch 96: MSE: 9.788577, MAE: 1.8983524\n",
      "Epoch 97: MSE: 10.66099, MAE: 1.8802255\n",
      "Epoch 98: MSE: 10.888719, MAE: 1.8690375\n",
      "Epoch 99: MSE: 9.688292, MAE: 1.8790358\n",
      "Epoch 100: MSE: 10.177305, MAE: 1.9587097\n",
      "Epoch 101: MSE: 9.050793, MAE: 1.8779981\n",
      "Epoch 102: MSE: 11.153925, MAE: 1.8418151\n",
      "Epoch 103: MSE: 9.344169, MAE: 1.8214471\n",
      "Epoch 104: MSE: 9.669353, MAE: 1.8238828\n",
      "Epoch 105: MSE: 10.590944, MAE: 1.8643112\n",
      "Epoch 106: MSE: 9.174935, MAE: 1.814603\n",
      "Epoch 107: MSE: 9.625037, MAE: 1.9176756\n",
      "Epoch 108: MSE: 9.964645, MAE: 1.8078402\n",
      "Epoch 109: MSE: 8.9984, MAE: 1.8179892\n",
      "Epoch 110: MSE: 9.031031, MAE: 1.8590785\n",
      "Epoch 111: MSE: 9.630301, MAE: 1.8542747\n",
      "Epoch 112: MSE: 8.282644, MAE: 1.8068893\n",
      "Epoch 113: MSE: 9.64012, MAE: 1.8585428\n",
      "Epoch 114: MSE: 9.413563, MAE: 1.7936227\n",
      "Epoch 115: MSE: 8.6665945, MAE: 1.7573842\n",
      "Epoch 116: MSE: 9.082226, MAE: 1.8620331\n",
      "Epoch 117: MSE: 8.185856, MAE: 1.7596196\n",
      "Epoch 118: MSE: 8.876541, MAE: 1.7849994\n",
      "Epoch 119: MSE: 8.520693, MAE: 1.8157098\n",
      "Epoch 120: MSE: 8.501909, MAE: 1.791685\n",
      "Epoch 121: MSE: 8.349201, MAE: 1.8270713\n",
      "Epoch 122: MSE: 8.176417, MAE: 1.8016274\n",
      "Epoch 123: MSE: 9.5508995, MAE: 1.7793056\n",
      "Epoch 124: MSE: 7.8008685, MAE: 1.6689415\n",
      "Epoch 125: MSE: 7.9858923, MAE: 1.7519995\n",
      "Epoch 126: MSE: 8.354227, MAE: 1.7946401\n",
      "Epoch 127: MSE: 7.5694814, MAE: 1.7312564\n",
      "Epoch 128: MSE: 7.7923326, MAE: 1.7780596\n",
      "Epoch 129: MSE: 7.1658554, MAE: 1.7521113\n",
      "Epoch 130: MSE: 7.852982, MAE: 1.7469858\n",
      "Epoch 131: MSE: 7.811585, MAE: 1.7141291\n",
      "Epoch 132: MSE: 7.73503, MAE: 1.6523856\n",
      "Epoch 133: MSE: 8.478133, MAE: 1.74233\n",
      "Epoch 134: MSE: 6.830758, MAE: 1.6841252\n",
      "Epoch 135: MSE: 7.0635557, MAE: 1.7441815\n",
      "Epoch 136: MSE: 7.3143396, MAE: 1.7575245\n",
      "Epoch 137: MSE: 8.243364, MAE: 1.7650598\n",
      "Epoch 138: MSE: 7.399633, MAE: 1.645204\n",
      "Epoch 139: MSE: 8.004756, MAE: 1.7397835\n",
      "Epoch 140: MSE: 7.3336086, MAE: 1.6476138\n",
      "Epoch 141: MSE: 6.639847, MAE: 1.724372\n",
      "Epoch 142: MSE: 7.723538, MAE: 1.7558112\n",
      "Epoch 143: MSE: 7.401545, MAE: 1.6417173\n",
      "Epoch 144: MSE: 6.382312, MAE: 1.6748582\n",
      "Epoch 145: MSE: 6.921776, MAE: 1.8097819\n",
      "Epoch 146: MSE: 7.494717, MAE: 1.6887466\n",
      "Epoch 147: MSE: 7.554471, MAE: 1.6676116\n",
      "Epoch 148: MSE: 6.116933, MAE: 1.6353084\n",
      "Epoch 149: MSE: 8.2120495, MAE: 1.7093037\n",
      "Epoch 150: MSE: 7.1362486, MAE: 1.6363703\n",
      "Epoch 151: MSE: 6.8765607, MAE: 1.6533294\n",
      "Epoch 152: MSE: 6.828214, MAE: 1.6884068\n",
      "Epoch 153: MSE: 6.648977, MAE: 1.6289203\n",
      "Epoch 154: MSE: 6.6075983, MAE: 1.7111118\n",
      "Epoch 155: MSE: 6.8308034, MAE: 1.6735336\n",
      "Epoch 156: MSE: 6.6985025, MAE: 1.6061137\n",
      "Epoch 157: MSE: 5.791763, MAE: 1.6094463\n",
      "Epoch 158: MSE: 7.2842355, MAE: 1.7060968\n",
      "Epoch 159: MSE: 5.4839163, MAE: 1.6117876\n",
      "Epoch 160: MSE: 7.1244655, MAE: 1.7154704\n",
      "Epoch 161: MSE: 6.4533153, MAE: 1.5912753\n",
      "Epoch 162: MSE: 6.321007, MAE: 1.6223772\n",
      "Epoch 163: MSE: 6.4049835, MAE: 1.6472455\n",
      "Epoch 164: MSE: 6.056039, MAE: 1.6393926\n",
      "Epoch 165: MSE: 6.2141376, MAE: 1.651432\n",
      "Epoch 166: MSE: 6.449661, MAE: 1.5609549\n",
      "Epoch 167: MSE: 5.62841, MAE: 1.6161106\n",
      "Epoch 168: MSE: 6.742246, MAE: 1.6904837\n",
      "Epoch 169: MSE: 5.286403, MAE: 1.5703944\n",
      "Epoch 170: MSE: 7.5238514, MAE: 1.7291113\n",
      "Epoch 171: MSE: 5.647568, MAE: 1.5172056\n",
      "Epoch 172: MSE: 6.4211955, MAE: 1.7134392\n",
      "Epoch 173: MSE: 4.9399867, MAE: 1.5350647\n",
      "Epoch 174: MSE: 7.7345653, MAE: 1.7207992\n",
      "Epoch 175: MSE: 5.4840856, MAE: 1.538952\n",
      "Epoch 176: MSE: 6.067053, MAE: 1.6051146\n",
      "Epoch 177: MSE: 5.8164372, MAE: 1.6331594\n",
      "Epoch 178: MSE: 6.959172, MAE: 1.5716777\n",
      "Epoch 179: MSE: 5.332464, MAE: 1.500406\n",
      "Epoch 180: MSE: 5.00486, MAE: 1.5635259\n",
      "Epoch 181: MSE: 5.9067407, MAE: 1.7597536\n",
      "Epoch 182: MSE: 5.4670286, MAE: 1.55562\n",
      "Epoch 183: MSE: 6.70442, MAE: 1.6222068\n",
      "Epoch 184: MSE: 5.022941, MAE: 1.5281065\n",
      "Epoch 185: MSE: 6.0882816, MAE: 1.6265391\n",
      "Epoch 186: MSE: 5.168029, MAE: 1.513897\n",
      "Epoch 187: MSE: 4.5428915, MAE: 1.5950743\n",
      "Epoch 188: MSE: 6.3006973, MAE: 1.7675942\n",
      "Epoch 189: MSE: 6.3280845, MAE: 1.5405252\n",
      "Epoch 190: MSE: 6.1637444, MAE: 1.555395\n",
      "Epoch 191: MSE: 5.198435, MAE: 1.5256019\n",
      "Epoch 192: MSE: 6.9442244, MAE: 1.5563393\n",
      "Epoch 193: MSE: 4.877986, MAE: 1.5259812\n",
      "Epoch 194: MSE: 5.1452913, MAE: 1.6022116\n",
      "Epoch 195: MSE: 5.0735674, MAE: 1.5358723\n",
      "Epoch 196: MSE: 5.540075, MAE: 1.6094214\n",
      "Epoch 197: MSE: 6.7983937, MAE: 1.6630868\n",
      "Epoch 198: MSE: 4.938917, MAE: 1.4756935\n",
      "Epoch 199: MSE: 5.6403785, MAE: 1.6060175\n",
      "Epoch 200: MSE: 5.54154, MAE: 1.5143044\n",
      "Epoch 201: MSE: 5.5753098, MAE: 1.5601764\n",
      "Epoch 202: MSE: 4.8973303, MAE: 1.5080678\n",
      "Epoch 203: MSE: 5.7611, MAE: 1.6440355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204: MSE: 5.8629656, MAE: 1.5529401\n",
      "Epoch 205: MSE: 4.417165, MAE: 1.4902012\n",
      "Epoch 206: MSE: 5.704977, MAE: 1.4926358\n",
      "Epoch 207: MSE: 5.9223166, MAE: 1.6661497\n",
      "Epoch 208: MSE: 5.144955, MAE: 1.4987527\n",
      "Epoch 209: MSE: 4.945585, MAE: 1.4947363\n",
      "Epoch 210: MSE: 5.53083, MAE: 1.5694433\n",
      "Epoch 211: MSE: 5.7050476, MAE: 1.505933\n",
      "Epoch 212: MSE: 4.429266, MAE: 1.4932778\n",
      "Epoch 213: MSE: 4.484904, MAE: 1.5041156\n",
      "Epoch 214: MSE: 5.63726, MAE: 1.6001498\n",
      "Epoch 215: MSE: 4.7366033, MAE: 1.4974208\n",
      "Epoch 216: MSE: 4.743142, MAE: 1.5536658\n",
      "Epoch 217: MSE: 4.5767946, MAE: 1.5780773\n",
      "Epoch 218: MSE: 4.9849277, MAE: 1.5846045\n",
      "Epoch 219: MSE: 5.520872, MAE: 1.4766389\n",
      "Epoch 220: MSE: 5.373663, MAE: 1.5041344\n",
      "Epoch 221: MSE: 4.6598024, MAE: 1.536872\n",
      "Epoch 222: MSE: 5.2643347, MAE: 1.5900631\n",
      "Epoch 223: MSE: 4.2423515, MAE: 1.4604989\n",
      "Epoch 224: MSE: 6.490338, MAE: 1.5707607\n",
      "Epoch 225: MSE: 4.6870074, MAE: 1.4319619\n",
      "Epoch 226: MSE: 4.6928105, MAE: 1.4971993\n",
      "Epoch 227: MSE: 4.9387407, MAE: 1.6457143\n",
      "Epoch 228: MSE: 5.1668453, MAE: 1.4944841\n",
      "Epoch 229: MSE: 5.18765, MAE: 1.4989674\n",
      "Epoch 230: MSE: 4.9600883, MAE: 1.4776751\n",
      "Epoch 231: MSE: 5.056186, MAE: 1.5476036\n",
      "Epoch 232: MSE: 4.4002533, MAE: 1.5072453\n",
      "Epoch 233: MSE: 4.356345, MAE: 1.4665067\n",
      "Epoch 234: MSE: 4.845162, MAE: 1.591468\n",
      "Epoch 235: MSE: 4.419121, MAE: 1.5031114\n",
      "Epoch 236: MSE: 4.9854684, MAE: 1.5562254\n",
      "Epoch 237: MSE: 5.360247, MAE: 1.5473295\n",
      "Epoch 238: MSE: 3.84642, MAE: 1.4304174\n",
      "Epoch 239: MSE: 5.0220423, MAE: 1.7203097\n",
      "Epoch 240: MSE: 4.2467456, MAE: 1.4137189\n",
      "Epoch 241: MSE: 3.8736904, MAE: 1.4915003\n",
      "Epoch 242: MSE: 5.378092, MAE: 1.5840405\n",
      "Epoch 243: MSE: 4.6747327, MAE: 1.4970626\n",
      "Epoch 244: MSE: 4.9176507, MAE: 1.4193931\n",
      "Epoch 245: MSE: 4.51889, MAE: 1.456739\n",
      "Epoch 246: MSE: 4.7192674, MAE: 1.5809537\n",
      "Epoch 247: MSE: 4.806693, MAE: 1.5415217\n",
      "Epoch 248: MSE: 5.782987, MAE: 1.5510871\n",
      "Epoch 249: MSE: 3.757776, MAE: 1.329395\n",
      "Epoch 250: MSE: 4.221088, MAE: 1.5426557\n",
      "Epoch 251: MSE: 5.2516484, MAE: 1.6208335\n",
      "Epoch 252: MSE: 4.180191, MAE: 1.4965401\n",
      "Epoch 253: MSE: 4.8225965, MAE: 1.5483482\n",
      "Epoch 254: MSE: 4.153195, MAE: 1.3487022\n",
      "Epoch 255: MSE: 4.7025, MAE: 1.6519198\n",
      "Epoch 256: MSE: 4.4452024, MAE: 1.4391321\n",
      "Epoch 257: MSE: 4.5113187, MAE: 1.491778\n",
      "Epoch 258: MSE: 3.774701, MAE: 1.4296616\n",
      "Epoch 259: MSE: 4.841899, MAE: 1.4470731\n",
      "Epoch 260: MSE: 4.5058103, MAE: 1.4555026\n",
      "Epoch 261: MSE: 4.4118, MAE: 1.4688866\n",
      "Epoch 262: MSE: 3.6882222, MAE: 1.4956023\n",
      "Epoch 263: MSE: 5.323984, MAE: 1.485394\n",
      "Epoch 264: MSE: 4.360008, MAE: 1.4212304\n",
      "Epoch 265: MSE: 4.0763445, MAE: 1.4740812\n",
      "Epoch 266: MSE: 4.693339, MAE: 1.5458939\n",
      "Epoch 267: MSE: 4.565251, MAE: 1.5768454\n",
      "Epoch 268: MSE: 4.3738527, MAE: 1.3774388\n",
      "Epoch 269: MSE: 4.3383756, MAE: 1.441599\n",
      "Epoch 270: MSE: 4.9027934, MAE: 1.5127183\n",
      "Epoch 271: MSE: 4.0907803, MAE: 1.3548249\n",
      "Epoch 272: MSE: 4.614232, MAE: 1.5495313\n",
      "Epoch 273: MSE: 4.3962593, MAE: 1.4436796\n",
      "Epoch 274: MSE: 4.2979035, MAE: 1.4541308\n",
      "Epoch 275: MSE: 4.389125, MAE: 1.3658975\n",
      "Epoch 276: MSE: 4.194319, MAE: 1.4363714\n",
      "Epoch 277: MSE: 4.256037, MAE: 1.510971\n",
      "Epoch 278: MSE: 4.5620465, MAE: 1.5700365\n",
      "Epoch 279: MSE: 4.3423476, MAE: 1.3396926\n",
      "Epoch 280: MSE: 3.1422186, MAE: 1.2960007\n",
      "Epoch 281: MSE: 5.3721795, MAE: 1.6643231\n",
      "Epoch 282: MSE: 3.483387, MAE: 1.296348\n",
      "Epoch 283: MSE: 4.7486525, MAE: 1.4632498\n",
      "Epoch 284: MSE: 3.3926969, MAE: 1.3810046\n",
      "Epoch 285: MSE: 5.0199757, MAE: 1.5525945\n",
      "Epoch 286: MSE: 4.2478886, MAE: 1.388286\n",
      "Epoch 287: MSE: 4.522189, MAE: 1.4460989\n",
      "Epoch 288: MSE: 4.132949, MAE: 1.4856188\n",
      "Epoch 289: MSE: 3.3173943, MAE: 1.3061469\n",
      "Epoch 290: MSE: 4.4158607, MAE: 1.593422\n",
      "Epoch 291: MSE: 4.810743, MAE: 1.4244571\n",
      "Epoch 292: MSE: 3.598059, MAE: 1.301817\n",
      "Epoch 293: MSE: 4.865298, MAE: 1.5658246\n",
      "Epoch 294: MSE: 3.748166, MAE: 1.2446828\n",
      "Epoch 295: MSE: 4.574025, MAE: 1.4669464\n",
      "Epoch 296: MSE: 3.7643087, MAE: 1.3729148\n",
      "Epoch 297: MSE: 4.5437694, MAE: 1.5075942\n",
      "Epoch 298: MSE: 3.216933, MAE: 1.3291414\n",
      "Epoch 299: MSE: 4.1188936, MAE: 1.4852163\n",
      "Epoch 300: MSE: 4.1023693, MAE: 1.4068489\n",
      "Epoch 301: MSE: 3.5293982, MAE: 1.3989888\n",
      "Epoch 302: MSE: 4.0618544, MAE: 1.572989\n",
      "Epoch 303: MSE: 4.374685, MAE: 1.5030961\n",
      "Epoch 304: MSE: 3.2362142, MAE: 1.2802714\n",
      "Epoch 305: MSE: 3.9145977, MAE: 1.5548387\n",
      "Epoch 306: MSE: 3.6688795, MAE: 1.4003627\n",
      "Epoch 307: MSE: 3.7091653, MAE: 1.4808863\n",
      "Epoch 308: MSE: 4.3514037, MAE: 1.3334999\n",
      "Epoch 309: MSE: 3.4885404, MAE: 1.3976823\n",
      "Epoch 310: MSE: 4.67812, MAE: 1.4233568\n",
      "Epoch 311: MSE: 3.2329166, MAE: 1.3309467\n",
      "Epoch 312: MSE: 3.6002452, MAE: 1.3944196\n",
      "Epoch 313: MSE: 4.9388075, MAE: 1.4032809\n",
      "Epoch 314: MSE: 3.9711962, MAE: 1.4108772\n",
      "Epoch 315: MSE: 3.5846775, MAE: 1.3765069\n",
      "Epoch 316: MSE: 4.546737, MAE: 1.4685274\n",
      "Epoch 317: MSE: 2.8996923, MAE: 1.2436929\n",
      "Epoch 318: MSE: 3.8325016, MAE: 1.547559\n",
      "Epoch 319: MSE: 4.274178, MAE: 1.369008\n",
      "Epoch 320: MSE: 3.5833423, MAE: 1.3877552\n",
      "Epoch 321: MSE: 4.421107, MAE: 1.4292787\n",
      "Epoch 322: MSE: 3.5779474, MAE: 1.398163\n",
      "Epoch 323: MSE: 3.861883, MAE: 1.3267975\n",
      "Epoch 324: MSE: 3.9373116, MAE: 1.4547768\n",
      "Epoch 325: MSE: 3.4410806, MAE: 1.3476354\n",
      "Epoch 326: MSE: 3.085612, MAE: 1.3372021\n",
      "Epoch 327: MSE: 4.794536, MAE: 1.4607503\n",
      "Epoch 328: MSE: 4.015384, MAE: 1.5019546\n",
      "Epoch 329: MSE: 3.75087, MAE: 1.3586408\n",
      "Epoch 330: MSE: 2.8295133, MAE: 1.3155607\n",
      "Epoch 331: MSE: 4.798132, MAE: 1.437765\n",
      "Epoch 332: MSE: 3.3818638, MAE: 1.2874844\n",
      "Epoch 333: MSE: 3.6135085, MAE: 1.4284532\n",
      "Epoch 334: MSE: 3.356032, MAE: 1.2858068\n",
      "Epoch 335: MSE: 4.714741, MAE: 1.6089343\n",
      "Epoch 336: MSE: 2.7738593, MAE: 1.261676\n",
      "Epoch 337: MSE: 4.052631, MAE: 1.4486606\n",
      "Epoch 338: MSE: 3.8746877, MAE: 1.3828405\n",
      "Epoch 339: MSE: 4.099684, MAE: 1.5034155\n",
      "Epoch 340: MSE: 3.6510472, MAE: 1.2878726\n",
      "Epoch 341: MSE: 3.576298, MAE: 1.4348245\n",
      "Epoch 342: MSE: 3.1170058, MAE: 1.3285714\n",
      "Epoch 343: MSE: 3.8407605, MAE: 1.3742853\n",
      "Epoch 344: MSE: 3.8252387, MAE: 1.3859696\n",
      "Epoch 345: MSE: 3.0324843, MAE: 1.3814877\n",
      "Epoch 346: MSE: 4.8341165, MAE: 1.5162631\n",
      "Epoch 347: MSE: 2.915747, MAE: 1.1936375\n",
      "Epoch 348: MSE: 3.9362626, MAE: 1.44293\n",
      "Epoch 349: MSE: 4.1281714, MAE: 1.332443\n",
      "Epoch 350: MSE: 3.2498732, MAE: 1.2785485\n",
      "Epoch 351: MSE: 3.7174363, MAE: 1.5271001\n",
      "Epoch 352: MSE: 3.076552, MAE: 1.3074006\n",
      "Epoch 353: MSE: 4.9139986, MAE: 1.569099\n",
      "Epoch 354: MSE: 2.7927752, MAE: 1.1655838\n",
      "Epoch 355: MSE: 3.641029, MAE: 1.374715\n",
      "Epoch 356: MSE: 3.7381506, MAE: 1.4720696\n",
      "Epoch 357: MSE: 3.3834825, MAE: 1.3013393\n",
      "Epoch 358: MSE: 3.7764182, MAE: 1.4139161\n",
      "Epoch 359: MSE: 3.606007, MAE: 1.2779208\n",
      "Epoch 360: MSE: 4.1177626, MAE: 1.3320857\n",
      "Epoch 361: MSE: 3.061688, MAE: 1.2626315\n",
      "Epoch 362: MSE: 3.8661842, MAE: 1.3653779\n",
      "Epoch 363: MSE: 3.5897193, MAE: 1.2156938\n",
      "Epoch 364: MSE: 4.10441, MAE: 1.4384836\n",
      "Epoch 365: MSE: 3.5855978, MAE: 1.3714595\n",
      "Epoch 366: MSE: 3.0410495, MAE: 1.280014\n",
      "Epoch 367: MSE: 3.3852282, MAE: 1.3224561\n",
      "Epoch 368: MSE: 3.312438, MAE: 1.3708532\n",
      "Epoch 369: MSE: 3.665947, MAE: 1.3920487\n",
      "Epoch 370: MSE: 2.8056254, MAE: 1.3352853\n",
      "Epoch 371: MSE: 3.7657864, MAE: 1.1662327\n",
      "Epoch 372: MSE: 3.2962537, MAE: 1.4689825\n",
      "Epoch 373: MSE: 3.6650798, MAE: 1.3706088\n",
      "Epoch 374: MSE: 3.9638166, MAE: 1.4373283\n",
      "Epoch 375: MSE: 3.0174417, MAE: 1.188218\n",
      "Epoch 376: MSE: 3.3447423, MAE: 1.4140074\n",
      "Epoch 377: MSE: 3.0078368, MAE: 1.226473\n",
      "Epoch 378: MSE: 4.2286005, MAE: 1.3175157\n",
      "Epoch 379: MSE: 3.7232707, MAE: 1.3354961\n",
      "Epoch 380: MSE: 3.3699484, MAE: 1.2714896\n",
      "Epoch 381: MSE: 3.1243632, MAE: 1.2703857\n",
      "Epoch 382: MSE: 3.815113, MAE: 1.4320754\n",
      "Epoch 383: MSE: 3.6431856, MAE: 1.2478554\n",
      "Epoch 384: MSE: 3.0047374, MAE: 1.3187387\n",
      "Epoch 385: MSE: 3.8033354, MAE: 1.4561517\n",
      "Epoch 386: MSE: 2.9862072, MAE: 1.2569718\n",
      "Epoch 387: MSE: 2.7385893, MAE: 1.408141\n",
      "Epoch 388: MSE: 4.3596706, MAE: 1.3651212\n",
      "Epoch 389: MSE: 3.0587676, MAE: 1.2755955\n",
      "Epoch 390: MSE: 2.8112667, MAE: 1.2978859\n",
      "Epoch 391: MSE: 3.4250958, MAE: 1.4171835\n",
      "Epoch 392: MSE: 3.5672374, MAE: 1.3540186\n",
      "Epoch 393: MSE: 2.7523775, MAE: 1.2632234\n",
      "Epoch 394: MSE: 4.0395794, MAE: 1.42898\n",
      "Epoch 395: MSE: 3.5501738, MAE: 1.3793905\n",
      "Epoch 396: MSE: 2.7454872, MAE: 1.2548038\n",
      "Epoch 397: MSE: 3.5955014, MAE: 1.3746597\n",
      "Epoch 398: MSE: 2.8453066, MAE: 1.1584487\n",
      "Epoch 399: MSE: 4.450648, MAE: 1.4675874\n",
      "Epoch 400: MSE: 3.7412934, MAE: 1.2744546\n",
      "Epoch 401: MSE: 2.5049684, MAE: 1.0944495\n",
      "Epoch 402: MSE: 4.0340137, MAE: 1.5176873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 403: MSE: 3.4096918, MAE: 1.2548369\n",
      "Epoch 404: MSE: 2.9269202, MAE: 1.244082\n",
      "Epoch 405: MSE: 3.844939, MAE: 1.488626\n",
      "Epoch 406: MSE: 2.9949586, MAE: 1.2515607\n",
      "Epoch 407: MSE: 3.779837, MAE: 1.3573115\n",
      "Epoch 408: MSE: 3.4122252, MAE: 1.2713642\n",
      "Epoch 409: MSE: 2.892384, MAE: 1.3365561\n",
      "Epoch 410: MSE: 3.2657328, MAE: 1.3454946\n",
      "Epoch 411: MSE: 2.6580122, MAE: 1.2607694\n",
      "Epoch 412: MSE: 3.0445685, MAE: 1.5545933\n",
      "Epoch 413: MSE: 3.7501073, MAE: 1.2351229\n",
      "Epoch 414: MSE: 3.8364525, MAE: 1.3822888\n",
      "Epoch 415: MSE: 2.7059512, MAE: 1.1257997\n",
      "Epoch 416: MSE: 3.18781, MAE: 1.292966\n",
      "Epoch 417: MSE: 3.3766801, MAE: 1.3980644\n",
      "Epoch 418: MSE: 3.3134542, MAE: 1.3437563\n",
      "Epoch 419: MSE: 2.9553287, MAE: 1.2430589\n",
      "Epoch 420: MSE: 3.4636464, MAE: 1.3513833\n",
      "Epoch 421: MSE: 2.9403396, MAE: 1.3400298\n",
      "Epoch 422: MSE: 3.486006, MAE: 1.3258051\n",
      "Epoch 423: MSE: 4.116498, MAE: 1.2646537\n",
      "Epoch 424: MSE: 2.6261592, MAE: 1.1723425\n",
      "Epoch 425: MSE: 2.8367643, MAE: 1.3372365\n",
      "Epoch 426: MSE: 3.361268, MAE: 1.4404712\n",
      "Epoch 427: MSE: 2.8118308, MAE: 1.4117407\n",
      "Epoch 428: MSE: 3.6902847, MAE: 1.3239286\n",
      "Epoch 429: MSE: 3.1152563, MAE: 1.1914855\n",
      "Epoch 430: MSE: 3.0670993, MAE: 1.3118443\n",
      "Epoch 431: MSE: 2.806876, MAE: 1.2705747\n",
      "Epoch 432: MSE: 3.5504675, MAE: 1.3836988\n",
      "Epoch 433: MSE: 2.846567, MAE: 1.3289244\n",
      "Epoch 434: MSE: 3.0362933, MAE: 1.4318763\n",
      "Epoch 435: MSE: 2.818375, MAE: 1.2841604\n",
      "Epoch 436: MSE: 4.1008453, MAE: 1.1059484\n",
      "Epoch 437: MSE: 2.3194134, MAE: 1.2190646\n",
      "Epoch 438: MSE: 4.0323057, MAE: 1.4000715\n",
      "Epoch 439: MSE: 2.6270463, MAE: 1.2269764\n",
      "Epoch 440: MSE: 4.013054, MAE: 1.3883735\n",
      "Epoch 441: MSE: 2.4573474, MAE: 1.1550717\n",
      "Epoch 442: MSE: 2.904307, MAE: 1.343411\n",
      "Epoch 443: MSE: 3.4714983, MAE: 1.2162505\n",
      "Epoch 444: MSE: 2.6684494, MAE: 1.2972894\n",
      "Epoch 445: MSE: 4.5535483, MAE: 1.5901052\n",
      "Epoch 446: MSE: 2.6128588, MAE: 1.0672781\n",
      "Epoch 447: MSE: 2.8347278, MAE: 1.2803236\n",
      "Epoch 448: MSE: 2.9682643, MAE: 1.207529\n",
      "Epoch 449: MSE: 2.7410684, MAE: 1.3137171\n",
      "Epoch 450: MSE: 3.5517104, MAE: 1.3792998\n",
      "Epoch 451: MSE: 2.972524, MAE: 1.3568347\n",
      "Epoch 452: MSE: 3.0637271, MAE: 1.2564147\n",
      "Epoch 453: MSE: 2.683665, MAE: 1.1922816\n",
      "Epoch 454: MSE: 2.7526705, MAE: 1.4079608\n",
      "Epoch 455: MSE: 2.908604, MAE: 1.3836259\n",
      "Epoch 456: MSE: 3.4414027, MAE: 1.4523534\n",
      "Epoch 457: MSE: 3.1470132, MAE: 1.2724073\n",
      "Epoch 458: MSE: 2.774986, MAE: 1.0993079\n",
      "Epoch 459: MSE: 2.768762, MAE: 1.4962325\n",
      "Epoch 460: MSE: 3.1156104, MAE: 1.277113\n",
      "Epoch 461: MSE: 3.034208, MAE: 1.2743058\n",
      "Epoch 462: MSE: 3.4000666, MAE: 1.3875985\n",
      "Epoch 463: MSE: 3.1172867, MAE: 1.1308141\n",
      "Epoch 464: MSE: 2.539945, MAE: 1.2006053\n",
      "Epoch 465: MSE: 2.3694215, MAE: 1.3751711\n",
      "Epoch 466: MSE: 3.1618438, MAE: 1.5155519\n",
      "Epoch 467: MSE: 3.5934753, MAE: 1.1606604\n",
      "Epoch 468: MSE: 2.863448, MAE: 1.3163751\n",
      "Epoch 469: MSE: 3.1151652, MAE: 1.227379\n",
      "Epoch 470: MSE: 2.7814763, MAE: 1.2011156\n",
      "Epoch 471: MSE: 2.7151408, MAE: 1.2496594\n",
      "Epoch 472: MSE: 2.6692119, MAE: 1.3863233\n",
      "Epoch 473: MSE: 2.8921776, MAE: 1.3732809\n",
      "Epoch 474: MSE: 3.2474835, MAE: 1.3636224\n",
      "Epoch 475: MSE: 3.6498158, MAE: 1.3237623\n",
      "Epoch 476: MSE: 2.3374808, MAE: 1.1444358\n",
      "Epoch 477: MSE: 2.8392704, MAE: 1.2755288\n",
      "Epoch 478: MSE: 2.9792643, MAE: 1.5053376\n",
      "Epoch 479: MSE: 3.0133438, MAE: 1.2553083\n",
      "Epoch 480: MSE: 2.704807, MAE: 1.240932\n",
      "Epoch 481: MSE: 3.1627378, MAE: 1.3491607\n",
      "Epoch 482: MSE: 2.8833852, MAE: 1.2803154\n",
      "Epoch 483: MSE: 3.744186, MAE: 1.1805222\n",
      "Epoch 484: MSE: 2.3161478, MAE: 1.1750917\n",
      "Epoch 485: MSE: 2.7554586, MAE: 1.3415356\n",
      "Epoch 486: MSE: 2.4834013, MAE: 1.1515296\n",
      "Epoch 487: MSE: 3.2512603, MAE: 1.4897945\n",
      "Epoch 488: MSE: 3.558472, MAE: 1.147642\n",
      "Epoch 489: MSE: 1.9013419, MAE: 0.9655231\n",
      "Epoch 490: MSE: 3.8682463, MAE: 1.3860887\n",
      "Epoch 491: MSE: 2.187068, MAE: 1.0769521\n",
      "Epoch 492: MSE: 3.3478446, MAE: 1.4013201\n",
      "Epoch 493: MSE: 2.783006, MAE: 1.1478016\n",
      "Epoch 494: MSE: 3.2108412, MAE: 1.3785377\n",
      "Epoch 495: MSE: 2.5828185, MAE: 1.1316888\n",
      "Epoch 496: MSE: 2.5031343, MAE: 1.2391537\n",
      "Epoch 497: MSE: 3.3329632, MAE: 1.2201573\n",
      "Epoch 498: MSE: 2.5168462, MAE: 1.2065153\n",
      "Epoch 499: MSE: 3.1814063, MAE: 1.2825177\n",
      "Epoch 500: MSE: 2.4191628, MAE: 1.2032305\n"
     ]
    }
   ],
   "source": [
    "for epoch in 1...epochCount {\n",
    "    var epochLoss: Float = 0\n",
    "    var epochMAE: Float = 0\n",
    "    var batchCount: Int = 0\n",
    "    var batchArray = Array(repeating: false, count: numberOfBatch)\n",
    "    for batch in 0..<numberOfBatch {\n",
    "        var r = batch\n",
    "        if shuffle {\n",
    "            while true {\n",
    "                r = Int.random(in: 0..<numberOfBatch)\n",
    "                if !batchArray[r] {\n",
    "                    batchArray[r] = true\n",
    "                    break\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        let batchStart = r * batchSize\n",
    "        let batchEnd = min(numTrainRecords, batchStart + batchSize)\n",
    "        let (loss, grad) = model.valueWithGradient { (model: RegressionModel) -> Tensor<Float> in\n",
    "            let multiInput = MultiInputs(numerical: [XNumericalTrain[batchStart..<batchEnd]],\n",
    "                                         categorical: [XCategoricalTrain[0][batchStart..<batchEnd],\n",
    "                                                       XCategoricalTrain[1][batchStart..<batchEnd]])\n",
    "            let logits = model(multiInput)\n",
    "            return meanSquaredError(predicted: logits, expected: YTrain[batchStart..<batchEnd])\n",
    "        }\n",
    "        optimizer.update(&model, along: grad)\n",
    "        \n",
    "        let multiInput = MultiInputs(numerical: [XNumericalTrain[batchStart..<batchEnd]],\n",
    "                                     categorical: [XCategoricalTrain[0][batchStart..<batchEnd],\n",
    "                                                   XCategoricalTrain[1][batchStart..<batchEnd]])\n",
    "        let logits = model(multiInput)\n",
    "        epochMAE += mae(predictions: logits, truths: YTrain[batchStart..<batchEnd])\n",
    "        epochLoss += loss.scalarized()\n",
    "        batchCount += 1\n",
    "    }\n",
    "    epochMAE /= Float(batchCount)\n",
    "    epochLoss /= Float(batchCount)\n",
    "\n",
    "    print(\"Epoch \\(epoch): MSE: \\(epochLoss), MAE: \\(epochMAE)\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.39759186, MAE: 0.044550754\r\n"
     ]
    }
   ],
   "source": [
    "Context.local.learningPhase = .inference\n",
    "\n",
    "let multiInputTest = MultiInputs(numerical: [XNumericalTest],\n",
    "                                 categorical: [XCategoricalTest[0],\n",
    "                                               XCategoricalTest[1]])\n",
    "\n",
    "let prediction = model(multiInputTest)\n",
    "\n",
    "let predictionMse = meanSquaredError(predicted: prediction, expected: YTest).scalarized()/Float(numTestRecords)\n",
    "let predictionMae = mae(predictions: prediction, truths: YTest)/Float(numTestRecords)\n",
    "\n",
    "print(\"MSE: \\(predictionMse), MAE: \\(predictionMae)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fkf29HLlohIP"
   },
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "// print(model.layer1.weight.shape, model.layer2.weight.shape, model.layer3.weight.shape)\n",
    "// print(model.layer1.bias.shape, model.layer2.bias.shape, model.layer3.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "// let coremlModel = Model(version: 4,\n",
    "//                         shortDescription: \"Regression\",\n",
    "//                         author: \"Jacopo Mangiavacchi\",\n",
    "//                         license: \"MIT\",\n",
    "//                         userDefined: [\"SwiftCoremltoolsVersion\" : \"0.0.3\"]) {\n",
    "//     Input(name: \"input\", shape: [13])\n",
    "//     Output(name: \"output\", shape: [1])\n",
    "//     NeuralNetwork {\n",
    "//         InnerProduct(name: \"dense1\",\n",
    "//                      input: [\"input\"],\n",
    "//                      output: [\"outDense1\"],\n",
    "//                      weight: model.layer1.weight.transposed().flattened().scalars,\n",
    "//                      bias: model.layer1.bias.flattened().scalars,\n",
    "//                      inputChannels: 13,\n",
    "//                      outputChannels: 64)\n",
    "//         ReLu(name: \"Relu1\",\n",
    "//              input: [\"outDense1\"],\n",
    "//              output: [\"outRelu1\"])\n",
    "//         InnerProduct(name: \"dense2\",\n",
    "//                      input: [\"outRelu1\"],\n",
    "//                      output: [\"outDense2\"],\n",
    "//                      weight: model.layer2.weight.transposed().flattened().scalars,\n",
    "//                      bias: model.layer2.bias.flattened().scalars,\n",
    "//                      inputChannels: 64,\n",
    "//                      outputChannels: 32)\n",
    "//         ReLu(name: \"Relu2\",\n",
    "//              input: [\"outDense2\"],\n",
    "//              output: [\"outRelu2\"])\n",
    "//         InnerProduct(name: \"dense3\",\n",
    "//                      input: [\"outRelu2\"],\n",
    "//                      output: [\"output\"],\n",
    "//                      weight: model.layer3.weight.transposed().flattened().scalars,\n",
    "//                      bias: model.layer3.bias.flattened().scalars,\n",
    "//                      inputChannels: 32,\n",
    "//                      outputChannels: 1)\n",
    "//     }\n",
    "// }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "// let coreMLData = coremlModel.coreMLData\n",
    "// try! coreMLData!.write(to: URL(fileURLWithPath: \"../model/s4tf_train_model.mlmodel\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "S4TF_House.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
