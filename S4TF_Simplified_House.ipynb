{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(url: \"https://github.com/JacopoMangiavacchi/SwiftCoreMLTools.git\", from: \"0.0.6\")\n",
      "\t\tSwiftCoreMLTools\n",
      "\t.package(url: \"https://github.com/dduan/Just.git\", from: \"0.8.0\")\n",
      "\t\tJust\n",
      "With SwiftPM flags: ['-c', 'release']\n",
      "Working in: /tmp/tmpwxxl_7ss/swift-install\n",
      "Fetching https://github.com/JacopoMangiavacchi/SwiftCoreMLTools.git\n",
      "Fetching https://github.com/dduan/Just.git\n",
      "Fetching https://github.com/apple/swift-protobuf.git\n",
      "Cloning https://github.com/dduan/Just.git\n",
      "Resolving https://github.com/dduan/Just.git at 0.8.0\n",
      "Cloning https://github.com/apple/swift-protobuf.git\n",
      "Resolving https://github.com/apple/swift-protobuf.git at 1.8.0\n",
      "Cloning https://github.com/JacopoMangiavacchi/SwiftCoreMLTools.git\n",
      "Resolving https://github.com/JacopoMangiavacchi/SwiftCoreMLTools.git at 0.0.6\n",
      "[1/3] Compiling Just Just.swift\n",
      "[2/3] Compiling SwiftProtobuf AnyMessageStorage.swift\n",
      "[3/4] Compiling SwiftCoreMLTools Activations.swift\n",
      "[4/5] Compiling jupyterInstalledPackages jupyterInstalledPackages.swift\n",
      "[5/5] Linking libjupyterInstalledPackages.so\n",
      "Initializing Swift...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install-swiftpm-flags -c release\n",
    "%install '.package(url: \"https://github.com/JacopoMangiavacchi/SwiftCoreMLTools.git\", from: \"0.0.6\")' SwiftCoreMLTools\n",
    "%install '.package(url: \"https://github.com/dduan/Just.git\", from: \"0.8.0\")' Just"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TO4FT9bUohHx"
   },
   "outputs": [],
   "source": [
    "import Foundation\n",
    "import SwiftCoreMLTools\n",
    "import TensorFlow\n",
    "import Just"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Download\n",
    "\n",
    "Boston house prices dataset\n",
    "---------------------------\n",
    "\n",
    "**Data Set Characteristics:**  \n",
    "\n",
    "    :Number of Instances: 506 \n",
    "\n",
    "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
    "\n",
    "    :Attribute Information (in order):\n",
    "        - CRIM     per capita crime rate by town\n",
    "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "        - INDUS    proportion of non-retail business acres per town\n",
    "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "        - NOX      nitric oxides concentration (parts per 10 million)\n",
    "        - RM       average number of rooms per dwelling\n",
    "        - AGE      proportion of owner-occupied units built prior to 1940\n",
    "        - DIS      weighted distances to five Boston employment centres\n",
    "        - RAD      index of accessibility to radial highways\n",
    "        - TAX      full-value property-tax rate per ten thousand dollars\n",
    "        - PTRATIO  pupil-teacher ratio by town\n",
    "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "        - LSTAT    % lower status of the population\n",
    "        - MEDV     Median value of owner-occupied homes in a thousand dollar\n",
    "\n",
    "    :Missing Attribute Values: None\n",
    "\n",
    "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
    "\n",
    "This is a copy of UCI ML housing dataset.\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// if let cts = Just.get(URL(string: \"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\")!).content {\n",
    "//     try! cts.write(to: URL(fileURLWithPath:\"../data/housing.csv\"))\n",
    "// }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "let data = try String(contentsOfFile:\"./data/housing.csv\", encoding: String.Encoding.utf8)\n",
    "let dataRecords: [[Float]] = data.split(separator: \"\\n\").map{ String($0).split(separator: \" \").compactMap{ Float(String($0)) } }\n",
    "\n",
    "let numRecords = dataRecords.count\n",
    "let numColumns = dataRecords[0].count\n",
    "\n",
    "let dataFeatures = dataRecords.map{ Array($0[0..<numColumns-1]) }\n",
    "let dataLabels = dataRecords.map{ Array($0[(numColumns-1)...]) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Numerical Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "let categoricalColumns = [3, 8]\n",
    "let numericalColumns = [0, 1, 2, 4, 5, 6, 7, 9, 10, 11, 12]\n",
    "let numCategoricalFeatures = categoricalColumns.count\n",
    "let numNumericalFeatures = numericalColumns.count\n",
    "let numLabels = 1\n",
    "\n",
    "assert(numColumns == numCategoricalFeatures + numNumericalFeatures + 1)\n",
    "\n",
    "// Get Categorical Features\n",
    "let allCategoriesValues = dataFeatures.map{ row in categoricalColumns.map{ Int32(row[$0]) } }\n",
    "                                .reduce(into: Array(repeating: [Int32](), count: 2)){ total, value in\n",
    "                                    total[0].append(value[0])\n",
    "                                    total[1].append(value[1]) }\n",
    "                                .map{ Set($0).sorted() }\n",
    "\n",
    "let categoricalFeatures = dataFeatures.map{ row in categoricalColumns.map{ Int32(row[$0]) } }\n",
    "\n",
    "// Get Numerical Features\n",
    "let numericalFeatures = dataFeatures.map{ row in numericalColumns.map{ row[$0] } }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorize Categorical Features with Ordinal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "var categoricalValues = Array(repeating: Set<Int32>(), count: 2)\n",
    "\n",
    "for record in categoricalFeatures {\n",
    "    categoricalValues[0].insert(record[0])\n",
    "    categoricalValues[1].insert(record[1])\n",
    "}\n",
    "\n",
    "let sortedCategoricalValues = [categoricalValues[0].sorted(), categoricalValues[1].sorted()]\n",
    "\n",
    "let ordinalCategoricalFeatures = categoricalFeatures.map{ [Int32(sortedCategoricalValues[0].firstIndex(of:$0[0])!), \n",
    "                                                           Int32(sortedCategoricalValues[1].firstIndex(of:$0[1])!)] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "let trainPercentage:Float = 0.8\n",
    "let numTrainRecords = Int(ceil(Float(numRecords) * trainPercentage))\n",
    "let numTestRecords = numRecords - numTrainRecords\n",
    "\n",
    "func matrixTranspose<T>(_ matrix: [[T]]) -> [[T]] {\n",
    "    if matrix.isEmpty {return matrix}\n",
    "    var result = [[T]]()\n",
    "    for index in 0..<matrix.first!.count {\n",
    "        result.append(matrix.map{$0[index]})\n",
    "    }\n",
    "    return result\n",
    "}\n",
    "\n",
    "let xCategoricalAllTrain = matrixTranspose(Array(ordinalCategoricalFeatures[0..<numTrainRecords]))\n",
    "let xCategoricalAllTest = matrixTranspose(Array(ordinalCategoricalFeatures[numTrainRecords...]))\n",
    "let xNumericalAllTrain = Array(Array(numericalFeatures[0..<numTrainRecords]).joined())\n",
    "let xNumericalAllTest = Array(Array(numericalFeatures[numTrainRecords...]).joined())\n",
    "let yAllTrain = Array(Array(dataLabels[0..<numTrainRecords]).joined())\n",
    "let yAllTest = Array(Array(dataLabels[numTrainRecords...]).joined())\n",
    "\n",
    "let XCategoricalTrain = xCategoricalAllTrain.enumerated().map{ (offset, element) in \n",
    "    Tensor<Int32>(element).reshaped(to: TensorShape([numTrainRecords, 1]))\n",
    "}\n",
    "let XCategoricalTest = xCategoricalAllTest.enumerated().map{ (offset, element) in \n",
    "    Tensor<Int32>(element).reshaped(to: TensorShape([numTestRecords, 1]))\n",
    "}\n",
    "\n",
    "let XNumericalTrainDeNorm = Tensor<Float>(xNumericalAllTrain).reshaped(to: TensorShape([numTrainRecords, numNumericalFeatures]))\n",
    "let XNumericalTestDeNorm = Tensor<Float>(xNumericalAllTest).reshaped(to: TensorShape([numTestRecords, numNumericalFeatures]))\n",
    "let YTrain = Tensor<Float>(yAllTrain).reshaped(to: TensorShape([numTrainRecords, numLabels]))\n",
    "let YTest = Tensor<Float>(yAllTest).reshaped(to: TensorShape([numTestRecords, numLabels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.0137098,  14.197531,   9.523555, 0.53213036,  6.3311296,   64.47929,  4.1678762,  353.68396,\r\n",
      "    18.03163,  379.84735,  11.394517]] [[ 6.5076075,  25.258776,   6.534038, 0.11449408,  0.7311985,  29.000755,  2.1797554,  132.14561,\r\n",
      "    2.217345,  40.494495,   6.852825]]\r\n"
     ]
    }
   ],
   "source": [
    "let mean = XNumericalTrainDeNorm.mean(alongAxes: 0)\n",
    "let std = XNumericalTrainDeNorm.standardDeviation(alongAxes: 0)\n",
    "\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "let XNumericalTrain = (XNumericalTrainDeNorm - mean)/std\n",
    "let XNumericalTest = (XNumericalTestDeNorm - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shapes [405, 11] [405, 1] [405, 1] [405, 1]\r\n",
      "Testing shapes  [101, 11] [101, 1] [101, 1] [101, 1]\r\n"
     ]
    }
   ],
   "source": [
    "print(\"Training shapes \\(XNumericalTrain.shape) \\(XCategoricalTrain[0].shape) \\(XCategoricalTrain[1].shape) \\(YTrain.shape)\")\n",
    "print(\"Testing shapes  \\(XNumericalTest.shape) \\(XCategoricalTest[0].shape) \\(XCategoricalTest[1].shape) \\(YTest.shape)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HDJZCCgqohIC"
   },
   "outputs": [],
   "source": [
    "struct MultiInputs<N: Differentiable, C>: Differentiable {\n",
    "  var numerical: N\n",
    "  \n",
    "  @noDerivative\n",
    "  var categorical: C\n",
    "\n",
    "  @differentiable\n",
    "  init(numerical: N, categorical: C) {\n",
    "    self.numerical = numerical\n",
    "    self.categorical = categorical\n",
    "  }\n",
    "}\n",
    "\n",
    "struct RegressionModel: Module {\n",
    "    var embedding1 = TensorFlow.Embedding<Float>(vocabularySize: 2, embeddingSize: 2)\n",
    "    var embedding2 = TensorFlow.Embedding<Float>(vocabularySize: 9, embeddingSize: 5)\n",
    "    var allInputConcatLayer = Dense<Float>(inputSize: (11 + 2 + 5), outputSize: 64, activation: relu)\n",
    "    var hiddenLayer = Dense<Float>(inputSize: 64, outputSize: 32, activation: relu)\n",
    "    var outputLayer = Dense<Float>(inputSize: 32, outputSize: 1)\n",
    "    \n",
    "    @differentiable\n",
    "    func callAsFunction(_ input: MultiInputs<[Tensor<Float>], [Tensor<Int32>]>) -> Tensor<Float> {\n",
    "        let embeddingOutput1 = embedding1(input.categorical[0])\n",
    "        let embeddingOutput1Reshaped = embeddingOutput1.reshaped(to: \n",
    "            TensorShape([embeddingOutput1.shape[0], embeddingOutput1.shape[2]]))\n",
    "        let embeddingOutput2 = embedding2(input.categorical[1])\n",
    "        let embeddingOutput2Reshaped = embeddingOutput2.reshaped(to: \n",
    "            TensorShape([embeddingOutput2.shape[0], embeddingOutput2.shape[2]]))\n",
    "        let allConcat = Tensor<Float>(concatenating: [input.numerical[0], embeddingOutput1Reshaped, embeddingOutput2Reshaped], alongAxis: 1)\n",
    "        return allConcat.sequenced(through: allInputConcatLayer, hiddenLayer, outputLayer)\n",
    "    }\n",
    "}\n",
    "\n",
    "var model = RegressionModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JK0Vj7bSohIF"
   },
   "outputs": [],
   "source": [
    "let optimizer = RMSProp(for: model, learningRate: 0.001)\n",
    "Context.local.learningPhase = .training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gY8C7yHJohIH"
   },
   "outputs": [],
   "source": [
    "let epochCount = 500\n",
    "let batchSize = 32\n",
    "let numberOfBatch = Int(ceil(Double(numTrainRecords) / Double(batchSize)))\n",
    "let shuffle = true\n",
    "\n",
    "func mae(predictions: Tensor<Float>, truths: Tensor<Float>) -> Float {\n",
    "    return abs(Tensor<Float>(predictions - truths)).mean().scalarized()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "L9bU9HsdohIK",
    "outputId": "692b81c5-3286-4e7c-9246-eb56e6a3eaee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: MSE: 629.58466, MAE: 23.11697\n",
      "Epoch 2: MSE: 569.8215, MAE: 21.903145\n",
      "Epoch 3: MSE: 507.82208, MAE: 20.533684\n",
      "Epoch 4: MSE: 437.7764, MAE: 18.820034\n",
      "Epoch 5: MSE: 362.32455, MAE: 16.728703\n",
      "Epoch 6: MSE: 281.21158, MAE: 14.396033\n",
      "Epoch 7: MSE: 203.11066, MAE: 11.521348\n",
      "Epoch 8: MSE: 139.76155, MAE: 8.793625\n",
      "Epoch 9: MSE: 97.55573, MAE: 6.717092\n",
      "Epoch 10: MSE: 72.564285, MAE: 5.4857526\n",
      "Epoch 11: MSE: 56.397984, MAE: 4.7238894\n",
      "Epoch 12: MSE: 48.320656, MAE: 4.3293858\n",
      "Epoch 13: MSE: 43.571487, MAE: 4.110623\n",
      "Epoch 14: MSE: 40.232944, MAE: 3.8525772\n",
      "Epoch 15: MSE: 37.14219, MAE: 3.750439\n",
      "Epoch 16: MSE: 35.128788, MAE: 3.5926704\n",
      "Epoch 17: MSE: 33.282864, MAE: 3.4505336\n",
      "Epoch 18: MSE: 31.539206, MAE: 3.3956962\n",
      "Epoch 19: MSE: 30.507399, MAE: 3.2626429\n",
      "Epoch 20: MSE: 29.406546, MAE: 3.2253046\n",
      "Epoch 21: MSE: 28.389687, MAE: 3.1390784\n",
      "Epoch 22: MSE: 27.679405, MAE: 3.0726068\n",
      "Epoch 23: MSE: 26.825886, MAE: 3.0767207\n",
      "Epoch 24: MSE: 26.078176, MAE: 3.0019925\n",
      "Epoch 25: MSE: 25.744408, MAE: 2.9980521\n",
      "Epoch 26: MSE: 25.11871, MAE: 3.0119874\n",
      "Epoch 27: MSE: 24.826082, MAE: 2.9553041\n",
      "Epoch 28: MSE: 24.198761, MAE: 2.857408\n",
      "Epoch 29: MSE: 23.23219, MAE: 2.8793986\n",
      "Epoch 30: MSE: 23.193542, MAE: 2.8072739\n",
      "Epoch 31: MSE: 23.10017, MAE: 2.8961387\n",
      "Epoch 32: MSE: 22.309208, MAE: 2.7295904\n",
      "Epoch 33: MSE: 22.092258, MAE: 2.7692657\n",
      "Epoch 34: MSE: 21.75702, MAE: 2.7826712\n",
      "Epoch 35: MSE: 21.585442, MAE: 2.7758813\n",
      "Epoch 36: MSE: 21.183128, MAE: 2.7499084\n",
      "Epoch 37: MSE: 20.890755, MAE: 2.7340586\n",
      "Epoch 38: MSE: 20.54848, MAE: 2.6900907\n",
      "Epoch 39: MSE: 20.346174, MAE: 2.6409986\n",
      "Epoch 40: MSE: 19.702688, MAE: 2.6141305\n",
      "Epoch 41: MSE: 19.746044, MAE: 2.6210623\n",
      "Epoch 42: MSE: 19.124447, MAE: 2.6455011\n",
      "Epoch 43: MSE: 19.035402, MAE: 2.568731\n",
      "Epoch 44: MSE: 18.782452, MAE: 2.5470793\n",
      "Epoch 45: MSE: 18.581697, MAE: 2.5399616\n",
      "Epoch 46: MSE: 18.224657, MAE: 2.4932177\n",
      "Epoch 47: MSE: 17.62485, MAE: 2.5124106\n",
      "Epoch 48: MSE: 17.336214, MAE: 2.476138\n",
      "Epoch 49: MSE: 17.298058, MAE: 2.4449468\n",
      "Epoch 50: MSE: 17.385712, MAE: 2.4416816\n",
      "Epoch 51: MSE: 16.739668, MAE: 2.3898466\n",
      "Epoch 52: MSE: 16.498108, MAE: 2.4087052\n",
      "Epoch 53: MSE: 16.283304, MAE: 2.3973594\n",
      "Epoch 54: MSE: 15.85581, MAE: 2.3854403\n",
      "Epoch 55: MSE: 15.5452795, MAE: 2.2903972\n",
      "Epoch 56: MSE: 15.734283, MAE: 2.31363\n",
      "Epoch 57: MSE: 15.439677, MAE: 2.3015149\n",
      "Epoch 58: MSE: 14.955271, MAE: 2.264007\n",
      "Epoch 59: MSE: 14.311797, MAE: 2.2854793\n",
      "Epoch 60: MSE: 14.743386, MAE: 2.2615776\n",
      "Epoch 61: MSE: 14.389335, MAE: 2.2604184\n",
      "Epoch 62: MSE: 14.104353, MAE: 2.2190785\n",
      "Epoch 63: MSE: 13.923264, MAE: 2.1666079\n",
      "Epoch 64: MSE: 13.653237, MAE: 2.1582198\n",
      "Epoch 65: MSE: 13.441671, MAE: 2.1496594\n",
      "Epoch 66: MSE: 13.050966, MAE: 2.1707327\n",
      "Epoch 67: MSE: 12.747824, MAE: 2.1061046\n",
      "Epoch 68: MSE: 13.034339, MAE: 2.1021433\n",
      "Epoch 69: MSE: 12.467745, MAE: 2.1333055\n",
      "Epoch 70: MSE: 12.410548, MAE: 2.0511186\n",
      "Epoch 71: MSE: 12.501837, MAE: 2.0787606\n",
      "Epoch 72: MSE: 12.294129, MAE: 2.049856\n",
      "Epoch 73: MSE: 12.117193, MAE: 2.0416472\n",
      "Epoch 74: MSE: 11.68469, MAE: 2.0539658\n",
      "Epoch 75: MSE: 11.522187, MAE: 2.0220556\n",
      "Epoch 76: MSE: 11.819091, MAE: 2.0434835\n",
      "Epoch 77: MSE: 11.166429, MAE: 1.9950256\n",
      "Epoch 78: MSE: 11.386383, MAE: 2.0453985\n",
      "Epoch 79: MSE: 11.019907, MAE: 1.9479994\n",
      "Epoch 80: MSE: 11.011484, MAE: 1.9681249\n",
      "Epoch 81: MSE: 10.720112, MAE: 1.9673587\n",
      "Epoch 82: MSE: 10.809511, MAE: 1.9501885\n",
      "Epoch 83: MSE: 10.452022, MAE: 1.9428489\n",
      "Epoch 84: MSE: 10.535157, MAE: 1.9489825\n",
      "Epoch 85: MSE: 10.424251, MAE: 1.956824\n",
      "Epoch 86: MSE: 10.124912, MAE: 1.9484484\n",
      "Epoch 87: MSE: 9.873498, MAE: 1.9123071\n",
      "Epoch 88: MSE: 10.204198, MAE: 1.9263183\n",
      "Epoch 89: MSE: 10.146987, MAE: 1.8894175\n",
      "Epoch 90: MSE: 9.860564, MAE: 1.9047096\n",
      "Epoch 91: MSE: 9.461685, MAE: 1.8831437\n",
      "Epoch 92: MSE: 9.810104, MAE: 1.8685988\n",
      "Epoch 93: MSE: 9.498625, MAE: 1.8506595\n",
      "Epoch 94: MSE: 9.51275, MAE: 1.8738456\n",
      "Epoch 95: MSE: 9.236965, MAE: 1.8470345\n",
      "Epoch 96: MSE: 9.340064, MAE: 1.8605641\n",
      "Epoch 97: MSE: 9.080156, MAE: 1.8210324\n",
      "Epoch 98: MSE: 8.915795, MAE: 1.8184847\n",
      "Epoch 99: MSE: 9.17034, MAE: 1.8482776\n",
      "Epoch 100: MSE: 8.94985, MAE: 1.841677\n",
      "Epoch 101: MSE: 9.140893, MAE: 1.8415653\n",
      "Epoch 102: MSE: 8.867895, MAE: 1.8323264\n",
      "Epoch 103: MSE: 8.698793, MAE: 1.8204821\n",
      "Epoch 104: MSE: 8.570967, MAE: 1.7985919\n",
      "Epoch 105: MSE: 8.701496, MAE: 1.800887\n",
      "Epoch 106: MSE: 8.595744, MAE: 1.796021\n",
      "Epoch 107: MSE: 8.474279, MAE: 1.7975446\n",
      "Epoch 108: MSE: 8.545732, MAE: 1.7802238\n",
      "Epoch 109: MSE: 8.155441, MAE: 1.7793782\n",
      "Epoch 110: MSE: 8.32045, MAE: 1.7859051\n",
      "Epoch 111: MSE: 7.9386454, MAE: 1.7677492\n",
      "Epoch 112: MSE: 8.21799, MAE: 1.7606125\n",
      "Epoch 113: MSE: 7.425616, MAE: 1.7180519\n",
      "Epoch 114: MSE: 8.097047, MAE: 1.7373575\n",
      "Epoch 115: MSE: 8.17407, MAE: 1.7228585\n",
      "Epoch 116: MSE: 7.9103594, MAE: 1.7411244\n",
      "Epoch 117: MSE: 7.9454756, MAE: 1.7424178\n",
      "Epoch 118: MSE: 7.6313887, MAE: 1.7363293\n",
      "Epoch 119: MSE: 7.929986, MAE: 1.7320321\n",
      "Epoch 120: MSE: 7.659405, MAE: 1.7288933\n",
      "Epoch 121: MSE: 7.593468, MAE: 1.7008542\n",
      "Epoch 122: MSE: 7.5812373, MAE: 1.7041711\n",
      "Epoch 123: MSE: 7.481977, MAE: 1.6849905\n",
      "Epoch 124: MSE: 7.3057427, MAE: 1.6998893\n",
      "Epoch 125: MSE: 7.460897, MAE: 1.6815865\n",
      "Epoch 126: MSE: 7.288655, MAE: 1.6876748\n",
      "Epoch 127: MSE: 7.3414893, MAE: 1.6897964\n",
      "Epoch 128: MSE: 7.2203774, MAE: 1.6703585\n",
      "Epoch 129: MSE: 7.0585246, MAE: 1.6515324\n",
      "Epoch 130: MSE: 6.912648, MAE: 1.6492198\n",
      "Epoch 131: MSE: 7.133911, MAE: 1.6554017\n",
      "Epoch 132: MSE: 7.038925, MAE: 1.6475537\n",
      "Epoch 133: MSE: 6.6092267, MAE: 1.6282427\n",
      "Epoch 134: MSE: 7.103241, MAE: 1.6430682\n",
      "Epoch 135: MSE: 6.8500357, MAE: 1.6378719\n",
      "Epoch 136: MSE: 6.9447737, MAE: 1.6317519\n",
      "Epoch 137: MSE: 6.8725486, MAE: 1.6109447\n",
      "Epoch 138: MSE: 6.827672, MAE: 1.6392626\n",
      "Epoch 139: MSE: 6.4777327, MAE: 1.6148796\n",
      "Epoch 140: MSE: 6.8931932, MAE: 1.6023699\n",
      "Epoch 141: MSE: 6.573606, MAE: 1.6038458\n",
      "Epoch 142: MSE: 6.598665, MAE: 1.6034156\n",
      "Epoch 143: MSE: 6.6025195, MAE: 1.6123776\n",
      "Epoch 144: MSE: 6.3754125, MAE: 1.5876607\n",
      "Epoch 145: MSE: 6.609337, MAE: 1.6014451\n",
      "Epoch 146: MSE: 6.105994, MAE: 1.5696379\n",
      "Epoch 147: MSE: 6.370225, MAE: 1.5669544\n",
      "Epoch 148: MSE: 6.242147, MAE: 1.5574523\n",
      "Epoch 149: MSE: 6.1687593, MAE: 1.5628873\n",
      "Epoch 150: MSE: 6.170433, MAE: 1.5559648\n",
      "Epoch 151: MSE: 6.315385, MAE: 1.5528855\n",
      "Epoch 152: MSE: 6.107436, MAE: 1.550329\n",
      "Epoch 153: MSE: 6.176591, MAE: 1.546585\n",
      "Epoch 154: MSE: 6.1249456, MAE: 1.5612566\n",
      "Epoch 155: MSE: 6.0808496, MAE: 1.5291303\n",
      "Epoch 156: MSE: 6.078616, MAE: 1.5436026\n",
      "Epoch 157: MSE: 6.037275, MAE: 1.5456252\n",
      "Epoch 158: MSE: 6.048901, MAE: 1.5256189\n",
      "Epoch 159: MSE: 5.765643, MAE: 1.5236863\n",
      "Epoch 160: MSE: 5.7184067, MAE: 1.5190974\n",
      "Epoch 161: MSE: 5.77785, MAE: 1.5200609\n",
      "Epoch 162: MSE: 5.7654424, MAE: 1.513922\n",
      "Epoch 163: MSE: 5.9324117, MAE: 1.5184977\n",
      "Epoch 164: MSE: 5.508215, MAE: 1.5034359\n",
      "Epoch 165: MSE: 5.761258, MAE: 1.514145\n",
      "Epoch 166: MSE: 5.6243505, MAE: 1.4951478\n",
      "Epoch 167: MSE: 5.481283, MAE: 1.4911702\n",
      "Epoch 168: MSE: 5.504316, MAE: 1.5087882\n",
      "Epoch 169: MSE: 5.654534, MAE: 1.4969885\n",
      "Epoch 170: MSE: 5.603403, MAE: 1.4835194\n",
      "Epoch 171: MSE: 5.5215793, MAE: 1.4748312\n",
      "Epoch 172: MSE: 5.589396, MAE: 1.4846975\n",
      "Epoch 173: MSE: 5.4631476, MAE: 1.4757298\n",
      "Epoch 174: MSE: 5.3173795, MAE: 1.4661838\n",
      "Epoch 175: MSE: 5.582053, MAE: 1.4899502\n",
      "Epoch 176: MSE: 5.272794, MAE: 1.4594041\n",
      "Epoch 177: MSE: 5.254096, MAE: 1.4670602\n",
      "Epoch 178: MSE: 5.299313, MAE: 1.4380378\n",
      "Epoch 179: MSE: 5.3349967, MAE: 1.4633511\n",
      "Epoch 180: MSE: 5.1291604, MAE: 1.4503919\n",
      "Epoch 181: MSE: 4.926068, MAE: 1.4278941\n",
      "Epoch 182: MSE: 5.3025813, MAE: 1.4404062\n",
      "Epoch 183: MSE: 5.1599097, MAE: 1.4276553\n",
      "Epoch 184: MSE: 5.3570056, MAE: 1.4343429\n",
      "Epoch 185: MSE: 5.0328417, MAE: 1.430581\n",
      "Epoch 186: MSE: 4.86307, MAE: 1.421055\n",
      "Epoch 187: MSE: 5.169326, MAE: 1.4280359\n",
      "Epoch 188: MSE: 5.0039434, MAE: 1.4250954\n",
      "Epoch 189: MSE: 4.878348, MAE: 1.417109\n",
      "Epoch 190: MSE: 4.6636157, MAE: 1.4026067\n",
      "Epoch 191: MSE: 4.9637103, MAE: 1.4057816\n",
      "Epoch 192: MSE: 4.9085317, MAE: 1.4132743\n",
      "Epoch 193: MSE: 4.939575, MAE: 1.3986319\n",
      "Epoch 194: MSE: 4.906172, MAE: 1.409637\n",
      "Epoch 195: MSE: 4.623504, MAE: 1.3854749\n",
      "Epoch 196: MSE: 4.9873943, MAE: 1.4021056\n",
      "Epoch 197: MSE: 4.79779, MAE: 1.4130312\n",
      "Epoch 198: MSE: 4.7732344, MAE: 1.3989482\n",
      "Epoch 199: MSE: 4.602734, MAE: 1.3983371\n",
      "Epoch 200: MSE: 4.9225855, MAE: 1.379869\n",
      "Epoch 201: MSE: 4.5867763, MAE: 1.3826917\n",
      "Epoch 202: MSE: 4.7324743, MAE: 1.380594\n",
      "Epoch 203: MSE: 4.612139, MAE: 1.3812162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204: MSE: 4.5593343, MAE: 1.3742963\n",
      "Epoch 205: MSE: 4.6487865, MAE: 1.3793479\n",
      "Epoch 206: MSE: 4.414052, MAE: 1.3822147\n",
      "Epoch 207: MSE: 4.7290735, MAE: 1.3812848\n",
      "Epoch 208: MSE: 4.1453533, MAE: 1.3496896\n",
      "Epoch 209: MSE: 4.7030582, MAE: 1.375815\n",
      "Epoch 210: MSE: 4.4231906, MAE: 1.3613276\n",
      "Epoch 211: MSE: 4.3875513, MAE: 1.3606963\n",
      "Epoch 212: MSE: 4.113841, MAE: 1.3427184\n",
      "Epoch 213: MSE: 4.3402133, MAE: 1.362984\n",
      "Epoch 214: MSE: 4.3597045, MAE: 1.3436236\n",
      "Epoch 215: MSE: 4.299067, MAE: 1.3543054\n",
      "Epoch 216: MSE: 4.294114, MAE: 1.3368479\n",
      "Epoch 217: MSE: 4.216857, MAE: 1.3340079\n",
      "Epoch 218: MSE: 4.601817, MAE: 1.3581208\n",
      "Epoch 219: MSE: 4.0256743, MAE: 1.337351\n",
      "Epoch 220: MSE: 4.1342416, MAE: 1.3337433\n",
      "Epoch 221: MSE: 4.058764, MAE: 1.3229325\n",
      "Epoch 222: MSE: 4.1086392, MAE: 1.3208662\n",
      "Epoch 223: MSE: 4.1123614, MAE: 1.3081975\n",
      "Epoch 224: MSE: 4.070235, MAE: 1.3242009\n",
      "Epoch 225: MSE: 4.0501947, MAE: 1.317912\n",
      "Epoch 226: MSE: 3.971472, MAE: 1.2927272\n",
      "Epoch 227: MSE: 4.0339823, MAE: 1.2983512\n",
      "Epoch 228: MSE: 4.271251, MAE: 1.299884\n",
      "Epoch 229: MSE: 4.1008735, MAE: 1.2947873\n",
      "Epoch 230: MSE: 3.9888327, MAE: 1.2852454\n",
      "Epoch 231: MSE: 3.709143, MAE: 1.318103\n",
      "Epoch 232: MSE: 4.235402, MAE: 1.2961733\n",
      "Epoch 233: MSE: 4.0663114, MAE: 1.2832634\n",
      "Epoch 234: MSE: 4.1519885, MAE: 1.2925091\n",
      "Epoch 235: MSE: 4.006318, MAE: 1.2787274\n",
      "Epoch 236: MSE: 3.7865107, MAE: 1.2758416\n",
      "Epoch 237: MSE: 3.8915687, MAE: 1.2919352\n",
      "Epoch 238: MSE: 4.0011926, MAE: 1.2858344\n",
      "Epoch 239: MSE: 3.9206507, MAE: 1.2945415\n",
      "Epoch 240: MSE: 3.8162193, MAE: 1.3000138\n",
      "Epoch 241: MSE: 4.0128245, MAE: 1.2641057\n",
      "Epoch 242: MSE: 4.046254, MAE: 1.2645872\n",
      "Epoch 243: MSE: 3.7631822, MAE: 1.2519009\n",
      "Epoch 244: MSE: 3.9238358, MAE: 1.2637414\n",
      "Epoch 245: MSE: 3.7766135, MAE: 1.2849345\n",
      "Epoch 246: MSE: 3.8643343, MAE: 1.2755555\n",
      "Epoch 247: MSE: 3.7376795, MAE: 1.2903947\n",
      "Epoch 248: MSE: 3.825991, MAE: 1.2666554\n",
      "Epoch 249: MSE: 3.7522302, MAE: 1.2637194\n",
      "Epoch 250: MSE: 3.7890477, MAE: 1.2757242\n",
      "Epoch 251: MSE: 3.719194, MAE: 1.2480485\n",
      "Epoch 252: MSE: 3.6912942, MAE: 1.2835529\n",
      "Epoch 253: MSE: 3.6618207, MAE: 1.2789751\n",
      "Epoch 254: MSE: 3.6200488, MAE: 1.2528431\n",
      "Epoch 255: MSE: 3.8153903, MAE: 1.2394494\n",
      "Epoch 256: MSE: 3.6366572, MAE: 1.2690324\n",
      "Epoch 257: MSE: 3.7378354, MAE: 1.2386234\n",
      "Epoch 258: MSE: 3.6348138, MAE: 1.2630757\n",
      "Epoch 259: MSE: 3.5182467, MAE: 1.2299745\n",
      "Epoch 260: MSE: 3.66154, MAE: 1.2496793\n",
      "Epoch 261: MSE: 3.5035186, MAE: 1.2426103\n",
      "Epoch 262: MSE: 3.770529, MAE: 1.2496972\n",
      "Epoch 263: MSE: 3.463846, MAE: 1.230854\n",
      "Epoch 264: MSE: 3.4265742, MAE: 1.2114991\n",
      "Epoch 265: MSE: 3.8419135, MAE: 1.2548196\n",
      "Epoch 266: MSE: 3.3627684, MAE: 1.2241789\n",
      "Epoch 267: MSE: 3.7151096, MAE: 1.2359068\n",
      "Epoch 268: MSE: 3.5019412, MAE: 1.2153659\n",
      "Epoch 269: MSE: 3.4406157, MAE: 1.2202036\n",
      "Epoch 270: MSE: 3.47051, MAE: 1.2200809\n",
      "Epoch 271: MSE: 3.738648, MAE: 1.2381829\n",
      "Epoch 272: MSE: 3.3216681, MAE: 1.2021381\n",
      "Epoch 273: MSE: 3.4001083, MAE: 1.1971576\n",
      "Epoch 274: MSE: 3.325757, MAE: 1.1768075\n",
      "Epoch 275: MSE: 3.3227632, MAE: 1.232198\n",
      "Epoch 276: MSE: 3.4490297, MAE: 1.1992623\n",
      "Epoch 277: MSE: 3.399261, MAE: 1.1860667\n",
      "Epoch 278: MSE: 3.4776938, MAE: 1.2140218\n",
      "Epoch 279: MSE: 3.4786055, MAE: 1.2126532\n",
      "Epoch 280: MSE: 3.585205, MAE: 1.2037101\n",
      "Epoch 281: MSE: 3.3487103, MAE: 1.1898186\n",
      "Epoch 282: MSE: 3.4219594, MAE: 1.2036226\n",
      "Epoch 283: MSE: 3.3050308, MAE: 1.2008964\n",
      "Epoch 284: MSE: 3.4368396, MAE: 1.1851304\n",
      "Epoch 285: MSE: 3.2787845, MAE: 1.1848365\n",
      "Epoch 286: MSE: 3.1307878, MAE: 1.1548514\n",
      "Epoch 287: MSE: 3.3315825, MAE: 1.2169948\n",
      "Epoch 288: MSE: 3.2990897, MAE: 1.1880342\n",
      "Epoch 289: MSE: 3.4979358, MAE: 1.1777165\n",
      "Epoch 290: MSE: 3.2828212, MAE: 1.1628674\n",
      "Epoch 291: MSE: 3.5186152, MAE: 1.2058707\n",
      "Epoch 292: MSE: 3.0970793, MAE: 1.1634227\n",
      "Epoch 293: MSE: 3.4695742, MAE: 1.1926255\n",
      "Epoch 294: MSE: 3.3336895, MAE: 1.1875737\n",
      "Epoch 295: MSE: 3.2176535, MAE: 1.1449516\n",
      "Epoch 296: MSE: 3.2448003, MAE: 1.158814\n",
      "Epoch 297: MSE: 3.428755, MAE: 1.1894637\n",
      "Epoch 298: MSE: 3.1705678, MAE: 1.1638073\n",
      "Epoch 299: MSE: 3.1918173, MAE: 1.1719347\n",
      "Epoch 300: MSE: 3.2830915, MAE: 1.183977\n",
      "Epoch 301: MSE: 3.2377348, MAE: 1.1803973\n",
      "Epoch 302: MSE: 3.1931157, MAE: 1.1778901\n",
      "Epoch 303: MSE: 2.9695973, MAE: 1.1455628\n",
      "Epoch 304: MSE: 3.2594829, MAE: 1.1425062\n",
      "Epoch 305: MSE: 3.342593, MAE: 1.1689712\n",
      "Epoch 306: MSE: 3.0297658, MAE: 1.161811\n",
      "Epoch 307: MSE: 3.2526844, MAE: 1.1775178\n",
      "Epoch 308: MSE: 3.2850251, MAE: 1.1489971\n",
      "Epoch 309: MSE: 2.8986049, MAE: 1.1658593\n",
      "Epoch 310: MSE: 3.4062593, MAE: 1.1551547\n",
      "Epoch 311: MSE: 3.1863132, MAE: 1.1700345\n",
      "Epoch 312: MSE: 3.0673146, MAE: 1.1420618\n",
      "Epoch 313: MSE: 3.1864016, MAE: 1.1373194\n",
      "Epoch 314: MSE: 2.9308336, MAE: 1.1349702\n",
      "Epoch 315: MSE: 3.3464677, MAE: 1.1466854\n",
      "Epoch 316: MSE: 3.0743916, MAE: 1.1409041\n",
      "Epoch 317: MSE: 3.0604284, MAE: 1.1738105\n",
      "Epoch 318: MSE: 3.0312512, MAE: 1.1327753\n",
      "Epoch 319: MSE: 3.1855173, MAE: 1.1402689\n",
      "Epoch 320: MSE: 2.9117162, MAE: 1.134814\n",
      "Epoch 321: MSE: 3.1565378, MAE: 1.1527811\n",
      "Epoch 322: MSE: 2.9936016, MAE: 1.1500732\n",
      "Epoch 323: MSE: 3.066636, MAE: 1.131235\n",
      "Epoch 324: MSE: 3.0438907, MAE: 1.1329575\n",
      "Epoch 325: MSE: 2.919472, MAE: 1.1135315\n",
      "Epoch 326: MSE: 3.056748, MAE: 1.1469299\n",
      "Epoch 327: MSE: 2.8744893, MAE: 1.1241361\n",
      "Epoch 328: MSE: 3.0425158, MAE: 1.1683831\n",
      "Epoch 329: MSE: 2.9336958, MAE: 1.1304841\n",
      "Epoch 330: MSE: 2.9543407, MAE: 1.148556\n",
      "Epoch 331: MSE: 2.963627, MAE: 1.1668082\n",
      "Epoch 332: MSE: 3.2521718, MAE: 1.1020029\n",
      "Epoch 333: MSE: 3.1124187, MAE: 1.1279562\n",
      "Epoch 334: MSE: 2.8333907, MAE: 1.1228573\n",
      "Epoch 335: MSE: 2.943624, MAE: 1.1295224\n",
      "Epoch 336: MSE: 2.9662714, MAE: 1.1643659\n",
      "Epoch 337: MSE: 2.8154566, MAE: 1.0793922\n",
      "Epoch 338: MSE: 2.8787766, MAE: 1.1425391\n",
      "Epoch 339: MSE: 2.7837203, MAE: 1.1233269\n",
      "Epoch 340: MSE: 3.1194048, MAE: 1.1358986\n",
      "Epoch 341: MSE: 2.8737319, MAE: 1.1221598\n",
      "Epoch 342: MSE: 2.8629892, MAE: 1.1357033\n",
      "Epoch 343: MSE: 3.2111554, MAE: 1.1487283\n",
      "Epoch 344: MSE: 2.7929366, MAE: 1.0809809\n",
      "Epoch 345: MSE: 2.8467596, MAE: 1.1266501\n",
      "Epoch 346: MSE: 2.931404, MAE: 1.1242925\n",
      "Epoch 347: MSE: 2.8185005, MAE: 1.0849856\n",
      "Epoch 348: MSE: 2.6326213, MAE: 1.1018419\n",
      "Epoch 349: MSE: 2.8737166, MAE: 1.1345925\n",
      "Epoch 350: MSE: 2.8285975, MAE: 1.1100147\n",
      "Epoch 351: MSE: 2.9417639, MAE: 1.120337\n",
      "Epoch 352: MSE: 2.731297, MAE: 1.0980402\n",
      "Epoch 353: MSE: 2.7931883, MAE: 1.1149815\n",
      "Epoch 354: MSE: 2.7830687, MAE: 1.0834142\n",
      "Epoch 355: MSE: 2.6219172, MAE: 1.0699623\n",
      "Epoch 356: MSE: 2.9803655, MAE: 1.1409192\n",
      "Epoch 357: MSE: 2.6979802, MAE: 1.0955014\n",
      "Epoch 358: MSE: 2.9033065, MAE: 1.0798266\n",
      "Epoch 359: MSE: 2.8012772, MAE: 1.1314539\n",
      "Epoch 360: MSE: 2.7024236, MAE: 1.0892661\n",
      "Epoch 361: MSE: 2.7477438, MAE: 1.0875318\n",
      "Epoch 362: MSE: 2.835941, MAE: 1.1154935\n",
      "Epoch 363: MSE: 2.8575575, MAE: 1.076411\n",
      "Epoch 364: MSE: 2.6234221, MAE: 1.0936334\n",
      "Epoch 365: MSE: 2.8416324, MAE: 1.1057544\n",
      "Epoch 366: MSE: 2.6973014, MAE: 1.0737592\n",
      "Epoch 367: MSE: 2.7176106, MAE: 1.1117301\n",
      "Epoch 368: MSE: 2.6460347, MAE: 1.0789593\n",
      "Epoch 369: MSE: 2.8364527, MAE: 1.1153438\n",
      "Epoch 370: MSE: 2.7426863, MAE: 1.0926583\n",
      "Epoch 371: MSE: 2.6370444, MAE: 1.1092142\n",
      "Epoch 372: MSE: 2.6777453, MAE: 1.1124059\n",
      "Epoch 373: MSE: 2.8160644, MAE: 1.0805738\n",
      "Epoch 374: MSE: 2.5816236, MAE: 1.1042855\n",
      "Epoch 375: MSE: 2.4858904, MAE: 1.0592575\n",
      "Epoch 376: MSE: 2.671218, MAE: 1.095176\n",
      "Epoch 377: MSE: 2.7195883, MAE: 1.0661981\n",
      "Epoch 378: MSE: 2.6190317, MAE: 1.1073836\n",
      "Epoch 379: MSE: 2.6252284, MAE: 1.0963775\n",
      "Epoch 380: MSE: 2.479453, MAE: 1.0446744\n",
      "Epoch 381: MSE: 2.5538685, MAE: 1.1038082\n",
      "Epoch 382: MSE: 2.6994777, MAE: 1.0950502\n",
      "Epoch 383: MSE: 2.7565248, MAE: 1.0921239\n",
      "Epoch 384: MSE: 2.4380693, MAE: 1.0746585\n",
      "Epoch 385: MSE: 2.8117282, MAE: 1.1274655\n",
      "Epoch 386: MSE: 2.462821, MAE: 1.0698587\n",
      "Epoch 387: MSE: 2.5678353, MAE: 1.0383009\n",
      "Epoch 388: MSE: 2.7061234, MAE: 1.0738236\n",
      "Epoch 389: MSE: 2.4286966, MAE: 1.019367\n",
      "Epoch 390: MSE: 2.698402, MAE: 1.0655293\n",
      "Epoch 391: MSE: 2.7904134, MAE: 1.0646653\n",
      "Epoch 392: MSE: 2.3487983, MAE: 1.0446995\n",
      "Epoch 393: MSE: 2.5914235, MAE: 1.0992811\n",
      "Epoch 394: MSE: 2.5343678, MAE: 1.047753\n",
      "Epoch 395: MSE: 2.5956752, MAE: 1.0857362\n",
      "Epoch 396: MSE: 2.481754, MAE: 1.0531083\n",
      "Epoch 397: MSE: 2.4662993, MAE: 1.0959948\n",
      "Epoch 398: MSE: 2.4369388, MAE: 1.0886899\n",
      "Epoch 399: MSE: 2.4000936, MAE: 1.0481889\n",
      "Epoch 400: MSE: 2.4508684, MAE: 1.0593634\n",
      "Epoch 401: MSE: 2.6344438, MAE: 1.0290368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 402: MSE: 2.6133566, MAE: 1.0423671\n",
      "Epoch 403: MSE: 2.406435, MAE: 1.050483\n",
      "Epoch 404: MSE: 2.3432908, MAE: 1.1001995\n",
      "Epoch 405: MSE: 2.6220045, MAE: 1.0788184\n",
      "Epoch 406: MSE: 2.4184482, MAE: 1.0323774\n",
      "Epoch 407: MSE: 2.6634583, MAE: 1.0649166\n",
      "Epoch 408: MSE: 2.455541, MAE: 0.9939105\n",
      "Epoch 409: MSE: 2.5038664, MAE: 1.069643\n",
      "Epoch 410: MSE: 2.338223, MAE: 1.0484989\n",
      "Epoch 411: MSE: 2.4772298, MAE: 1.0958463\n",
      "Epoch 412: MSE: 2.4899092, MAE: 1.0576552\n",
      "Epoch 413: MSE: 2.2908878, MAE: 1.0070841\n",
      "Epoch 414: MSE: 2.462776, MAE: 1.0786923\n",
      "Epoch 415: MSE: 2.4756677, MAE: 1.0475224\n",
      "Epoch 416: MSE: 2.1973124, MAE: 1.0063245\n",
      "Epoch 417: MSE: 2.2664263, MAE: 1.0355222\n",
      "Epoch 418: MSE: 2.3486886, MAE: 1.045028\n",
      "Epoch 419: MSE: 2.6732073, MAE: 1.030545\n",
      "Epoch 420: MSE: 2.401425, MAE: 0.985335\n",
      "Epoch 421: MSE: 2.3671148, MAE: 1.0460799\n",
      "Epoch 422: MSE: 2.524806, MAE: 1.0412619\n",
      "Epoch 423: MSE: 2.2683346, MAE: 0.9877845\n",
      "Epoch 424: MSE: 2.4685006, MAE: 1.0427268\n",
      "Epoch 425: MSE: 2.2109466, MAE: 1.0170729\n",
      "Epoch 426: MSE: 2.601214, MAE: 1.0475047\n",
      "Epoch 427: MSE: 2.360961, MAE: 0.99790883\n",
      "Epoch 428: MSE: 2.244044, MAE: 1.0686474\n",
      "Epoch 429: MSE: 2.4532554, MAE: 1.0244155\n",
      "Epoch 430: MSE: 2.2521539, MAE: 1.041672\n",
      "Epoch 431: MSE: 2.6119664, MAE: 1.038386\n",
      "Epoch 432: MSE: 2.2290895, MAE: 0.9981047\n",
      "Epoch 433: MSE: 2.311751, MAE: 1.0513152\n",
      "Epoch 434: MSE: 2.2067397, MAE: 1.0282351\n",
      "Epoch 435: MSE: 2.4383266, MAE: 1.068232\n",
      "Epoch 436: MSE: 2.1222513, MAE: 0.9484431\n",
      "Epoch 437: MSE: 2.4369316, MAE: 1.0362347\n",
      "Epoch 438: MSE: 2.3764536, MAE: 1.0571376\n",
      "Epoch 439: MSE: 2.092556, MAE: 0.97255135\n",
      "Epoch 440: MSE: 2.284344, MAE: 1.0891753\n",
      "Epoch 441: MSE: 2.6078362, MAE: 0.9851995\n",
      "Epoch 442: MSE: 2.1873574, MAE: 0.96721584\n",
      "Epoch 443: MSE: 2.1708674, MAE: 1.0456014\n",
      "Epoch 444: MSE: 2.5585072, MAE: 1.0543799\n",
      "Epoch 445: MSE: 2.1347878, MAE: 0.9921588\n",
      "Epoch 446: MSE: 2.071611, MAE: 1.036377\n",
      "Epoch 447: MSE: 2.3708963, MAE: 1.0308608\n",
      "Epoch 448: MSE: 2.0958822, MAE: 0.95208424\n",
      "Epoch 449: MSE: 2.2141492, MAE: 0.999756\n",
      "Epoch 450: MSE: 2.2035887, MAE: 1.0219257\n",
      "Epoch 451: MSE: 2.579539, MAE: 1.0224941\n",
      "Epoch 452: MSE: 2.1353395, MAE: 0.970815\n",
      "Epoch 453: MSE: 2.3045857, MAE: 0.9914361\n",
      "Epoch 454: MSE: 2.09685, MAE: 1.0129169\n",
      "Epoch 455: MSE: 2.0444148, MAE: 0.9893501\n",
      "Epoch 456: MSE: 2.7381902, MAE: 1.0736976\n",
      "Epoch 457: MSE: 2.0967598, MAE: 0.94628215\n",
      "Epoch 458: MSE: 2.1711588, MAE: 0.98167646\n",
      "Epoch 459: MSE: 1.9976163, MAE: 1.0227375\n",
      "Epoch 460: MSE: 2.463741, MAE: 1.0197029\n",
      "Epoch 461: MSE: 2.0280638, MAE: 0.9746141\n",
      "Epoch 462: MSE: 2.0882568, MAE: 1.0449368\n",
      "Epoch 463: MSE: 2.294264, MAE: 1.0666909\n",
      "Epoch 464: MSE: 2.1710947, MAE: 0.97800934\n",
      "Epoch 465: MSE: 2.2655592, MAE: 0.9786116\n",
      "Epoch 466: MSE: 2.1415057, MAE: 0.9674486\n",
      "Epoch 467: MSE: 2.209503, MAE: 1.0528759\n",
      "Epoch 468: MSE: 2.0571134, MAE: 1.0337775\n",
      "Epoch 469: MSE: 2.5211825, MAE: 1.0509142\n",
      "Epoch 470: MSE: 2.217246, MAE: 0.9705172\n",
      "Epoch 471: MSE: 2.002207, MAE: 1.0129234\n",
      "Epoch 472: MSE: 2.2597146, MAE: 1.046997\n",
      "Epoch 473: MSE: 2.310621, MAE: 0.9927119\n",
      "Epoch 474: MSE: 1.999021, MAE: 0.9629764\n",
      "Epoch 475: MSE: 2.3046155, MAE: 1.0497484\n",
      "Epoch 476: MSE: 2.298408, MAE: 1.0166951\n",
      "Epoch 477: MSE: 2.1459136, MAE: 0.9686346\n",
      "Epoch 478: MSE: 2.0375888, MAE: 1.0156653\n",
      "Epoch 479: MSE: 2.2695127, MAE: 0.964692\n",
      "Epoch 480: MSE: 2.0367792, MAE: 0.9603892\n",
      "Epoch 481: MSE: 2.0038733, MAE: 1.0258371\n",
      "Epoch 482: MSE: 1.9957396, MAE: 0.97975963\n",
      "Epoch 483: MSE: 2.1955462, MAE: 0.9758681\n",
      "Epoch 484: MSE: 2.3236933, MAE: 0.99903136\n",
      "Epoch 485: MSE: 1.9647751, MAE: 0.94148964\n",
      "Epoch 486: MSE: 1.8612055, MAE: 0.96390426\n",
      "Epoch 487: MSE: 2.2393098, MAE: 1.0100574\n",
      "Epoch 488: MSE: 1.9239383, MAE: 0.9446143\n",
      "Epoch 489: MSE: 2.005178, MAE: 0.96574366\n",
      "Epoch 490: MSE: 2.0902123, MAE: 1.0215843\n",
      "Epoch 491: MSE: 2.5440938, MAE: 0.965153\n",
      "Epoch 492: MSE: 1.9090652, MAE: 0.91495574\n",
      "Epoch 493: MSE: 2.30452, MAE: 1.0431972\n",
      "Epoch 494: MSE: 2.0805824, MAE: 0.92482185\n",
      "Epoch 495: MSE: 1.8839607, MAE: 0.96664685\n",
      "Epoch 496: MSE: 2.3816607, MAE: 0.9919307\n",
      "Epoch 497: MSE: 1.9567324, MAE: 0.94575346\n",
      "Epoch 498: MSE: 2.0558019, MAE: 0.99370915\n",
      "Epoch 499: MSE: 2.0168433, MAE: 1.0112548\n",
      "Epoch 500: MSE: 2.3429186, MAE: 0.98698467\n"
     ]
    }
   ],
   "source": [
    "for epoch in 1...epochCount {\n",
    "    var epochLoss: Float = 0\n",
    "    var epochMAE: Float = 0\n",
    "    var batchCount: Int = 0\n",
    "    var batchArray = Array(repeating: false, count: numberOfBatch)\n",
    "    for batch in 0..<numberOfBatch {\n",
    "        var r = batch\n",
    "        if shuffle {\n",
    "            while true {\n",
    "                r = Int.random(in: 0..<numberOfBatch)\n",
    "                if !batchArray[r] {\n",
    "                    batchArray[r] = true\n",
    "                    break\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        let batchStart = r * batchSize\n",
    "        let batchEnd = min(numTrainRecords, batchStart + batchSize)\n",
    "        let (loss, grad) = model.valueWithGradient { (model: RegressionModel) -> Tensor<Float> in\n",
    "            let multiInput = MultiInputs(numerical: [XNumericalTrain[batchStart..<batchEnd]],\n",
    "                                         categorical: [XCategoricalTrain[0][batchStart..<batchEnd],\n",
    "                                                       XCategoricalTrain[1][batchStart..<batchEnd]])\n",
    "            let logits = model(multiInput)\n",
    "            return meanSquaredError(predicted: logits, expected: YTrain[batchStart..<batchEnd])\n",
    "        }\n",
    "        optimizer.update(&model, along: grad)\n",
    "        \n",
    "        let multiInput = MultiInputs(numerical: [XNumericalTrain[batchStart..<batchEnd]],\n",
    "                                     categorical: [XCategoricalTrain[0][batchStart..<batchEnd],\n",
    "                                                   XCategoricalTrain[1][batchStart..<batchEnd]])\n",
    "        let logits = model(multiInput)\n",
    "        epochMAE += mae(predictions: logits, truths: YTrain[batchStart..<batchEnd])\n",
    "        epochLoss += loss.scalarized()\n",
    "        batchCount += 1\n",
    "    }\n",
    "    epochMAE /= Float(batchCount)\n",
    "    epochLoss /= Float(batchCount)\n",
    "\n",
    "    print(\"Epoch \\(epoch): MSE: \\(epochLoss), MAE: \\(epochMAE)\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.5485814, MAE: 0.052630015\r\n"
     ]
    }
   ],
   "source": [
    "Context.local.learningPhase = .inference\n",
    "\n",
    "let multiInputTest = MultiInputs(numerical: [XNumericalTest],\n",
    "                                 categorical: [XCategoricalTest[0],\n",
    "                                               XCategoricalTest[1]])\n",
    "\n",
    "let prediction = model(multiInputTest)\n",
    "\n",
    "let predictionMse = meanSquaredError(predicted: prediction, expected: YTest).scalarized()/Float(numTestRecords)\n",
    "let predictionMae = mae(predictions: prediction, truths: YTest)/Float(numTestRecords)\n",
    "\n",
    "print(\"MSE: \\(predictionMse), MAE: \\(predictionMae)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fkf29HLlohIP"
   },
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2] [9, 5]\r\n"
     ]
    }
   ],
   "source": [
    "print(model.embedding1.embeddings.shape, model.embedding2.embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "let coremlModel = Model(version: 4,\n",
    "                        shortDescription: \"Regression\",\n",
    "                        author: \"Jacopo Mangiavacchi\",\n",
    "                        license: \"MIT\",\n",
    "                        userDefined: [\"SwiftCoremltoolsVersion\" : \"0.0.6\"]) {\n",
    "    Input(name: \"numericalInput\", shape: [11])\n",
    "    Input(name: \"categoricalInput1\", shape: [1])\n",
    "    Input(name: \"categoricalInput2\", shape: [1])\n",
    "    Output(name: \"output\", shape: [1])\n",
    "    NeuralNetwork {\n",
    "        Embedding(name: \"embedding1\",\n",
    "                     input: [\"categoricalInput1\"],\n",
    "                     output: [\"outEmbedding1\"],\n",
    "                     weight: model.embedding1.embeddings.transposed().flattened().scalars,\n",
    "                     inputDim: 2,\n",
    "                     outputChannels: 2)\n",
    "        Permute(name: \"permute1\",\n",
    "                     input: [\"outEmbedding1\"],\n",
    "                     output: [\"outPermute1\"],\n",
    "                     axis: [2, 1, 0, 3])\n",
    "        Flatten(name: \"flatten1\",\n",
    "                     input: [\"outPermute1\"],\n",
    "                     output: [\"outFlatten1\"],\n",
    "                     mode: .last)\n",
    "        Embedding(name: \"embedding2\",\n",
    "                     input: [\"categoricalInput2\"],\n",
    "                     output: [\"outEmbedding2\"],\n",
    "                     weight: model.embedding2.embeddings.transposed().flattened().scalars,\n",
    "                     inputDim: 9,\n",
    "                     outputChannels: 5)\n",
    "        Permute(name: \"permute2\",\n",
    "                     input: [\"outEmbedding2\"],\n",
    "                     output: [\"outPermute2\"],\n",
    "                     axis: [2, 1, 0, 3])\n",
    "        Flatten(name: \"flatten2\",\n",
    "                     input: [\"outPermute2\"],\n",
    "                     output: [\"outFlatten2\"],\n",
    "                     mode: .last)\n",
    "        Concat(name: \"concat\",\n",
    "                     input: [\"numericalInput\", \"outFlatten1\", \"outFlatten2\"],\n",
    "                     output: [\"outConcat\"])\n",
    "        InnerProduct(name: \"dense1\",\n",
    "                     input: [\"outConcat\"],\n",
    "                     output: [\"outDense1\"],\n",
    "                     weight: model.allInputConcatLayer.weight.transposed().flattened().scalars,\n",
    "                     bias: model.allInputConcatLayer.bias.flattened().scalars,\n",
    "                     inputChannels: 11 + 2 + 5,\n",
    "                     outputChannels: 64)\n",
    "        ReLu(name: \"Relu1\",\n",
    "             input: [\"outDense1\"],\n",
    "             output: [\"outRelu1\"])\n",
    "        InnerProduct(name: \"dense2\",\n",
    "                     input: [\"outRelu1\"],\n",
    "                     output: [\"outDense2\"],\n",
    "                     weight: model.hiddenLayer.weight.transposed().flattened().scalars,\n",
    "                     bias: model.hiddenLayer.bias.flattened().scalars,\n",
    "                     inputChannels: 64,\n",
    "                     outputChannels: 32)\n",
    "        ReLu(name: \"Relu2\",\n",
    "             input: [\"outDense2\"],\n",
    "             output: [\"outRelu2\"])\n",
    "        InnerProduct(name: \"dense3\",\n",
    "                     input: [\"outRelu2\"],\n",
    "                     output: [\"output\"],\n",
    "                     weight: model.outputLayer.weight.transposed().flattened().scalars,\n",
    "                     bias: model.outputLayer.bias.flattened().scalars,\n",
    "                     inputChannels: 32,\n",
    "                     outputChannels: 1)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "let coreMLData = coremlModel.coreMLData\n",
    "try! coreMLData!.write(to: URL(fileURLWithPath: \"./s4tf_house_simplified_trained_model.mlmodel\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  10.127699, -0.56208307,   1.3125796,   1.4050479,  -0.8863933,   1.2248203,  -1.2581577,\r\n",
      "   2.3634233,   0.9779133,  0.12650238,   1.6906141] [0] [8] => [5.0]\r\n",
      "[   1.541963, -0.56208307,   1.3125796,   0.7150559, -0.93426037,   0.7972452,  -1.0169381,\r\n",
      "   2.3634233,   0.9779133,   -2.180478,  0.39479825] [0] [8] => [20.8]\r\n",
      "[-0.29233322, -0.56208307,   2.7879307,   0.6713855,  -0.4761084,   0.6558696, -0.94413173,\r\n",
      "   2.7039568,   0.9328142,  0.42111015,  0.28535435] [0] [3] => [20.1]\r\n"
     ]
    }
   ],
   "source": [
    "print(XNumericalTest[0], XCategoricalTest[0][0], XCategoricalTest[1][0], \"=>\", YTest[0])\n",
    "print(XNumericalTest[17], XCategoricalTest[0][17], XCategoricalTest[1][17], \"=>\", YTest[17])\n",
    "print(XNumericalTest[87], XCategoricalTest[0][87], XCategoricalTest[1][87], \"=>\", YTest[87])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "S4TF_House.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
