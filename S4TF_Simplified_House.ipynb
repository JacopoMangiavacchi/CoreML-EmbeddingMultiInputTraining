{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(url: \"https://github.com/JacopoMangiavacchi/SwiftCoreMLTools.git\", from: \"0.0.6\")\n",
      "\t\tSwiftCoreMLTools\n",
      "\t.package(url: \"https://github.com/dduan/Just.git\", from: \"0.8.0\")\n",
      "\t\tJust\n",
      "With SwiftPM flags: ['-c', 'release']\n",
      "Working in: /tmp/tmpnxaapqu1/swift-install\n",
      "Fetching https://github.com/JacopoMangiavacchi/SwiftCoreMLTools.git\n",
      "Fetching https://github.com/dduan/Just.git\n",
      "Fetching https://github.com/apple/swift-protobuf.git\n",
      "Cloning https://github.com/JacopoMangiavacchi/SwiftCoreMLTools.git\n",
      "Resolving https://github.com/JacopoMangiavacchi/SwiftCoreMLTools.git at 0.0.6\n",
      "Cloning https://github.com/apple/swift-protobuf.git\n",
      "Resolving https://github.com/apple/swift-protobuf.git at 1.8.0\n",
      "Cloning https://github.com/dduan/Just.git\n",
      "Resolving https://github.com/dduan/Just.git at 0.8.0\n",
      "[1/3] Compiling Just Just.swift\n",
      "[2/3] Compiling SwiftProtobuf AnyMessageStorage.swift\n",
      "[3/4] Compiling SwiftCoreMLTools Activations.swift\n",
      "[4/5] Compiling jupyterInstalledPackages jupyterInstalledPackages.swift\n",
      "[5/5] Linking libjupyterInstalledPackages.so\n",
      "Initializing Swift...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install-swiftpm-flags -c release\n",
    "%install '.package(url: \"https://github.com/JacopoMangiavacchi/SwiftCoreMLTools.git\", from: \"0.0.6\")' SwiftCoreMLTools\n",
    "%install '.package(url: \"https://github.com/dduan/Just.git\", from: \"0.8.0\")' Just"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TO4FT9bUohHx"
   },
   "outputs": [],
   "source": [
    "import Foundation\n",
    "import SwiftCoreMLTools\n",
    "import TensorFlow\n",
    "import Just"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Download\n",
    "\n",
    "Boston house prices dataset\n",
    "---------------------------\n",
    "\n",
    "**Data Set Characteristics:**  \n",
    "\n",
    "    :Number of Instances: 506 \n",
    "\n",
    "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
    "\n",
    "    :Attribute Information (in order):\n",
    "        - CRIM     per capita crime rate by town\n",
    "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "        - INDUS    proportion of non-retail business acres per town\n",
    "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "        - NOX      nitric oxides concentration (parts per 10 million)\n",
    "        - RM       average number of rooms per dwelling\n",
    "        - AGE      proportion of owner-occupied units built prior to 1940\n",
    "        - DIS      weighted distances to five Boston employment centres\n",
    "        - RAD      index of accessibility to radial highways\n",
    "        - TAX      full-value property-tax rate per ten thousand dollars\n",
    "        - PTRATIO  pupil-teacher ratio by town\n",
    "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "        - LSTAT    % lower status of the population\n",
    "        - MEDV     Median value of owner-occupied homes in a thousand dollar\n",
    "\n",
    "    :Missing Attribute Values: None\n",
    "\n",
    "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
    "\n",
    "This is a copy of UCI ML housing dataset.\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// if let cts = Just.get(URL(string: \"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\")!).content {\n",
    "//     try! cts.write(to: URL(fileURLWithPath:\"../data/housing.csv\"))\n",
    "// }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "let data = try String(contentsOfFile:\"./data/housing.csv\", encoding: String.Encoding.utf8)\n",
    "let dataRecords: [[Float]] = data.split(separator: \"\\n\").map{ String($0).split(separator: \" \").compactMap{ Float(String($0)) } }\n",
    "\n",
    "let numRecords = dataRecords.count\n",
    "let numColumns = dataRecords[0].count\n",
    "\n",
    "var index = Set<Int>()\n",
    "\n",
    "while index.count < numRecords {\n",
    "    index.insert(Int.random(in: 0..<numRecords))\n",
    "}\n",
    "\n",
    "let randomDataRecords = index.map{ dataRecords[$0] }\n",
    "\n",
    "let dataFeatures = randomDataRecords.map{ Array($0[0..<numColumns-1]) }\n",
    "let dataLabels = randomDataRecords.map{ Array($0[(numColumns-1)...]) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Numerical Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "let categoricalColumns = [3, 8]\n",
    "let numericalColumns = [0, 1, 2, 4, 5, 6, 7, 9, 10, 11, 12]\n",
    "let numCategoricalFeatures = categoricalColumns.count\n",
    "let numNumericalFeatures = numericalColumns.count\n",
    "let numLabels = 1\n",
    "\n",
    "assert(numColumns == numCategoricalFeatures + numNumericalFeatures + 1)\n",
    "\n",
    "// Get Categorical Features\n",
    "let allCategoriesValues = dataFeatures.map{ row in categoricalColumns.map{ Int32(row[$0]) } }\n",
    "                                .reduce(into: Array(repeating: [Int32](), count: 2)){ total, value in\n",
    "                                    total[0].append(value[0])\n",
    "                                    total[1].append(value[1]) }\n",
    "                                .map{ Set($0).sorted() }\n",
    "\n",
    "let categoricalFeatures = dataFeatures.map{ row in categoricalColumns.map{ Int32(row[$0]) } }\n",
    "\n",
    "// Get Numerical Features\n",
    "let numericalFeatures = dataFeatures.map{ row in numericalColumns.map{ row[$0] } }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorize Categorical Features with Ordinal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "var categoricalValues = Array(repeating: Set<Int32>(), count: 2)\n",
    "\n",
    "for record in categoricalFeatures {\n",
    "    categoricalValues[0].insert(record[0])\n",
    "    categoricalValues[1].insert(record[1])\n",
    "}\n",
    "\n",
    "let sortedCategoricalValues = [categoricalValues[0].sorted(), categoricalValues[1].sorted()]\n",
    "\n",
    "let ordinalCategoricalFeatures = categoricalFeatures.map{ [Int32(sortedCategoricalValues[0].firstIndex(of:$0[0])!), \n",
    "                                                           Int32(sortedCategoricalValues[1].firstIndex(of:$0[1])!)] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "let trainPercentage:Float = 0.8\n",
    "let numTrainRecords = Int(ceil(Float(numRecords) * trainPercentage))\n",
    "let numTestRecords = numRecords - numTrainRecords\n",
    "\n",
    "func matrixTranspose<T>(_ matrix: [[T]]) -> [[T]] {\n",
    "    if matrix.isEmpty {return matrix}\n",
    "    var result = [[T]]()\n",
    "    for index in 0..<matrix.first!.count {\n",
    "        result.append(matrix.map{$0[index]})\n",
    "    }\n",
    "    return result\n",
    "}\n",
    "\n",
    "let xCategoricalAllTrain = matrixTranspose(Array(ordinalCategoricalFeatures[0..<numTrainRecords]))\n",
    "let xCategoricalAllTest = matrixTranspose(Array(ordinalCategoricalFeatures[numTrainRecords...]))\n",
    "let xNumericalAllTrain = Array(Array(numericalFeatures[0..<numTrainRecords]).joined())\n",
    "let xNumericalAllTest = Array(Array(numericalFeatures[numTrainRecords...]).joined())\n",
    "let yAllTrain = Array(Array(dataLabels[0..<numTrainRecords]).joined())\n",
    "let yAllTest = Array(Array(dataLabels[numTrainRecords...]).joined())\n",
    "\n",
    "let XCategoricalTrain = xCategoricalAllTrain.enumerated().map{ (offset, element) in \n",
    "    Tensor<Int32>(element).reshaped(to: TensorShape([numTrainRecords, 1]))\n",
    "}\n",
    "let XCategoricalTest = xCategoricalAllTest.enumerated().map{ (offset, element) in \n",
    "    Tensor<Int32>(element).reshaped(to: TensorShape([numTestRecords, 1]))\n",
    "}\n",
    "\n",
    "let XNumericalTrainDeNorm = Tensor<Float>(xNumericalAllTrain).reshaped(to: TensorShape([numTrainRecords, numNumericalFeatures]))\n",
    "let XNumericalTestDeNorm = Tensor<Float>(xNumericalAllTest).reshaped(to: TensorShape([numTestRecords, numNumericalFeatures]))\n",
    "let YTrain = Tensor<Float>(yAllTrain).reshaped(to: TensorShape([numTrainRecords, numLabels]))\n",
    "let YTest = Tensor<Float>(yAllTest).reshaped(to: TensorShape([numTestRecords, numLabels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.7090604, 12.055555, 11.054204, 0.5516017,   6.27021, 67.946434, 3.8624096,  405.0469,\r\n",
      "  18.481758, 356.45288, 12.810893]] [[  9.113623,  24.196712,  6.8583913, 0.11535243, 0.70409226,  28.822058,   2.176942,  166.97446,\r\n",
      "   2.1447299,   92.18375,  7.3877473]]\r\n"
     ]
    }
   ],
   "source": [
    "let mean = XNumericalTrainDeNorm.mean(alongAxes: 0)\n",
    "let std = XNumericalTrainDeNorm.standardDeviation(alongAxes: 0)\n",
    "\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "let XNumericalTrain = (XNumericalTrainDeNorm - mean)/std\n",
    "let XNumericalTest = (XNumericalTestDeNorm - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shapes [405, 11] [405, 1] [405, 1] [405, 1]\r\n",
      "Testing shapes  [101, 11] [101, 1] [101, 1] [101, 1]\r\n"
     ]
    }
   ],
   "source": [
    "print(\"Training shapes \\(XNumericalTrain.shape) \\(XCategoricalTrain[0].shape) \\(XCategoricalTrain[1].shape) \\(YTrain.shape)\")\n",
    "print(\"Testing shapes  \\(XNumericalTest.shape) \\(XCategoricalTest[0].shape) \\(XCategoricalTest[1].shape) \\(YTest.shape)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HDJZCCgqohIC"
   },
   "outputs": [],
   "source": [
    "struct MultiInputs<N: Differentiable, C>: Differentiable {\n",
    "  var numerical: N\n",
    "  \n",
    "  @noDerivative\n",
    "  var categorical: C\n",
    "\n",
    "  @differentiable\n",
    "  init(numerical: N, categorical: C) {\n",
    "    self.numerical = numerical\n",
    "    self.categorical = categorical\n",
    "  }\n",
    "}\n",
    "\n",
    "struct RegressionModel: Module {\n",
    "    var embedding1 = TensorFlow.Embedding<Float>(vocabularySize: 2, embeddingSize: 2)\n",
    "    var embedding2 = TensorFlow.Embedding<Float>(vocabularySize: 9, embeddingSize: 5)\n",
    "    var allInputConcatLayer = Dense<Float>(inputSize: (11 + 2 + 5), outputSize: 64, activation: relu)\n",
    "    var hiddenLayer = Dense<Float>(inputSize: 64, outputSize: 32, activation: relu)\n",
    "    var outputLayer = Dense<Float>(inputSize: 32, outputSize: 1)\n",
    "    \n",
    "    @differentiable\n",
    "    func callAsFunction(_ input: MultiInputs<[Tensor<Float>], [Tensor<Int32>]>) -> Tensor<Float> {\n",
    "        let embeddingOutput1 = embedding1(input.categorical[0])\n",
    "        let embeddingOutput1Reshaped = embeddingOutput1.reshaped(to: \n",
    "            TensorShape([embeddingOutput1.shape[0], embeddingOutput1.shape[2]]))\n",
    "        let embeddingOutput2 = embedding2(input.categorical[1])\n",
    "        let embeddingOutput2Reshaped = embeddingOutput2.reshaped(to: \n",
    "            TensorShape([embeddingOutput2.shape[0], embeddingOutput2.shape[2]]))\n",
    "        let allConcat = Tensor<Float>(concatenating: [input.numerical[0], embeddingOutput1Reshaped, embeddingOutput2Reshaped], alongAxis: 1)\n",
    "        return allConcat.sequenced(through: allInputConcatLayer, hiddenLayer, outputLayer)\n",
    "    }\n",
    "}\n",
    "\n",
    "var model = RegressionModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JK0Vj7bSohIF"
   },
   "outputs": [],
   "source": [
    "let optimizer = RMSProp(for: model, learningRate: 0.001)\n",
    "Context.local.learningPhase = .training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gY8C7yHJohIH"
   },
   "outputs": [],
   "source": [
    "let epochCount = 500\n",
    "let batchSize = 32\n",
    "let numberOfBatch = Int(ceil(Double(numTrainRecords) / Double(batchSize)))\n",
    "let shuffle = true\n",
    "\n",
    "func mae(predictions: Tensor<Float>, truths: Tensor<Float>) -> Float {\n",
    "    return abs(Tensor<Float>(predictions - truths)).mean().scalarized()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "L9bU9HsdohIK",
    "outputId": "692b81c5-3286-4e7c-9246-eb56e6a3eaee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: MSE: 529.0884, MAE: 21.016209\n",
      "Epoch 2: MSE: 449.94937, MAE: 19.12399\n",
      "Epoch 3: MSE: 365.3185, MAE: 16.928745\n",
      "Epoch 4: MSE: 275.31314, MAE: 14.302202\n",
      "Epoch 5: MSE: 190.93709, MAE: 11.408555\n",
      "Epoch 6: MSE: 123.30173, MAE: 8.57655\n",
      "Epoch 7: MSE: 79.491295, MAE: 6.537755\n",
      "Epoch 8: MSE: 55.950478, MAE: 5.202893\n",
      "Epoch 9: MSE: 44.580727, MAE: 4.5634108\n",
      "Epoch 10: MSE: 37.279842, MAE: 4.168285\n",
      "Epoch 11: MSE: 32.211872, MAE: 3.87214\n",
      "Epoch 12: MSE: 28.353092, MAE: 3.6448083\n",
      "Epoch 13: MSE: 25.91916, MAE: 3.442046\n",
      "Epoch 14: MSE: 23.809538, MAE: 3.289499\n",
      "Epoch 15: MSE: 22.509584, MAE: 3.1895945\n",
      "Epoch 16: MSE: 21.451454, MAE: 3.1035998\n",
      "Epoch 17: MSE: 20.647305, MAE: 3.0469809\n",
      "Epoch 18: MSE: 19.880947, MAE: 2.9606042\n",
      "Epoch 19: MSE: 19.185884, MAE: 2.9118884\n",
      "Epoch 20: MSE: 18.542118, MAE: 2.8709092\n",
      "Epoch 21: MSE: 17.781507, MAE: 2.7943006\n",
      "Epoch 22: MSE: 17.459024, MAE: 2.762055\n",
      "Epoch 23: MSE: 16.846321, MAE: 2.7299106\n",
      "Epoch 24: MSE: 16.437881, MAE: 2.6759505\n",
      "Epoch 25: MSE: 15.83022, MAE: 2.630382\n",
      "Epoch 26: MSE: 15.637585, MAE: 2.5997124\n",
      "Epoch 27: MSE: 15.1398535, MAE: 2.5326548\n",
      "Epoch 28: MSE: 14.718124, MAE: 2.5070033\n",
      "Epoch 29: MSE: 14.443236, MAE: 2.4715953\n",
      "Epoch 30: MSE: 13.913939, MAE: 2.4398882\n",
      "Epoch 31: MSE: 13.782312, MAE: 2.4101605\n",
      "Epoch 32: MSE: 13.561244, MAE: 2.3891416\n",
      "Epoch 33: MSE: 13.055952, MAE: 2.3418665\n",
      "Epoch 34: MSE: 12.969399, MAE: 2.3398771\n",
      "Epoch 35: MSE: 12.524186, MAE: 2.298536\n",
      "Epoch 36: MSE: 12.335832, MAE: 2.2607863\n",
      "Epoch 37: MSE: 12.205327, MAE: 2.2529612\n",
      "Epoch 38: MSE: 12.093312, MAE: 2.2261407\n",
      "Epoch 39: MSE: 11.768137, MAE: 2.2168756\n",
      "Epoch 40: MSE: 11.726003, MAE: 2.20438\n",
      "Epoch 41: MSE: 11.426788, MAE: 2.1740134\n",
      "Epoch 42: MSE: 11.425762, MAE: 2.1737854\n",
      "Epoch 43: MSE: 11.217622, MAE: 2.148262\n",
      "Epoch 44: MSE: 11.160559, MAE: 2.138519\n",
      "Epoch 45: MSE: 11.003158, MAE: 2.1078033\n",
      "Epoch 46: MSE: 10.787708, MAE: 2.1233404\n",
      "Epoch 47: MSE: 10.614327, MAE: 2.0887682\n",
      "Epoch 48: MSE: 10.656907, MAE: 2.0728498\n",
      "Epoch 49: MSE: 10.603037, MAE: 2.0718682\n",
      "Epoch 50: MSE: 10.363005, MAE: 2.0617769\n",
      "Epoch 51: MSE: 10.247313, MAE: 2.0408332\n",
      "Epoch 52: MSE: 10.253823, MAE: 2.0364847\n",
      "Epoch 53: MSE: 10.285305, MAE: 2.0416317\n",
      "Epoch 54: MSE: 10.070423, MAE: 2.0264876\n",
      "Epoch 55: MSE: 9.907628, MAE: 2.007704\n",
      "Epoch 56: MSE: 9.95365, MAE: 2.0065227\n",
      "Epoch 57: MSE: 9.848585, MAE: 2.005532\n",
      "Epoch 58: MSE: 9.487759, MAE: 1.9934604\n",
      "Epoch 59: MSE: 9.828363, MAE: 1.9764581\n",
      "Epoch 60: MSE: 9.6220665, MAE: 1.9803599\n",
      "Epoch 61: MSE: 9.457592, MAE: 1.9616795\n",
      "Epoch 62: MSE: 9.459951, MAE: 1.9527543\n",
      "Epoch 63: MSE: 9.2504425, MAE: 1.9584992\n",
      "Epoch 64: MSE: 9.013216, MAE: 1.92832\n",
      "Epoch 65: MSE: 9.306432, MAE: 1.9563776\n",
      "Epoch 66: MSE: 9.211383, MAE: 1.9307082\n",
      "Epoch 67: MSE: 9.10476, MAE: 1.9200706\n",
      "Epoch 68: MSE: 9.032848, MAE: 1.9279993\n",
      "Epoch 69: MSE: 8.939154, MAE: 1.9047751\n",
      "Epoch 70: MSE: 8.868769, MAE: 1.9056083\n",
      "Epoch 71: MSE: 8.868879, MAE: 1.8990897\n",
      "Epoch 72: MSE: 8.75049, MAE: 1.894558\n",
      "Epoch 73: MSE: 8.420861, MAE: 1.8714503\n",
      "Epoch 74: MSE: 8.606353, MAE: 1.8926449\n",
      "Epoch 75: MSE: 8.459958, MAE: 1.8683071\n",
      "Epoch 76: MSE: 8.55582, MAE: 1.8592038\n",
      "Epoch 77: MSE: 8.352378, MAE: 1.8529294\n",
      "Epoch 78: MSE: 8.243612, MAE: 1.8672632\n",
      "Epoch 79: MSE: 8.378941, MAE: 1.8406682\n",
      "Epoch 80: MSE: 8.285161, MAE: 1.8454986\n",
      "Epoch 81: MSE: 8.364572, MAE: 1.8425092\n",
      "Epoch 82: MSE: 8.099937, MAE: 1.8364075\n",
      "Epoch 83: MSE: 8.033824, MAE: 1.8283435\n",
      "Epoch 84: MSE: 8.122153, MAE: 1.8347762\n",
      "Epoch 85: MSE: 7.997205, MAE: 1.811657\n",
      "Epoch 86: MSE: 7.9454336, MAE: 1.8239667\n",
      "Epoch 87: MSE: 7.6823115, MAE: 1.803323\n",
      "Epoch 88: MSE: 8.037832, MAE: 1.816417\n",
      "Epoch 89: MSE: 7.8622065, MAE: 1.8053969\n",
      "Epoch 90: MSE: 7.769477, MAE: 1.7846812\n",
      "Epoch 91: MSE: 7.760735, MAE: 1.7970694\n",
      "Epoch 92: MSE: 7.5293612, MAE: 1.7793784\n",
      "Epoch 93: MSE: 7.528936, MAE: 1.7710891\n",
      "Epoch 94: MSE: 7.506841, MAE: 1.7657949\n",
      "Epoch 95: MSE: 7.578219, MAE: 1.7867506\n",
      "Epoch 96: MSE: 7.5144577, MAE: 1.7598119\n",
      "Epoch 97: MSE: 7.196022, MAE: 1.7419686\n",
      "Epoch 98: MSE: 7.5434794, MAE: 1.7560918\n",
      "Epoch 99: MSE: 7.3679304, MAE: 1.7552142\n",
      "Epoch 100: MSE: 7.368963, MAE: 1.7508397\n",
      "Epoch 101: MSE: 7.1051106, MAE: 1.7379061\n",
      "Epoch 102: MSE: 7.26014, MAE: 1.7314515\n",
      "Epoch 103: MSE: 7.177046, MAE: 1.7358488\n",
      "Epoch 104: MSE: 7.162856, MAE: 1.7320004\n",
      "Epoch 105: MSE: 7.070704, MAE: 1.7177202\n",
      "Epoch 106: MSE: 7.0217896, MAE: 1.7219626\n",
      "Epoch 107: MSE: 7.052513, MAE: 1.7296798\n",
      "Epoch 108: MSE: 6.9803, MAE: 1.7190554\n",
      "Epoch 109: MSE: 6.943829, MAE: 1.702887\n",
      "Epoch 110: MSE: 6.649411, MAE: 1.6888506\n",
      "Epoch 111: MSE: 7.0444803, MAE: 1.7128987\n",
      "Epoch 112: MSE: 6.8679056, MAE: 1.7006174\n",
      "Epoch 113: MSE: 6.8445387, MAE: 1.6848112\n",
      "Epoch 114: MSE: 6.648048, MAE: 1.6887215\n",
      "Epoch 115: MSE: 6.752276, MAE: 1.6831361\n",
      "Epoch 116: MSE: 6.62366, MAE: 1.6796944\n",
      "Epoch 117: MSE: 6.641207, MAE: 1.6808883\n",
      "Epoch 118: MSE: 6.630549, MAE: 1.6725593\n",
      "Epoch 119: MSE: 6.6130395, MAE: 1.6547061\n",
      "Epoch 120: MSE: 6.4226065, MAE: 1.6533406\n",
      "Epoch 121: MSE: 6.456703, MAE: 1.6565001\n",
      "Epoch 122: MSE: 6.433858, MAE: 1.6564969\n",
      "Epoch 123: MSE: 6.407213, MAE: 1.6568936\n",
      "Epoch 124: MSE: 6.4258204, MAE: 1.6502501\n",
      "Epoch 125: MSE: 6.334686, MAE: 1.6510056\n",
      "Epoch 126: MSE: 6.2329745, MAE: 1.637423\n",
      "Epoch 127: MSE: 6.3138275, MAE: 1.6533682\n",
      "Epoch 128: MSE: 6.2855015, MAE: 1.6378529\n",
      "Epoch 129: MSE: 6.210642, MAE: 1.6310813\n",
      "Epoch 130: MSE: 6.1969547, MAE: 1.6340369\n",
      "Epoch 131: MSE: 6.1089935, MAE: 1.6306975\n",
      "Epoch 132: MSE: 6.144628, MAE: 1.6338216\n",
      "Epoch 133: MSE: 6.2048154, MAE: 1.6149248\n",
      "Epoch 134: MSE: 5.9772024, MAE: 1.6080804\n",
      "Epoch 135: MSE: 6.1883116, MAE: 1.6172976\n",
      "Epoch 136: MSE: 6.110381, MAE: 1.6167642\n",
      "Epoch 137: MSE: 6.010376, MAE: 1.6085805\n",
      "Epoch 138: MSE: 6.1714435, MAE: 1.6158849\n",
      "Epoch 139: MSE: 5.823538, MAE: 1.5793669\n",
      "Epoch 140: MSE: 5.872382, MAE: 1.5892473\n",
      "Epoch 141: MSE: 5.91161, MAE: 1.5891997\n",
      "Epoch 142: MSE: 5.9709573, MAE: 1.5909284\n",
      "Epoch 143: MSE: 5.8000126, MAE: 1.5917575\n",
      "Epoch 144: MSE: 5.7460685, MAE: 1.5770683\n",
      "Epoch 145: MSE: 5.701932, MAE: 1.5868874\n",
      "Epoch 146: MSE: 5.9139304, MAE: 1.5825775\n",
      "Epoch 147: MSE: 5.6051707, MAE: 1.5797471\n",
      "Epoch 148: MSE: 5.875836, MAE: 1.5808226\n",
      "Epoch 149: MSE: 5.6511755, MAE: 1.5720077\n",
      "Epoch 150: MSE: 5.6673055, MAE: 1.5537155\n",
      "Epoch 151: MSE: 5.436082, MAE: 1.5481915\n",
      "Epoch 152: MSE: 5.6877103, MAE: 1.5617491\n",
      "Epoch 153: MSE: 5.628059, MAE: 1.5571738\n",
      "Epoch 154: MSE: 5.626007, MAE: 1.5563896\n",
      "Epoch 155: MSE: 5.3749614, MAE: 1.538588\n",
      "Epoch 156: MSE: 5.6698985, MAE: 1.5531154\n",
      "Epoch 157: MSE: 5.600678, MAE: 1.5417544\n",
      "Epoch 158: MSE: 5.5305166, MAE: 1.5474855\n",
      "Epoch 159: MSE: 5.6499557, MAE: 1.5390756\n",
      "Epoch 160: MSE: 5.494997, MAE: 1.5368478\n",
      "Epoch 161: MSE: 5.5171027, MAE: 1.5354116\n",
      "Epoch 162: MSE: 5.431432, MAE: 1.5338364\n",
      "Epoch 163: MSE: 5.432602, MAE: 1.5302716\n",
      "Epoch 164: MSE: 5.2929482, MAE: 1.5235674\n",
      "Epoch 165: MSE: 5.4268756, MAE: 1.526105\n",
      "Epoch 166: MSE: 5.324685, MAE: 1.5174291\n",
      "Epoch 167: MSE: 5.424673, MAE: 1.5096983\n",
      "Epoch 168: MSE: 5.352504, MAE: 1.518113\n",
      "Epoch 169: MSE: 5.299596, MAE: 1.5006354\n",
      "Epoch 170: MSE: 5.304209, MAE: 1.5089368\n",
      "Epoch 171: MSE: 5.2134027, MAE: 1.4973748\n",
      "Epoch 172: MSE: 5.1795425, MAE: 1.4884752\n",
      "Epoch 173: MSE: 5.2239575, MAE: 1.4954902\n",
      "Epoch 174: MSE: 5.2558417, MAE: 1.494154\n",
      "Epoch 175: MSE: 5.1061425, MAE: 1.4886137\n",
      "Epoch 176: MSE: 5.300076, MAE: 1.4811167\n",
      "Epoch 177: MSE: 5.113111, MAE: 1.4813225\n",
      "Epoch 178: MSE: 5.021101, MAE: 1.4745648\n",
      "Epoch 179: MSE: 5.169693, MAE: 1.4824383\n",
      "Epoch 180: MSE: 5.1424747, MAE: 1.4850249\n",
      "Epoch 181: MSE: 4.9100895, MAE: 1.4750587\n",
      "Epoch 182: MSE: 5.0533776, MAE: 1.4775999\n",
      "Epoch 183: MSE: 5.0121536, MAE: 1.4570934\n",
      "Epoch 184: MSE: 5.02073, MAE: 1.4527563\n",
      "Epoch 185: MSE: 5.0719914, MAE: 1.4664649\n",
      "Epoch 186: MSE: 4.850752, MAE: 1.4495599\n",
      "Epoch 187: MSE: 4.95256, MAE: 1.4503601\n",
      "Epoch 188: MSE: 4.8633094, MAE: 1.4435827\n",
      "Epoch 189: MSE: 4.791616, MAE: 1.4421271\n",
      "Epoch 190: MSE: 4.9192066, MAE: 1.441171\n",
      "Epoch 191: MSE: 4.803857, MAE: 1.4348005\n",
      "Epoch 192: MSE: 4.9373603, MAE: 1.4393134\n",
      "Epoch 193: MSE: 4.925969, MAE: 1.4476466\n",
      "Epoch 194: MSE: 4.8589225, MAE: 1.4318777\n",
      "Epoch 195: MSE: 4.751917, MAE: 1.4256275\n",
      "Epoch 196: MSE: 4.895515, MAE: 1.4305431\n",
      "Epoch 197: MSE: 4.889884, MAE: 1.4355125\n",
      "Epoch 198: MSE: 4.758949, MAE: 1.4231689\n",
      "Epoch 199: MSE: 4.628492, MAE: 1.408924\n",
      "Epoch 200: MSE: 4.5735435, MAE: 1.411225\n",
      "Epoch 201: MSE: 4.7479243, MAE: 1.4159396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 202: MSE: 4.773335, MAE: 1.4187578\n",
      "Epoch 203: MSE: 4.700536, MAE: 1.4117839\n",
      "Epoch 204: MSE: 4.821341, MAE: 1.4132253\n",
      "Epoch 205: MSE: 4.633688, MAE: 1.4077339\n",
      "Epoch 206: MSE: 4.5749235, MAE: 1.4185086\n",
      "Epoch 207: MSE: 4.7668104, MAE: 1.4001565\n",
      "Epoch 208: MSE: 4.6548786, MAE: 1.3993344\n",
      "Epoch 209: MSE: 4.607834, MAE: 1.4057206\n",
      "Epoch 210: MSE: 4.7243176, MAE: 1.402136\n",
      "Epoch 211: MSE: 4.5674067, MAE: 1.4045955\n",
      "Epoch 212: MSE: 4.3653684, MAE: 1.3834531\n",
      "Epoch 213: MSE: 4.518463, MAE: 1.3781703\n",
      "Epoch 214: MSE: 4.5822277, MAE: 1.3813382\n",
      "Epoch 215: MSE: 4.346852, MAE: 1.3678825\n",
      "Epoch 216: MSE: 4.326457, MAE: 1.3724117\n",
      "Epoch 217: MSE: 4.6579447, MAE: 1.3765104\n",
      "Epoch 218: MSE: 4.439329, MAE: 1.3778381\n",
      "Epoch 219: MSE: 4.326722, MAE: 1.3732371\n",
      "Epoch 220: MSE: 4.5309215, MAE: 1.3793356\n",
      "Epoch 221: MSE: 4.4751353, MAE: 1.371872\n",
      "Epoch 222: MSE: 4.3121037, MAE: 1.3609478\n",
      "Epoch 223: MSE: 4.2852054, MAE: 1.3417854\n",
      "Epoch 224: MSE: 4.308532, MAE: 1.3477483\n",
      "Epoch 225: MSE: 4.2976003, MAE: 1.3700719\n",
      "Epoch 226: MSE: 4.146439, MAE: 1.3501675\n",
      "Epoch 227: MSE: 4.3719177, MAE: 1.345155\n",
      "Epoch 228: MSE: 4.174986, MAE: 1.3576888\n",
      "Epoch 229: MSE: 4.348545, MAE: 1.3502628\n",
      "Epoch 230: MSE: 4.216318, MAE: 1.3444653\n",
      "Epoch 231: MSE: 4.181237, MAE: 1.3351305\n",
      "Epoch 232: MSE: 4.166325, MAE: 1.3332547\n",
      "Epoch 233: MSE: 4.094218, MAE: 1.3386803\n",
      "Epoch 234: MSE: 4.2123103, MAE: 1.3209035\n",
      "Epoch 235: MSE: 4.294212, MAE: 1.3434862\n",
      "Epoch 236: MSE: 4.2004175, MAE: 1.3348674\n",
      "Epoch 237: MSE: 4.15828, MAE: 1.3381855\n",
      "Epoch 238: MSE: 4.0009418, MAE: 1.328265\n",
      "Epoch 239: MSE: 4.2675076, MAE: 1.3165114\n",
      "Epoch 240: MSE: 4.082375, MAE: 1.324374\n",
      "Epoch 241: MSE: 4.0763803, MAE: 1.31832\n",
      "Epoch 242: MSE: 3.9503036, MAE: 1.317418\n",
      "Epoch 243: MSE: 4.0045238, MAE: 1.3114009\n",
      "Epoch 244: MSE: 4.1414404, MAE: 1.3149428\n",
      "Epoch 245: MSE: 3.973147, MAE: 1.3076292\n",
      "Epoch 246: MSE: 4.033638, MAE: 1.2868037\n",
      "Epoch 247: MSE: 4.0474343, MAE: 1.3054085\n",
      "Epoch 248: MSE: 4.0043836, MAE: 1.2973914\n",
      "Epoch 249: MSE: 3.935832, MAE: 1.3098922\n",
      "Epoch 250: MSE: 4.017684, MAE: 1.3018409\n",
      "Epoch 251: MSE: 4.077695, MAE: 1.3011254\n",
      "Epoch 252: MSE: 3.9124255, MAE: 1.3047174\n",
      "Epoch 253: MSE: 3.9627473, MAE: 1.304744\n",
      "Epoch 254: MSE: 3.8834443, MAE: 1.2835004\n",
      "Epoch 255: MSE: 3.9420755, MAE: 1.3043823\n",
      "Epoch 256: MSE: 3.866109, MAE: 1.281256\n",
      "Epoch 257: MSE: 3.9127069, MAE: 1.2923459\n",
      "Epoch 258: MSE: 3.9876134, MAE: 1.2859992\n",
      "Epoch 259: MSE: 3.8149784, MAE: 1.269727\n",
      "Epoch 260: MSE: 3.76717, MAE: 1.27114\n",
      "Epoch 261: MSE: 3.944421, MAE: 1.2754909\n",
      "Epoch 262: MSE: 3.7876456, MAE: 1.2726977\n",
      "Epoch 263: MSE: 3.7207818, MAE: 1.2767721\n",
      "Epoch 264: MSE: 3.869825, MAE: 1.2680727\n",
      "Epoch 265: MSE: 3.9135103, MAE: 1.2656877\n",
      "Epoch 266: MSE: 3.776164, MAE: 1.2674819\n",
      "Epoch 267: MSE: 3.7297678, MAE: 1.2457589\n",
      "Epoch 268: MSE: 3.7078428, MAE: 1.2539604\n",
      "Epoch 269: MSE: 3.6443963, MAE: 1.2456778\n",
      "Epoch 270: MSE: 3.7402186, MAE: 1.252295\n",
      "Epoch 271: MSE: 3.866295, MAE: 1.2531844\n",
      "Epoch 272: MSE: 3.6399343, MAE: 1.2356637\n",
      "Epoch 273: MSE: 3.7316792, MAE: 1.2595522\n",
      "Epoch 274: MSE: 3.7034376, MAE: 1.2368262\n",
      "Epoch 275: MSE: 3.5090065, MAE: 1.2167753\n",
      "Epoch 276: MSE: 3.701242, MAE: 1.2316394\n",
      "Epoch 277: MSE: 3.7026622, MAE: 1.2387046\n",
      "Epoch 278: MSE: 3.5521214, MAE: 1.2303642\n",
      "Epoch 279: MSE: 3.6387885, MAE: 1.2326744\n",
      "Epoch 280: MSE: 3.536302, MAE: 1.2252693\n",
      "Epoch 281: MSE: 3.7262259, MAE: 1.2346528\n",
      "Epoch 282: MSE: 3.601924, MAE: 1.2203839\n",
      "Epoch 283: MSE: 3.6404643, MAE: 1.2202835\n",
      "Epoch 284: MSE: 3.6998649, MAE: 1.2183832\n",
      "Epoch 285: MSE: 3.550999, MAE: 1.212089\n",
      "Epoch 286: MSE: 3.615669, MAE: 1.2138473\n",
      "Epoch 287: MSE: 3.3617272, MAE: 1.1982586\n",
      "Epoch 288: MSE: 3.4457746, MAE: 1.2133918\n",
      "Epoch 289: MSE: 3.3936357, MAE: 1.1951181\n",
      "Epoch 290: MSE: 3.29379, MAE: 1.1825848\n",
      "Epoch 291: MSE: 3.5212958, MAE: 1.2035874\n",
      "Epoch 292: MSE: 3.5913057, MAE: 1.2000066\n",
      "Epoch 293: MSE: 3.511878, MAE: 1.1896738\n",
      "Epoch 294: MSE: 3.3307076, MAE: 1.1825618\n",
      "Epoch 295: MSE: 3.359915, MAE: 1.1826851\n",
      "Epoch 296: MSE: 3.4002686, MAE: 1.1709346\n",
      "Epoch 297: MSE: 3.3919413, MAE: 1.1804162\n",
      "Epoch 298: MSE: 3.2708783, MAE: 1.1787915\n",
      "Epoch 299: MSE: 3.3999906, MAE: 1.1692793\n",
      "Epoch 300: MSE: 3.1262991, MAE: 1.1567926\n",
      "Epoch 301: MSE: 3.349739, MAE: 1.1693552\n",
      "Epoch 302: MSE: 3.2059414, MAE: 1.153744\n",
      "Epoch 303: MSE: 3.3943002, MAE: 1.166709\n",
      "Epoch 304: MSE: 3.3941367, MAE: 1.157678\n",
      "Epoch 305: MSE: 3.1005974, MAE: 1.1554763\n",
      "Epoch 306: MSE: 3.442045, MAE: 1.1720438\n",
      "Epoch 307: MSE: 3.2307942, MAE: 1.1496754\n",
      "Epoch 308: MSE: 3.3130147, MAE: 1.1413112\n",
      "Epoch 309: MSE: 3.1323404, MAE: 1.1330068\n",
      "Epoch 310: MSE: 3.155407, MAE: 1.1401124\n",
      "Epoch 311: MSE: 3.0863454, MAE: 1.1399345\n",
      "Epoch 312: MSE: 3.270715, MAE: 1.1470916\n",
      "Epoch 313: MSE: 3.3932436, MAE: 1.1401055\n",
      "Epoch 314: MSE: 3.1710103, MAE: 1.1285924\n",
      "Epoch 315: MSE: 3.2417872, MAE: 1.1384896\n",
      "Epoch 316: MSE: 3.1412916, MAE: 1.1240606\n",
      "Epoch 317: MSE: 3.067514, MAE: 1.1306674\n",
      "Epoch 318: MSE: 3.1932652, MAE: 1.1275052\n",
      "Epoch 319: MSE: 3.0375416, MAE: 1.1265563\n",
      "Epoch 320: MSE: 3.1799827, MAE: 1.1326613\n",
      "Epoch 321: MSE: 3.0889812, MAE: 1.1147943\n",
      "Epoch 322: MSE: 3.0020158, MAE: 1.1122763\n",
      "Epoch 323: MSE: 3.091976, MAE: 1.1096928\n",
      "Epoch 324: MSE: 3.2188902, MAE: 1.1200312\n",
      "Epoch 325: MSE: 3.0580876, MAE: 1.1233268\n",
      "Epoch 326: MSE: 2.955091, MAE: 1.0980523\n",
      "Epoch 327: MSE: 3.0123873, MAE: 1.1130903\n",
      "Epoch 328: MSE: 3.1039221, MAE: 1.1097028\n",
      "Epoch 329: MSE: 2.9959552, MAE: 1.0940967\n",
      "Epoch 330: MSE: 3.0402586, MAE: 1.0953776\n",
      "Epoch 331: MSE: 3.046949, MAE: 1.100971\n",
      "Epoch 332: MSE: 2.9712393, MAE: 1.0939783\n",
      "Epoch 333: MSE: 2.9837358, MAE: 1.0967282\n",
      "Epoch 334: MSE: 2.9115603, MAE: 1.0975565\n",
      "Epoch 335: MSE: 3.0422919, MAE: 1.0888631\n",
      "Epoch 336: MSE: 2.8157587, MAE: 1.0903426\n",
      "Epoch 337: MSE: 3.062434, MAE: 1.0775901\n",
      "Epoch 338: MSE: 3.0292215, MAE: 1.0948882\n",
      "Epoch 339: MSE: 2.8331194, MAE: 1.0784253\n",
      "Epoch 340: MSE: 2.9718578, MAE: 1.08884\n",
      "Epoch 341: MSE: 2.9137547, MAE: 1.0764469\n",
      "Epoch 342: MSE: 2.8389115, MAE: 1.0601426\n",
      "Epoch 343: MSE: 2.9148703, MAE: 1.0620041\n",
      "Epoch 344: MSE: 2.9275393, MAE: 1.0723004\n",
      "Epoch 345: MSE: 2.9507306, MAE: 1.0696429\n",
      "Epoch 346: MSE: 2.872056, MAE: 1.0671929\n",
      "Epoch 347: MSE: 2.735327, MAE: 1.0526694\n",
      "Epoch 348: MSE: 2.8785667, MAE: 1.065547\n",
      "Epoch 349: MSE: 2.868364, MAE: 1.059408\n",
      "Epoch 350: MSE: 2.7527425, MAE: 1.0554373\n",
      "Epoch 351: MSE: 2.959056, MAE: 1.0569253\n",
      "Epoch 352: MSE: 2.732915, MAE: 1.0545583\n",
      "Epoch 353: MSE: 2.9129815, MAE: 1.0571967\n",
      "Epoch 354: MSE: 2.7133837, MAE: 1.045409\n",
      "Epoch 355: MSE: 2.7423272, MAE: 1.0616748\n",
      "Epoch 356: MSE: 2.9071543, MAE: 1.0478705\n",
      "Epoch 357: MSE: 2.77297, MAE: 1.037543\n",
      "Epoch 358: MSE: 2.667568, MAE: 1.0350497\n",
      "Epoch 359: MSE: 2.854636, MAE: 1.0490968\n",
      "Epoch 360: MSE: 2.8741293, MAE: 1.0392818\n",
      "Epoch 361: MSE: 2.6206274, MAE: 1.0393834\n",
      "Epoch 362: MSE: 2.6694248, MAE: 1.0398959\n",
      "Epoch 363: MSE: 2.693741, MAE: 1.02614\n",
      "Epoch 364: MSE: 2.7199564, MAE: 1.0343846\n",
      "Epoch 365: MSE: 2.6934001, MAE: 1.0299394\n",
      "Epoch 366: MSE: 2.713731, MAE: 1.0269902\n",
      "Epoch 367: MSE: 2.640923, MAE: 1.024656\n",
      "Epoch 368: MSE: 2.6436884, MAE: 1.0195711\n",
      "Epoch 369: MSE: 2.6520972, MAE: 1.0142397\n",
      "Epoch 370: MSE: 2.6666374, MAE: 1.0194323\n",
      "Epoch 371: MSE: 2.5879793, MAE: 1.0195091\n",
      "Epoch 372: MSE: 2.8154504, MAE: 1.0215658\n",
      "Epoch 373: MSE: 2.5601263, MAE: 1.0165111\n",
      "Epoch 374: MSE: 2.5702438, MAE: 1.0108248\n",
      "Epoch 375: MSE: 2.4994943, MAE: 1.0109776\n",
      "Epoch 376: MSE: 2.648956, MAE: 1.0129352\n",
      "Epoch 377: MSE: 2.560132, MAE: 1.0028719\n",
      "Epoch 378: MSE: 2.5787663, MAE: 1.0005193\n",
      "Epoch 379: MSE: 2.420923, MAE: 0.98645425\n",
      "Epoch 380: MSE: 2.752917, MAE: 1.0066292\n",
      "Epoch 381: MSE: 2.510467, MAE: 0.9840024\n",
      "Epoch 382: MSE: 2.597506, MAE: 1.0061017\n",
      "Epoch 383: MSE: 2.5937142, MAE: 0.9879351\n",
      "Epoch 384: MSE: 2.608842, MAE: 0.99246603\n",
      "Epoch 385: MSE: 2.4756596, MAE: 0.9857104\n",
      "Epoch 386: MSE: 2.5081592, MAE: 0.97810626\n",
      "Epoch 387: MSE: 2.4499831, MAE: 0.98572403\n",
      "Epoch 388: MSE: 2.609336, MAE: 0.9930732\n",
      "Epoch 389: MSE: 2.4941149, MAE: 0.98998445\n",
      "Epoch 390: MSE: 2.5032997, MAE: 0.9771467\n",
      "Epoch 391: MSE: 2.3330357, MAE: 0.98628145\n",
      "Epoch 392: MSE: 2.6970372, MAE: 0.9833956\n",
      "Epoch 393: MSE: 2.357159, MAE: 0.9742845\n",
      "Epoch 394: MSE: 2.3462281, MAE: 0.97432137\n",
      "Epoch 395: MSE: 2.6195543, MAE: 0.97967374\n",
      "Epoch 396: MSE: 2.3279498, MAE: 0.9696797\n",
      "Epoch 397: MSE: 2.4120643, MAE: 0.9618582\n",
      "Epoch 398: MSE: 2.407881, MAE: 0.9868301\n",
      "Epoch 399: MSE: 2.461713, MAE: 0.9757636\n",
      "Epoch 400: MSE: 2.4757016, MAE: 0.97522587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 401: MSE: 2.3579879, MAE: 0.96951634\n",
      "Epoch 402: MSE: 2.3830657, MAE: 0.964865\n",
      "Epoch 403: MSE: 2.4054065, MAE: 0.9535614\n",
      "Epoch 404: MSE: 2.505754, MAE: 0.9732986\n",
      "Epoch 405: MSE: 2.350547, MAE: 0.95539606\n",
      "Epoch 406: MSE: 2.2730005, MAE: 0.9564965\n",
      "Epoch 407: MSE: 2.4066305, MAE: 0.9482577\n",
      "Epoch 408: MSE: 2.4342864, MAE: 0.9730834\n",
      "Epoch 409: MSE: 2.3175821, MAE: 0.9546349\n",
      "Epoch 410: MSE: 2.2335894, MAE: 0.9459655\n",
      "Epoch 411: MSE: 2.2834647, MAE: 0.9526716\n",
      "Epoch 412: MSE: 2.345399, MAE: 0.94189507\n",
      "Epoch 413: MSE: 2.3000488, MAE: 0.96481264\n",
      "Epoch 414: MSE: 2.2842436, MAE: 0.9374243\n",
      "Epoch 415: MSE: 2.3539307, MAE: 0.9442634\n",
      "Epoch 416: MSE: 2.3343625, MAE: 0.9295705\n",
      "Epoch 417: MSE: 2.3314943, MAE: 0.9519411\n",
      "Epoch 418: MSE: 2.275968, MAE: 0.9440047\n",
      "Epoch 419: MSE: 2.2362616, MAE: 0.92421794\n",
      "Epoch 420: MSE: 2.1771753, MAE: 0.9354896\n",
      "Epoch 421: MSE: 2.2149634, MAE: 0.93411595\n",
      "Epoch 422: MSE: 2.148635, MAE: 0.93064976\n",
      "Epoch 423: MSE: 2.168746, MAE: 0.9246274\n",
      "Epoch 424: MSE: 2.3473446, MAE: 0.93514943\n",
      "Epoch 425: MSE: 2.3753555, MAE: 0.93262887\n",
      "Epoch 426: MSE: 2.2101173, MAE: 0.92531484\n",
      "Epoch 427: MSE: 2.21794, MAE: 0.9304397\n",
      "Epoch 428: MSE: 2.3082407, MAE: 0.9386996\n",
      "Epoch 429: MSE: 2.1815045, MAE: 0.91412187\n",
      "Epoch 430: MSE: 2.2295864, MAE: 0.9123628\n",
      "Epoch 431: MSE: 2.1872027, MAE: 0.9167845\n",
      "Epoch 432: MSE: 2.2104216, MAE: 0.91354823\n",
      "Epoch 433: MSE: 2.2893224, MAE: 0.9262145\n",
      "Epoch 434: MSE: 2.2515407, MAE: 0.92303824\n",
      "Epoch 435: MSE: 2.1615624, MAE: 0.91599727\n",
      "Epoch 436: MSE: 2.0739403, MAE: 0.9083906\n",
      "Epoch 437: MSE: 2.1576216, MAE: 0.9088344\n",
      "Epoch 438: MSE: 2.1037092, MAE: 0.9262034\n",
      "Epoch 439: MSE: 2.0821466, MAE: 0.9123459\n",
      "Epoch 440: MSE: 2.0680974, MAE: 0.90298325\n",
      "Epoch 441: MSE: 2.129204, MAE: 0.8973291\n",
      "Epoch 442: MSE: 2.1539056, MAE: 0.89941525\n",
      "Epoch 443: MSE: 2.127308, MAE: 0.9123002\n",
      "Epoch 444: MSE: 2.02242, MAE: 0.88775676\n",
      "Epoch 445: MSE: 2.1630437, MAE: 0.8981379\n",
      "Epoch 446: MSE: 2.1040242, MAE: 0.8953314\n",
      "Epoch 447: MSE: 2.2056112, MAE: 0.90883815\n",
      "Epoch 448: MSE: 2.1022246, MAE: 0.89394665\n",
      "Epoch 449: MSE: 2.135192, MAE: 0.9109638\n",
      "Epoch 450: MSE: 1.9462051, MAE: 0.88544774\n",
      "Epoch 451: MSE: 2.1088128, MAE: 0.88854736\n",
      "Epoch 452: MSE: 2.0835721, MAE: 0.8869574\n",
      "Epoch 453: MSE: 2.101241, MAE: 0.8857938\n",
      "Epoch 454: MSE: 2.0651224, MAE: 0.8977549\n",
      "Epoch 455: MSE: 2.138751, MAE: 0.88559777\n",
      "Epoch 456: MSE: 2.0695024, MAE: 0.8855166\n",
      "Epoch 457: MSE: 1.9677281, MAE: 0.88130945\n",
      "Epoch 458: MSE: 2.1726937, MAE: 0.8931582\n",
      "Epoch 459: MSE: 1.976738, MAE: 0.89446366\n",
      "Epoch 460: MSE: 2.04629, MAE: 0.8721183\n",
      "Epoch 461: MSE: 2.0449965, MAE: 0.881393\n",
      "Epoch 462: MSE: 2.1238406, MAE: 0.878903\n",
      "Epoch 463: MSE: 1.9283658, MAE: 0.86551106\n",
      "Epoch 464: MSE: 2.156202, MAE: 0.8834352\n",
      "Epoch 465: MSE: 1.9466345, MAE: 0.86934817\n",
      "Epoch 466: MSE: 1.8564318, MAE: 0.8737891\n",
      "Epoch 467: MSE: 1.9519048, MAE: 0.88812584\n",
      "Epoch 468: MSE: 2.178375, MAE: 0.87986565\n",
      "Epoch 469: MSE: 1.8362912, MAE: 0.8596295\n",
      "Epoch 470: MSE: 2.044242, MAE: 0.8758834\n",
      "Epoch 471: MSE: 2.096727, MAE: 0.8976791\n",
      "Epoch 472: MSE: 2.1623487, MAE: 0.8801479\n",
      "Epoch 473: MSE: 1.9101737, MAE: 0.8493199\n",
      "Epoch 474: MSE: 1.8782117, MAE: 0.85920435\n",
      "Epoch 475: MSE: 2.006999, MAE: 0.8600204\n",
      "Epoch 476: MSE: 2.0149903, MAE: 0.8806181\n",
      "Epoch 477: MSE: 1.8504564, MAE: 0.85541475\n",
      "Epoch 478: MSE: 1.9394094, MAE: 0.84759074\n",
      "Epoch 479: MSE: 1.7797167, MAE: 0.852943\n",
      "Epoch 480: MSE: 1.9381602, MAE: 0.87410396\n",
      "Epoch 481: MSE: 1.8972511, MAE: 0.863898\n",
      "Epoch 482: MSE: 1.7981951, MAE: 0.8520068\n",
      "Epoch 483: MSE: 1.9977422, MAE: 0.8468538\n",
      "Epoch 484: MSE: 1.9366816, MAE: 0.86585265\n",
      "Epoch 485: MSE: 1.8887538, MAE: 0.8565419\n",
      "Epoch 486: MSE: 1.9618928, MAE: 0.850125\n",
      "Epoch 487: MSE: 1.8263365, MAE: 0.8487259\n",
      "Epoch 488: MSE: 1.8920363, MAE: 0.8501805\n",
      "Epoch 489: MSE: 1.8555921, MAE: 0.85031617\n",
      "Epoch 490: MSE: 1.9669037, MAE: 0.8577422\n",
      "Epoch 491: MSE: 1.7448733, MAE: 0.8331452\n",
      "Epoch 492: MSE: 1.8521606, MAE: 0.849561\n",
      "Epoch 493: MSE: 1.9292489, MAE: 0.856562\n",
      "Epoch 494: MSE: 1.8779296, MAE: 0.8362415\n",
      "Epoch 495: MSE: 1.9792804, MAE: 0.8462254\n",
      "Epoch 496: MSE: 1.7579472, MAE: 0.84972584\n",
      "Epoch 497: MSE: 1.9312685, MAE: 0.84347177\n",
      "Epoch 498: MSE: 1.9644097, MAE: 0.8536539\n",
      "Epoch 499: MSE: 1.7869353, MAE: 0.824769\n",
      "Epoch 500: MSE: 1.8284417, MAE: 0.84563345\n"
     ]
    }
   ],
   "source": [
    "for epoch in 1...epochCount {\n",
    "    var epochLoss: Float = 0\n",
    "    var epochMAE: Float = 0\n",
    "    var batchCount: Int = 0\n",
    "    var batchArray = Array(repeating: false, count: numberOfBatch)\n",
    "    for batch in 0..<numberOfBatch {\n",
    "        var r = batch\n",
    "        if shuffle {\n",
    "            while true {\n",
    "                r = Int.random(in: 0..<numberOfBatch)\n",
    "                if !batchArray[r] {\n",
    "                    batchArray[r] = true\n",
    "                    break\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        let batchStart = r * batchSize\n",
    "        let batchEnd = min(numTrainRecords, batchStart + batchSize)\n",
    "        let (loss, grad) = model.valueWithGradient { (model: RegressionModel) -> Tensor<Float> in\n",
    "            let multiInput = MultiInputs(numerical: [XNumericalTrain[batchStart..<batchEnd]],\n",
    "                                         categorical: [XCategoricalTrain[0][batchStart..<batchEnd],\n",
    "                                                       XCategoricalTrain[1][batchStart..<batchEnd]])\n",
    "            let logits = model(multiInput)\n",
    "            return meanSquaredError(predicted: logits, expected: YTrain[batchStart..<batchEnd])\n",
    "        }\n",
    "        optimizer.update(&model, along: grad)\n",
    "        \n",
    "        let multiInput = MultiInputs(numerical: [XNumericalTrain[batchStart..<batchEnd]],\n",
    "                                     categorical: [XCategoricalTrain[0][batchStart..<batchEnd],\n",
    "                                                   XCategoricalTrain[1][batchStart..<batchEnd]])\n",
    "        let logits = model(multiInput)\n",
    "        epochMAE += mae(predictions: logits, truths: YTrain[batchStart..<batchEnd])\n",
    "        epochLoss += loss.scalarized()\n",
    "        batchCount += 1\n",
    "    }\n",
    "    epochMAE /= Float(batchCount)\n",
    "    epochLoss /= Float(batchCount)\n",
    "\n",
    "    print(\"Epoch \\(epoch): MSE: \\(epochLoss), MAE: \\(epochMAE)\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.10507653, MAE: 0.02263178\r\n"
     ]
    }
   ],
   "source": [
    "Context.local.learningPhase = .inference\n",
    "\n",
    "let multiInputTest = MultiInputs(numerical: [XNumericalTest],\n",
    "                                 categorical: [XCategoricalTest[0],\n",
    "                                               XCategoricalTest[1]])\n",
    "\n",
    "let prediction = model(multiInputTest)\n",
    "\n",
    "let predictionMse = meanSquaredError(predicted: prediction, expected: YTest).scalarized()/Float(numTestRecords)\n",
    "let predictionMae = mae(predictions: prediction, truths: YTest)/Float(numTestRecords)\n",
    "\n",
    "print(\"MSE: \\(predictionMse), MAE: \\(predictionMae)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fkf29HLlohIP"
   },
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2] [9, 5]\r\n"
     ]
    }
   ],
   "source": [
    "print(model.embedding1.embeddings.shape, model.embedding2.embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "let coremlModel = Model(version: 4,\n",
    "                        shortDescription: \"Regression\",\n",
    "                        author: \"Jacopo Mangiavacchi\",\n",
    "                        license: \"MIT\",\n",
    "                        userDefined: [\"SwiftCoremltoolsVersion\" : \"0.0.6\"]) {\n",
    "    Input(name: \"numericalInput\", shape: [11])\n",
    "    Input(name: \"categoricalInput1\", shape: [1])\n",
    "    Input(name: \"categoricalInput2\", shape: [1])\n",
    "    Output(name: \"output\", shape: [1])\n",
    "    NeuralNetwork {\n",
    "        Embedding(name: \"embedding1\",\n",
    "                     input: [\"categoricalInput1\"],\n",
    "                     output: [\"outEmbedding1\"],\n",
    "                     weight: model.embedding1.embeddings.transposed().flattened().scalars,\n",
    "                     inputDim: 2,\n",
    "                     outputChannels: 2)\n",
    "        Permute(name: \"permute1\",\n",
    "                     input: [\"outEmbedding1\"],\n",
    "                     output: [\"outPermute1\"],\n",
    "                     axis: [2, 1, 0, 3])\n",
    "        Flatten(name: \"flatten1\",\n",
    "                     input: [\"outPermute1\"],\n",
    "                     output: [\"outFlatten1\"],\n",
    "                     mode: .last)\n",
    "        Embedding(name: \"embedding2\",\n",
    "                     input: [\"categoricalInput2\"],\n",
    "                     output: [\"outEmbedding2\"],\n",
    "                     weight: model.embedding2.embeddings.transposed().flattened().scalars,\n",
    "                     inputDim: 9,\n",
    "                     outputChannels: 5)\n",
    "        Permute(name: \"permute2\",\n",
    "                     input: [\"outEmbedding2\"],\n",
    "                     output: [\"outPermute2\"],\n",
    "                     axis: [2, 1, 0, 3])\n",
    "        Flatten(name: \"flatten2\",\n",
    "                     input: [\"outPermute2\"],\n",
    "                     output: [\"outFlatten2\"],\n",
    "                     mode: .last)\n",
    "        Concat(name: \"concat\",\n",
    "                     input: [\"numericalInput\", \"outFlatten1\", \"outFlatten2\"],\n",
    "                     output: [\"outConcat\"])\n",
    "        InnerProduct(name: \"dense1\",\n",
    "                     input: [\"outConcat\"],\n",
    "                     output: [\"outDense1\"],\n",
    "                     weight: model.allInputConcatLayer.weight.transposed().flattened().scalars,\n",
    "                     bias: model.allInputConcatLayer.bias.flattened().scalars,\n",
    "                     inputChannels: 11 + 2 + 5,\n",
    "                     outputChannels: 64)\n",
    "        ReLu(name: \"Relu1\",\n",
    "             input: [\"outDense1\"],\n",
    "             output: [\"outRelu1\"])\n",
    "        InnerProduct(name: \"dense2\",\n",
    "                     input: [\"outRelu1\"],\n",
    "                     output: [\"outDense2\"],\n",
    "                     weight: model.hiddenLayer.weight.transposed().flattened().scalars,\n",
    "                     bias: model.hiddenLayer.bias.flattened().scalars,\n",
    "                     inputChannels: 64,\n",
    "                     outputChannels: 32)\n",
    "        ReLu(name: \"Relu2\",\n",
    "             input: [\"outDense2\"],\n",
    "             output: [\"outRelu2\"])\n",
    "        InnerProduct(name: \"dense3\",\n",
    "                     input: [\"outRelu2\"],\n",
    "                     output: [\"output\"],\n",
    "                     weight: model.outputLayer.weight.transposed().flattened().scalars,\n",
    "                     bias: model.outputLayer.bias.flattened().scalars,\n",
    "                     inputChannels: 32,\n",
    "                     outputChannels: 1)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "let coreMLData = coremlModel.coreMLData\n",
    "try! coreMLData!.write(to: URL(fileURLWithPath: \"./s4tf_house_simplified_trained_model.mlmodel\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.012596, -0.4982311,  1.0273249,  1.6332408, -0.6422595,  0.8484324, -0.9169787,  1.5628325,\r\n",
      " 0.80114645, 0.37964535,  1.2587203] [0] [8] => [10.5]\r\n",
      "[ -0.3916621,  -0.4982311,  -0.3636718, -0.27395806, -0.14658587,  0.76516277,  -0.6621258,\r\n",
      " -0.12604865,   1.1275274,  0.39309666, -0.06509334] [0] [4] => [20.1]\r\n",
      "[-0.40257868,    2.808003,  -1.3901517,  -1.2795717,   1.4441153,   -1.174324,   1.5832256,\r\n",
      " -0.45544034,  -2.7424235,   0.4387662,  -1.1818073] [0] [1] => [33.3]\r\n"
     ]
    }
   ],
   "source": [
    "print(XNumericalTest[0], XCategoricalTest[0][0], XCategoricalTest[1][0], \"=>\", YTest[0])\n",
    "print(XNumericalTest[17], XCategoricalTest[0][17], XCategoricalTest[1][17], \"=>\", YTest[17])\n",
    "print(XNumericalTest[87], XCategoricalTest[0][87], XCategoricalTest[1][87], \"=>\", YTest[87])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "S4TF_House.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
