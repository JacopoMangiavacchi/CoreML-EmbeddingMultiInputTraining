{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(url: \"https://github.com/JacopoMangiavacchi/SwiftCoreMLTools.git\", from: \"0.0.6\")\n",
      "\t\tSwiftCoreMLTools\n",
      "\t.package(url: \"https://github.com/dduan/Just.git\", from: \"0.8.0\")\n",
      "\t\tJust\n",
      "With SwiftPM flags: ['-c', 'release']\n",
      "Working in: /tmp/tmpam7mbnoa/swift-install\n",
      "Fetching https://github.com/JacopoMangiavacchi/SwiftCoreMLTools.git\n",
      "Fetching https://github.com/dduan/Just.git\n",
      "Fetching https://github.com/apple/swift-protobuf.git\n",
      "Cloning https://github.com/apple/swift-protobuf.git\n",
      "Resolving https://github.com/apple/swift-protobuf.git at 1.8.0\n",
      "Cloning https://github.com/JacopoMangiavacchi/SwiftCoreMLTools.git\n",
      "Resolving https://github.com/JacopoMangiavacchi/SwiftCoreMLTools.git at 0.0.6\n",
      "Cloning https://github.com/dduan/Just.git\n",
      "Resolving https://github.com/dduan/Just.git at 0.8.0\n",
      "[1/3] Compiling Just Just.swift\n",
      "[2/3] Compiling SwiftProtobuf AnyMessageStorage.swift\n",
      "[3/4] Compiling SwiftCoreMLTools Activations.swift\n",
      "[4/5] Compiling jupyterInstalledPackages jupyterInstalledPackages.swift\n",
      "[5/5] Linking libjupyterInstalledPackages.so\n",
      "Initializing Swift...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install-swiftpm-flags -c release\n",
    "%install '.package(url: \"https://github.com/JacopoMangiavacchi/SwiftCoreMLTools.git\", from: \"0.0.6\")' SwiftCoreMLTools\n",
    "%install '.package(url: \"https://github.com/dduan/Just.git\", from: \"0.8.0\")' Just"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TO4FT9bUohHx"
   },
   "outputs": [],
   "source": [
    "import Foundation\n",
    "import SwiftCoreMLTools\n",
    "import TensorFlow\n",
    "import Just"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Download\n",
    "\n",
    "Boston house prices dataset\n",
    "---------------------------\n",
    "\n",
    "**Data Set Characteristics:**  \n",
    "\n",
    "    :Number of Instances: 506 \n",
    "\n",
    "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
    "\n",
    "    :Attribute Information (in order):\n",
    "        - CRIM     per capita crime rate by town\n",
    "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "        - INDUS    proportion of non-retail business acres per town\n",
    "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "        - NOX      nitric oxides concentration (parts per 10 million)\n",
    "        - RM       average number of rooms per dwelling\n",
    "        - AGE      proportion of owner-occupied units built prior to 1940\n",
    "        - DIS      weighted distances to five Boston employment centres\n",
    "        - RAD      index of accessibility to radial highways\n",
    "        - TAX      full-value property-tax rate per ten thousand dollars\n",
    "        - PTRATIO  pupil-teacher ratio by town\n",
    "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "        - LSTAT    % lower status of the population\n",
    "        - MEDV     Median value of owner-occupied homes in a thousand dollar\n",
    "\n",
    "    :Missing Attribute Values: None\n",
    "\n",
    "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
    "\n",
    "This is a copy of UCI ML housing dataset.\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// if let cts = Just.get(URL(string: \"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\")!).content {\n",
    "//     try! cts.write(to: URL(fileURLWithPath:\"../data/housing.csv\"))\n",
    "// }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "let data = try String(contentsOfFile:\"./data/housing.csv\", encoding: String.Encoding.utf8)\n",
    "let dataRecords: [[Float]] = data.split(separator: \"\\n\").map{ String($0).split(separator: \" \").compactMap{ Float(String($0)) } }\n",
    "\n",
    "let numRecords = dataRecords.count\n",
    "let numColumns = dataRecords[0].count\n",
    "\n",
    "let dataFeatures = dataRecords.map{ Array($0[0..<numColumns-1]) }\n",
    "let dataLabels = dataRecords.map{ Array($0[(numColumns-1)...]) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Numerical Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "let categoricalColumns = [3, 8]\n",
    "let numericalColumns = [0, 1, 2, 4, 5, 6, 7, 9, 10, 11, 12]\n",
    "let numCategoricalFeatures = categoricalColumns.count\n",
    "let numNumericalFeatures = numericalColumns.count\n",
    "let numLabels = 1\n",
    "\n",
    "assert(numColumns == numCategoricalFeatures + numNumericalFeatures + 1)\n",
    "\n",
    "// Get Categorical Features\n",
    "let allCategoriesValues = dataFeatures.map{ row in categoricalColumns.map{ Int32(row[$0]) } }\n",
    "                                .reduce(into: Array(repeating: [Int32](), count: 2)){ total, value in\n",
    "                                    total[0].append(value[0])\n",
    "                                    total[1].append(value[1]) }\n",
    "                                .map{ Set($0).sorted() }\n",
    "\n",
    "let embeddingSizes = allCategoriesValues.map{ $0.count }\n",
    "\n",
    "let categoricalFeatures = dataFeatures.map{ row in categoricalColumns.map{ Int32(row[$0]) } }\n",
    "let oneHotCategoricalFeatures:[[[Int32]]] = categoricalFeatures.map{ catArray in\n",
    "    var oneHotArray = [[Int32]]()\n",
    "    \n",
    "    for i in 0..<catArray.count {\n",
    "        var oneHot = Array(repeating: Int32(0), count: allCategoriesValues[i].count)\n",
    "        if let pos = allCategoriesValues[i].firstIndex(where: { $0 == catArray[i] }){\n",
    "            oneHot[pos] = 1\n",
    "        }\n",
    "        oneHotArray.append(oneHot)\n",
    "    }\n",
    "    \n",
    "    return oneHotArray\n",
    "}\n",
    "\n",
    "// Get Numerical Features\n",
    "let numericalFeatures = dataFeatures.map{ row in numericalColumns.map{ row[$0] } }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "let trainPercentage:Float = 0.8\n",
    "let numTrainRecords = Int(ceil(Float(numRecords) * trainPercentage))\n",
    "let numTestRecords = numRecords - numTrainRecords\n",
    "\n",
    "func matrixTranspose<T>(_ matrix: [[T]]) -> [[T]] {\n",
    "    if matrix.isEmpty {return matrix}\n",
    "    var result = [[T]]()\n",
    "    for index in 0..<matrix.first!.count {\n",
    "        result.append(matrix.map{$0[index]})\n",
    "    }\n",
    "    return result\n",
    "}\n",
    "\n",
    "let xCategoricalAllTrain = matrixTranspose(Array(oneHotCategoricalFeatures[0..<numTrainRecords])).map{ Array($0.joined()) }\n",
    "let xCategoricalAllTest = matrixTranspose(Array(oneHotCategoricalFeatures[numTrainRecords...])).map{ Array($0.joined()) }\n",
    "let xNumericalAllTrain = Array(Array(numericalFeatures[0..<numTrainRecords]).joined())\n",
    "let xNumericalAllTest = Array(Array(numericalFeatures[numTrainRecords...]).joined())\n",
    "let yAllTrain = Array(Array(dataLabels[0..<numTrainRecords]).joined())\n",
    "let yAllTest = Array(Array(dataLabels[numTrainRecords...]).joined())\n",
    "\n",
    "let XCategoricalTrain = xCategoricalAllTrain.enumerated().map{ (offset, element) in \n",
    "    Tensor<Int32>(element).reshaped(to: TensorShape([numTrainRecords, embeddingSizes[offset]]))\n",
    "}\n",
    "let XCategoricalTest = xCategoricalAllTest.enumerated().map{ (offset, element) in \n",
    "    Tensor<Int32>(element).reshaped(to: TensorShape([numTestRecords, embeddingSizes[offset]]))\n",
    "}\n",
    "let XNumericalTrainDeNorm = Tensor<Float>(xNumericalAllTrain).reshaped(to: TensorShape([numTrainRecords, numNumericalFeatures]))\n",
    "let XNumericalTestDeNorm = Tensor<Float>(xNumericalAllTest).reshaped(to: TensorShape([numTestRecords, numNumericalFeatures]))\n",
    "let YTrain = Tensor<Float>(yAllTrain).reshaped(to: TensorShape([numTrainRecords, numLabels]))\n",
    "let YTest = Tensor<Float>(yAllTest).reshaped(to: TensorShape([numTestRecords, numLabels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.0137098,  14.197531,   9.523555, 0.53213036,  6.3311296,   64.47929,  4.1678762,  353.68396,\r\n",
      "    18.03163,  379.84735,  11.394517]] [[ 6.5076075,  25.258776,   6.534038, 0.11449408,  0.7311985,  29.000755,  2.1797554,  132.14561,\r\n",
      "    2.217345,  40.494495,   6.852825]]\r\n"
     ]
    }
   ],
   "source": [
    "let mean = XNumericalTrainDeNorm.mean(alongAxes: 0)\n",
    "let std = XNumericalTrainDeNorm.standardDeviation(alongAxes: 0)\n",
    "\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "let XNumericalTrain = (XNumericalTrainDeNorm - mean)/std\n",
    "let XNumericalTest = (XNumericalTestDeNorm - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shapes [405, 11] [405, 2] [405, 9] [405, 1]\r\n",
      "Testing shapes  [101, 11] [101, 2] [101, 9] [101, 1]\r\n"
     ]
    }
   ],
   "source": [
    "print(\"Training shapes \\(XNumericalTrain.shape) \\(XCategoricalTrain[0].shape) \\(XCategoricalTrain[1].shape) \\(YTrain.shape)\")\n",
    "print(\"Testing shapes  \\(XNumericalTest.shape) \\(XCategoricalTest[0].shape) \\(XCategoricalTest[1].shape) \\(YTest.shape)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HDJZCCgqohIC"
   },
   "outputs": [],
   "source": [
    "struct MultiInputs<N: Differentiable, C>: Differentiable {\n",
    "  var numerical: N\n",
    "  \n",
    "  @noDerivative\n",
    "  var categorical: C\n",
    "\n",
    "  @differentiable\n",
    "  init(numerical: N, categorical: C) {\n",
    "    self.numerical = numerical\n",
    "    self.categorical = categorical\n",
    "  }\n",
    "}\n",
    "\n",
    "struct RegressionModel: Module {\n",
    "//     var numericalLayer = Dense<Float>(inputSize: 11, outputSize: 32, activation: relu)\n",
    "    var embedding1 = TensorFlow.Embedding<Float>(vocabularySize: 2, embeddingSize: 2)\n",
    "    var embedding2 = TensorFlow.Embedding<Float>(vocabularySize: 9, embeddingSize: 5)\n",
    "//     var embeddingLayer = Dense<Float>(inputSize: (4 + 45), outputSize: 64, activation: relu)\n",
    "//     var allInputConcatLayer = Dense<Float>(inputSize: (32 + 64), outputSize: 128, activation: relu)\n",
    "    var allInputConcatLayer = Dense<Float>(inputSize: (11 + 4 + 45), outputSize: 128, activation: relu)\n",
    "    var hiddenLayer = Dense<Float>(inputSize: 128, outputSize: 32, activation: relu)\n",
    "    var outputLayer = Dense<Float>(inputSize: 32, outputSize: 1)\n",
    "    \n",
    "    @differentiable\n",
    "    func callAsFunction(_ input: MultiInputs<[Tensor<Float>], [Tensor<Int32>]>) -> Tensor<Float> {\n",
    "//         let numericalInput = numericalLayer(input.numerical[0])\n",
    "        let embeddingOutput1 = embedding1(input.categorical[0])\n",
    "        let embeddingOutput1Reshaped = embeddingOutput1.reshaped(to: \n",
    "            TensorShape([embeddingOutput1.shape[0], embeddingOutput1.shape[1] * embeddingOutput1.shape[2]]))\n",
    "        let embeddingOutput2 = embedding2(input.categorical[1])\n",
    "        let embeddingOutput2Reshaped = embeddingOutput2.reshaped(to: \n",
    "            TensorShape([embeddingOutput2.shape[0], embeddingOutput2.shape[1] * embeddingOutput2.shape[2]]))\n",
    "//         let embeddingConcat = Tensor<Float>(concatenating: [embeddingOutput1Reshaped, embeddingOutput2Reshaped], alongAxis: 1)\n",
    "//         let embeddingInput = embeddingLayer(embeddingConcat)\n",
    "//         let allConcat = Tensor<Float>(concatenating: [numericalInput, embeddingInput], alongAxis: 1)\n",
    "        let allConcat = Tensor<Float>(concatenating: [input.numerical[0], embeddingOutput1Reshaped, embeddingOutput2Reshaped], alongAxis: 1)\n",
    "        return allConcat.sequenced(through: allInputConcatLayer, hiddenLayer, outputLayer)\n",
    "    }\n",
    "}\n",
    "\n",
    "var model = RegressionModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JK0Vj7bSohIF"
   },
   "outputs": [],
   "source": [
    "let optimizer = RMSProp(for: model, learningRate: 0.001)\n",
    "Context.local.learningPhase = .training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gY8C7yHJohIH"
   },
   "outputs": [],
   "source": [
    "let epochCount = 500\n",
    "let batchSize = 32\n",
    "let numberOfBatch = Int(ceil(Double(numTrainRecords) / Double(batchSize)))\n",
    "let shuffle = true\n",
    "\n",
    "func mae(predictions: Tensor<Float>, truths: Tensor<Float>) -> Float {\n",
    "    return abs(Tensor<Float>(predictions - truths)).mean().scalarized()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "L9bU9HsdohIK",
    "outputId": "692b81c5-3286-4e7c-9246-eb56e6a3eaee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: MSE: 569.4939, MAE: 21.429108\n",
      "Epoch 2: MSE: 390.6878, MAE: 16.88195\n",
      "Epoch 3: MSE: 210.39325, MAE: 11.00771\n",
      "Epoch 4: MSE: 104.52009, MAE: 6.3520937\n",
      "Epoch 5: MSE: 68.18122, MAE: 5.078712\n",
      "Epoch 6: MSE: 57.655075, MAE: 4.8604417\n",
      "Epoch 7: MSE: 52.805946, MAE: 4.589447\n",
      "Epoch 8: MSE: 47.35691, MAE: 4.1085215\n",
      "Epoch 9: MSE: 42.705585, MAE: 4.1543994\n",
      "Epoch 10: MSE: 39.254417, MAE: 3.8196628\n",
      "Epoch 11: MSE: 37.57303, MAE: 3.5332248\n",
      "Epoch 12: MSE: 33.682304, MAE: 3.3067052\n",
      "Epoch 13: MSE: 32.754135, MAE: 3.368258\n",
      "Epoch 14: MSE: 29.781002, MAE: 3.1790557\n",
      "Epoch 15: MSE: 27.873081, MAE: 3.0927036\n",
      "Epoch 16: MSE: 27.72235, MAE: 3.019946\n",
      "Epoch 17: MSE: 26.682173, MAE: 2.9898021\n",
      "Epoch 18: MSE: 25.114681, MAE: 2.962586\n",
      "Epoch 19: MSE: 25.70701, MAE: 2.978217\n",
      "Epoch 20: MSE: 23.954433, MAE: 2.980096\n",
      "Epoch 21: MSE: 24.561592, MAE: 2.9191933\n",
      "Epoch 22: MSE: 23.741722, MAE: 2.8623164\n",
      "Epoch 23: MSE: 23.758007, MAE: 2.8418906\n",
      "Epoch 24: MSE: 23.093792, MAE: 2.848087\n",
      "Epoch 25: MSE: 21.722807, MAE: 2.8150325\n",
      "Epoch 26: MSE: 22.937695, MAE: 2.7615752\n",
      "Epoch 27: MSE: 21.382017, MAE: 2.7970984\n",
      "Epoch 28: MSE: 21.874424, MAE: 2.717297\n",
      "Epoch 29: MSE: 21.468851, MAE: 2.680262\n",
      "Epoch 30: MSE: 20.691994, MAE: 2.7357967\n",
      "Epoch 31: MSE: 20.923038, MAE: 2.705727\n",
      "Epoch 32: MSE: 20.07172, MAE: 2.6229727\n",
      "Epoch 33: MSE: 18.648455, MAE: 2.5556288\n",
      "Epoch 34: MSE: 19.939602, MAE: 2.5660715\n",
      "Epoch 35: MSE: 19.51674, MAE: 2.621919\n",
      "Epoch 36: MSE: 19.894243, MAE: 2.5596395\n",
      "Epoch 37: MSE: 19.20205, MAE: 2.5348232\n",
      "Epoch 38: MSE: 18.44894, MAE: 2.501377\n",
      "Epoch 39: MSE: 18.970085, MAE: 2.4733567\n",
      "Epoch 40: MSE: 19.07476, MAE: 2.4441683\n",
      "Epoch 41: MSE: 18.931017, MAE: 2.4207404\n",
      "Epoch 42: MSE: 17.943836, MAE: 2.4352438\n",
      "Epoch 43: MSE: 18.290413, MAE: 2.3981624\n",
      "Epoch 44: MSE: 18.555212, MAE: 2.4318788\n",
      "Epoch 45: MSE: 17.158571, MAE: 2.3665228\n",
      "Epoch 46: MSE: 17.218752, MAE: 2.4007604\n",
      "Epoch 47: MSE: 16.785303, MAE: 2.3635356\n",
      "Epoch 48: MSE: 15.707594, MAE: 2.3171556\n",
      "Epoch 49: MSE: 16.548754, MAE: 2.2707834\n",
      "Epoch 50: MSE: 17.452892, MAE: 2.3244855\n",
      "Epoch 51: MSE: 16.350292, MAE: 2.2966285\n",
      "Epoch 52: MSE: 15.227012, MAE: 2.2575636\n",
      "Epoch 53: MSE: 13.985295, MAE: 2.2275681\n",
      "Epoch 54: MSE: 17.470894, MAE: 2.2257552\n",
      "Epoch 55: MSE: 15.540411, MAE: 2.236695\n",
      "Epoch 56: MSE: 15.480262, MAE: 2.1799383\n",
      "Epoch 57: MSE: 15.124449, MAE: 2.1722674\n",
      "Epoch 58: MSE: 15.489504, MAE: 2.187771\n",
      "Epoch 59: MSE: 14.569636, MAE: 2.14867\n",
      "Epoch 60: MSE: 14.612721, MAE: 2.138245\n",
      "Epoch 61: MSE: 14.544429, MAE: 2.10363\n",
      "Epoch 62: MSE: 13.736199, MAE: 2.099976\n",
      "Epoch 63: MSE: 14.258631, MAE: 2.102784\n",
      "Epoch 64: MSE: 13.558306, MAE: 2.0965843\n",
      "Epoch 65: MSE: 13.5027275, MAE: 2.0606852\n",
      "Epoch 66: MSE: 13.90087, MAE: 2.0689886\n",
      "Epoch 67: MSE: 14.0656185, MAE: 2.0635817\n",
      "Epoch 68: MSE: 13.574589, MAE: 2.0076323\n",
      "Epoch 69: MSE: 13.599199, MAE: 2.0299592\n",
      "Epoch 70: MSE: 13.319296, MAE: 2.045449\n",
      "Epoch 71: MSE: 13.128418, MAE: 2.0336273\n",
      "Epoch 72: MSE: 12.97321, MAE: 2.0093834\n",
      "Epoch 73: MSE: 13.141871, MAE: 1.9826179\n",
      "Epoch 74: MSE: 11.816467, MAE: 1.98195\n",
      "Epoch 75: MSE: 12.930696, MAE: 1.9663846\n",
      "Epoch 76: MSE: 12.551122, MAE: 1.9623617\n",
      "Epoch 77: MSE: 11.682277, MAE: 1.9476235\n",
      "Epoch 78: MSE: 12.407111, MAE: 1.9525441\n",
      "Epoch 79: MSE: 11.077022, MAE: 1.9498112\n",
      "Epoch 80: MSE: 12.473368, MAE: 1.9461114\n",
      "Epoch 81: MSE: 10.713536, MAE: 1.9023263\n",
      "Epoch 82: MSE: 11.956918, MAE: 1.9115523\n",
      "Epoch 83: MSE: 10.79803, MAE: 1.8966737\n",
      "Epoch 84: MSE: 11.426896, MAE: 1.9027841\n",
      "Epoch 85: MSE: 11.583712, MAE: 1.8955942\n",
      "Epoch 86: MSE: 11.019577, MAE: 1.8866951\n",
      "Epoch 87: MSE: 10.523814, MAE: 1.8801965\n",
      "Epoch 88: MSE: 10.919032, MAE: 1.8823395\n",
      "Epoch 89: MSE: 10.704731, MAE: 1.869112\n",
      "Epoch 90: MSE: 10.938955, MAE: 1.8365881\n",
      "Epoch 91: MSE: 10.665023, MAE: 1.8371549\n",
      "Epoch 92: MSE: 10.101419, MAE: 1.8418171\n",
      "Epoch 93: MSE: 10.479767, MAE: 1.8727266\n",
      "Epoch 94: MSE: 10.39002, MAE: 1.8693864\n",
      "Epoch 95: MSE: 10.045597, MAE: 1.8413633\n",
      "Epoch 96: MSE: 9.645238, MAE: 1.8177279\n",
      "Epoch 97: MSE: 10.675518, MAE: 1.8058417\n",
      "Epoch 98: MSE: 10.143031, MAE: 1.8580124\n",
      "Epoch 99: MSE: 9.92747, MAE: 1.8471462\n",
      "Epoch 100: MSE: 8.490568, MAE: 1.8341012\n",
      "Epoch 101: MSE: 9.559267, MAE: 1.8333415\n",
      "Epoch 102: MSE: 9.526745, MAE: 1.8488244\n",
      "Epoch 103: MSE: 9.360699, MAE: 1.8187612\n",
      "Epoch 104: MSE: 9.40394, MAE: 1.8122516\n",
      "Epoch 105: MSE: 10.42337, MAE: 1.8161151\n",
      "Epoch 106: MSE: 8.539653, MAE: 1.7771885\n",
      "Epoch 107: MSE: 8.775973, MAE: 1.7765414\n",
      "Epoch 108: MSE: 9.160134, MAE: 1.831869\n",
      "Epoch 109: MSE: 8.314529, MAE: 1.8117884\n",
      "Epoch 110: MSE: 9.647629, MAE: 1.7686545\n",
      "Epoch 111: MSE: 9.5778885, MAE: 1.7728628\n",
      "Epoch 112: MSE: 7.9841623, MAE: 1.7437491\n",
      "Epoch 113: MSE: 8.463816, MAE: 1.739233\n",
      "Epoch 114: MSE: 7.619454, MAE: 1.8120439\n",
      "Epoch 115: MSE: 9.047434, MAE: 1.731096\n",
      "Epoch 116: MSE: 9.57804, MAE: 1.7599285\n",
      "Epoch 117: MSE: 8.010206, MAE: 1.6865758\n",
      "Epoch 118: MSE: 8.660277, MAE: 1.7693201\n",
      "Epoch 119: MSE: 8.612247, MAE: 1.72096\n",
      "Epoch 120: MSE: 8.414625, MAE: 1.7217218\n",
      "Epoch 121: MSE: 8.5598135, MAE: 1.7146592\n",
      "Epoch 122: MSE: 8.667153, MAE: 1.7303346\n",
      "Epoch 123: MSE: 7.9154453, MAE: 1.6922021\n",
      "Epoch 124: MSE: 8.591729, MAE: 1.704564\n",
      "Epoch 125: MSE: 7.959875, MAE: 1.7486777\n",
      "Epoch 126: MSE: 7.45096, MAE: 1.7323004\n",
      "Epoch 127: MSE: 7.627682, MAE: 1.743901\n",
      "Epoch 128: MSE: 7.970505, MAE: 1.7122233\n",
      "Epoch 129: MSE: 7.9951725, MAE: 1.7056851\n",
      "Epoch 130: MSE: 7.6120405, MAE: 1.6742853\n",
      "Epoch 131: MSE: 7.547872, MAE: 1.6867429\n",
      "Epoch 132: MSE: 7.462448, MAE: 1.6722614\n",
      "Epoch 133: MSE: 7.9653378, MAE: 1.748203\n",
      "Epoch 134: MSE: 8.378766, MAE: 1.7004402\n",
      "Epoch 135: MSE: 7.9617333, MAE: 1.6406453\n",
      "Epoch 136: MSE: 7.473415, MAE: 1.666877\n",
      "Epoch 137: MSE: 7.507467, MAE: 1.7156804\n",
      "Epoch 138: MSE: 7.617879, MAE: 1.6589799\n",
      "Epoch 139: MSE: 7.74383, MAE: 1.6025326\n",
      "Epoch 140: MSE: 7.4424415, MAE: 1.6937559\n",
      "Epoch 141: MSE: 7.0186768, MAE: 1.6337796\n",
      "Epoch 142: MSE: 6.7919736, MAE: 1.649142\n",
      "Epoch 143: MSE: 8.370134, MAE: 1.6247131\n",
      "Epoch 144: MSE: 7.819371, MAE: 1.6336166\n",
      "Epoch 145: MSE: 6.8866496, MAE: 1.6425592\n",
      "Epoch 146: MSE: 7.243177, MAE: 1.6551062\n",
      "Epoch 147: MSE: 5.8178844, MAE: 1.5846803\n",
      "Epoch 148: MSE: 8.665068, MAE: 1.6846795\n",
      "Epoch 149: MSE: 6.4743752, MAE: 1.5854734\n",
      "Epoch 150: MSE: 7.121936, MAE: 1.6604899\n",
      "Epoch 151: MSE: 6.5387526, MAE: 1.6255265\n",
      "Epoch 152: MSE: 7.0552745, MAE: 1.660181\n",
      "Epoch 153: MSE: 6.7624836, MAE: 1.6300286\n",
      "Epoch 154: MSE: 6.3352513, MAE: 1.5952646\n",
      "Epoch 155: MSE: 7.4440103, MAE: 1.6283627\n",
      "Epoch 156: MSE: 7.0848293, MAE: 1.5685471\n",
      "Epoch 157: MSE: 6.583456, MAE: 1.6586405\n",
      "Epoch 158: MSE: 6.5977316, MAE: 1.5984929\n",
      "Epoch 159: MSE: 6.9482856, MAE: 1.5590708\n",
      "Epoch 160: MSE: 5.624255, MAE: 1.5479819\n",
      "Epoch 161: MSE: 7.267711, MAE: 1.6004665\n",
      "Epoch 162: MSE: 7.122696, MAE: 1.5470823\n",
      "Epoch 163: MSE: 6.5120883, MAE: 1.5873871\n",
      "Epoch 164: MSE: 5.8740635, MAE: 1.5584596\n",
      "Epoch 165: MSE: 7.144833, MAE: 1.5283729\n",
      "Epoch 166: MSE: 6.349554, MAE: 1.5431098\n",
      "Epoch 167: MSE: 6.646355, MAE: 1.579468\n",
      "Epoch 168: MSE: 6.6899753, MAE: 1.5565791\n",
      "Epoch 169: MSE: 6.9879127, MAE: 1.5655932\n",
      "Epoch 170: MSE: 5.9660463, MAE: 1.5400141\n",
      "Epoch 171: MSE: 6.0301867, MAE: 1.6017331\n",
      "Epoch 172: MSE: 6.373565, MAE: 1.538887\n",
      "Epoch 173: MSE: 5.7555466, MAE: 1.5599163\n",
      "Epoch 174: MSE: 7.2137356, MAE: 1.6089263\n",
      "Epoch 175: MSE: 5.5469885, MAE: 1.5032024\n",
      "Epoch 176: MSE: 6.004585, MAE: 1.5229901\n",
      "Epoch 177: MSE: 5.9223833, MAE: 1.5702513\n",
      "Epoch 178: MSE: 6.8237743, MAE: 1.5271081\n",
      "Epoch 179: MSE: 5.90357, MAE: 1.5079303\n",
      "Epoch 180: MSE: 5.411654, MAE: 1.4940321\n",
      "Epoch 181: MSE: 6.0535455, MAE: 1.6028533\n",
      "Epoch 182: MSE: 6.5883894, MAE: 1.5421646\n",
      "Epoch 183: MSE: 6.4062033, MAE: 1.5135032\n",
      "Epoch 184: MSE: 5.770061, MAE: 1.5019118\n",
      "Epoch 185: MSE: 6.6930103, MAE: 1.5511117\n",
      "Epoch 186: MSE: 5.1251545, MAE: 1.4526124\n",
      "Epoch 187: MSE: 6.02901, MAE: 1.5385526\n",
      "Epoch 188: MSE: 5.7196207, MAE: 1.4842556\n",
      "Epoch 189: MSE: 5.759897, MAE: 1.5145581\n",
      "Epoch 190: MSE: 6.669021, MAE: 1.5526211\n",
      "Epoch 191: MSE: 5.271604, MAE: 1.4868238\n",
      "Epoch 192: MSE: 5.2850337, MAE: 1.5162263\n",
      "Epoch 193: MSE: 5.656432, MAE: 1.5042418\n",
      "Epoch 194: MSE: 6.8483973, MAE: 1.5429677\n",
      "Epoch 195: MSE: 6.134589, MAE: 1.4415829\n",
      "Epoch 196: MSE: 5.108773, MAE: 1.4439175\n",
      "Epoch 197: MSE: 5.9458017, MAE: 1.5214674\n",
      "Epoch 198: MSE: 5.471246, MAE: 1.5029517\n",
      "Epoch 199: MSE: 5.1492963, MAE: 1.4772664\n",
      "Epoch 200: MSE: 5.888877, MAE: 1.5519521\n",
      "Epoch 201: MSE: 4.89125, MAE: 1.5398681\n",
      "Epoch 202: MSE: 6.4772635, MAE: 1.4789172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203: MSE: 4.570264, MAE: 1.4334056\n",
      "Epoch 204: MSE: 5.570898, MAE: 1.5491028\n",
      "Epoch 205: MSE: 4.82101, MAE: 1.4860107\n",
      "Epoch 206: MSE: 6.6490674, MAE: 1.5334176\n",
      "Epoch 207: MSE: 5.1217422, MAE: 1.4058744\n",
      "Epoch 208: MSE: 4.9530644, MAE: 1.4546908\n",
      "Epoch 209: MSE: 5.4213004, MAE: 1.5061845\n",
      "Epoch 210: MSE: 5.441388, MAE: 1.4776095\n",
      "Epoch 211: MSE: 5.220281, MAE: 1.4890265\n",
      "Epoch 212: MSE: 5.5349097, MAE: 1.4914423\n",
      "Epoch 213: MSE: 5.739909, MAE: 1.4315777\n",
      "Epoch 214: MSE: 6.0274, MAE: 1.5860518\n",
      "Epoch 215: MSE: 5.509417, MAE: 1.449182\n",
      "Epoch 216: MSE: 5.213073, MAE: 1.4175414\n",
      "Epoch 217: MSE: 4.858862, MAE: 1.4542339\n",
      "Epoch 218: MSE: 5.999465, MAE: 1.4575531\n",
      "Epoch 219: MSE: 5.199783, MAE: 1.464971\n",
      "Epoch 220: MSE: 4.5445623, MAE: 1.4288995\n",
      "Epoch 221: MSE: 5.935419, MAE: 1.4926181\n",
      "Epoch 222: MSE: 5.000806, MAE: 1.4413104\n",
      "Epoch 223: MSE: 4.624112, MAE: 1.4747112\n",
      "Epoch 224: MSE: 5.1963305, MAE: 1.5666254\n",
      "Epoch 225: MSE: 4.2684913, MAE: 1.4466697\n",
      "Epoch 226: MSE: 6.380117, MAE: 1.5717989\n",
      "Epoch 227: MSE: 4.7126927, MAE: 1.4185424\n",
      "Epoch 228: MSE: 5.393952, MAE: 1.4945092\n",
      "Epoch 229: MSE: 4.9424233, MAE: 1.4070033\n",
      "Epoch 230: MSE: 5.1997576, MAE: 1.457856\n",
      "Epoch 231: MSE: 5.012574, MAE: 1.4254235\n",
      "Epoch 232: MSE: 5.3812046, MAE: 1.4626813\n",
      "Epoch 233: MSE: 5.6398654, MAE: 1.4830909\n",
      "Epoch 234: MSE: 4.591451, MAE: 1.3567716\n",
      "Epoch 235: MSE: 4.3709383, MAE: 1.4719821\n",
      "Epoch 236: MSE: 5.0667124, MAE: 1.4534953\n",
      "Epoch 237: MSE: 4.4597173, MAE: 1.4445466\n",
      "Epoch 238: MSE: 5.042958, MAE: 1.4669509\n",
      "Epoch 239: MSE: 4.7383957, MAE: 1.4762092\n",
      "Epoch 240: MSE: 4.2120647, MAE: 1.414304\n",
      "Epoch 241: MSE: 4.8864493, MAE: 1.509522\n",
      "Epoch 242: MSE: 6.2060947, MAE: 1.4969846\n",
      "Epoch 243: MSE: 4.429631, MAE: 1.3626066\n",
      "Epoch 244: MSE: 4.204399, MAE: 1.3604481\n",
      "Epoch 245: MSE: 5.6396136, MAE: 1.5264003\n",
      "Epoch 246: MSE: 4.4585233, MAE: 1.3867568\n",
      "Epoch 247: MSE: 4.9307504, MAE: 1.466326\n",
      "Epoch 248: MSE: 4.7958417, MAE: 1.4460634\n",
      "Epoch 249: MSE: 4.418399, MAE: 1.411687\n",
      "Epoch 250: MSE: 5.003869, MAE: 1.49068\n",
      "Epoch 251: MSE: 4.762702, MAE: 1.4022111\n",
      "Epoch 252: MSE: 5.1976566, MAE: 1.4390285\n",
      "Epoch 253: MSE: 4.405465, MAE: 1.3801898\n",
      "Epoch 254: MSE: 4.8388805, MAE: 1.3899951\n",
      "Epoch 255: MSE: 4.80443, MAE: 1.4748362\n",
      "Epoch 256: MSE: 4.216811, MAE: 1.3813653\n",
      "Epoch 257: MSE: 5.447457, MAE: 1.4603933\n",
      "Epoch 258: MSE: 4.983305, MAE: 1.3742326\n",
      "Epoch 259: MSE: 4.398505, MAE: 1.3747474\n",
      "Epoch 260: MSE: 4.6240926, MAE: 1.3281814\n",
      "Epoch 261: MSE: 4.701923, MAE: 1.4839971\n",
      "Epoch 262: MSE: 4.908961, MAE: 1.3313962\n",
      "Epoch 263: MSE: 4.676972, MAE: 1.4225249\n",
      "Epoch 264: MSE: 4.8468804, MAE: 1.4284389\n",
      "Epoch 265: MSE: 4.674582, MAE: 1.4522188\n",
      "Epoch 266: MSE: 4.198863, MAE: 1.3886683\n",
      "Epoch 267: MSE: 4.7952547, MAE: 1.4635046\n",
      "Epoch 268: MSE: 3.9369621, MAE: 1.3960965\n",
      "Epoch 269: MSE: 4.9777884, MAE: 1.4419656\n",
      "Epoch 270: MSE: 4.3279233, MAE: 1.382017\n",
      "Epoch 271: MSE: 4.1589966, MAE: 1.3448192\n",
      "Epoch 272: MSE: 4.446315, MAE: 1.4345483\n",
      "Epoch 273: MSE: 4.74366, MAE: 1.4622835\n",
      "Epoch 274: MSE: 4.642838, MAE: 1.4087245\n",
      "Epoch 275: MSE: 4.312723, MAE: 1.4248424\n",
      "Epoch 276: MSE: 5.0056734, MAE: 1.4288485\n",
      "Epoch 277: MSE: 4.283793, MAE: 1.3656701\n",
      "Epoch 278: MSE: 5.1449094, MAE: 1.3872528\n",
      "Epoch 279: MSE: 4.388023, MAE: 1.376673\n",
      "Epoch 280: MSE: 4.608626, MAE: 1.4636134\n",
      "Epoch 281: MSE: 3.677035, MAE: 1.3603753\n",
      "Epoch 282: MSE: 4.9241695, MAE: 1.5430927\n",
      "Epoch 283: MSE: 4.752234, MAE: 1.4054457\n",
      "Epoch 284: MSE: 3.6447282, MAE: 1.305748\n",
      "Epoch 285: MSE: 4.1061993, MAE: 1.4576932\n",
      "Epoch 286: MSE: 4.654309, MAE: 1.4594021\n",
      "Epoch 287: MSE: 4.099481, MAE: 1.4302166\n",
      "Epoch 288: MSE: 4.1361904, MAE: 1.3860391\n",
      "Epoch 289: MSE: 4.45665, MAE: 1.3800784\n",
      "Epoch 290: MSE: 4.031187, MAE: 1.3462831\n",
      "Epoch 291: MSE: 4.1901417, MAE: 1.3768375\n",
      "Epoch 292: MSE: 4.23959, MAE: 1.3995986\n",
      "Epoch 293: MSE: 4.4692163, MAE: 1.4708949\n",
      "Epoch 294: MSE: 4.592074, MAE: 1.3293198\n",
      "Epoch 295: MSE: 3.9171457, MAE: 1.3544948\n",
      "Epoch 296: MSE: 4.216725, MAE: 1.3069319\n",
      "Epoch 297: MSE: 4.510522, MAE: 1.3880998\n",
      "Epoch 298: MSE: 3.6426687, MAE: 1.3718169\n",
      "Epoch 299: MSE: 4.5715437, MAE: 1.3423826\n",
      "Epoch 300: MSE: 3.349682, MAE: 1.347683\n",
      "Epoch 301: MSE: 3.5822635, MAE: 1.4115746\n",
      "Epoch 302: MSE: 4.5743046, MAE: 1.5380763\n",
      "Epoch 303: MSE: 4.4544024, MAE: 1.271519\n",
      "Epoch 304: MSE: 4.2519984, MAE: 1.2714274\n",
      "Epoch 305: MSE: 4.4199724, MAE: 1.3620908\n",
      "Epoch 306: MSE: 3.5619528, MAE: 1.3391194\n",
      "Epoch 307: MSE: 3.2901788, MAE: 1.3320018\n",
      "Epoch 308: MSE: 5.319369, MAE: 1.4238441\n",
      "Epoch 309: MSE: 4.14127, MAE: 1.3124593\n",
      "Epoch 310: MSE: 4.0051336, MAE: 1.4144117\n",
      "Epoch 311: MSE: 3.2242815, MAE: 1.4029521\n",
      "Epoch 312: MSE: 5.7798977, MAE: 1.3888594\n",
      "Epoch 313: MSE: 3.5433016, MAE: 1.2713107\n",
      "Epoch 314: MSE: 3.0220811, MAE: 1.3003075\n",
      "Epoch 315: MSE: 3.9788337, MAE: 1.4942545\n",
      "Epoch 316: MSE: 3.709124, MAE: 1.4041846\n",
      "Epoch 317: MSE: 3.8036246, MAE: 1.2799345\n",
      "Epoch 318: MSE: 4.1770864, MAE: 1.4782093\n",
      "Epoch 319: MSE: 3.8784018, MAE: 1.440035\n",
      "Epoch 320: MSE: 4.6483717, MAE: 1.3780264\n",
      "Epoch 321: MSE: 3.4067664, MAE: 1.3087286\n",
      "Epoch 322: MSE: 4.3786044, MAE: 1.4508233\n",
      "Epoch 323: MSE: 3.6560833, MAE: 1.2779696\n",
      "Epoch 324: MSE: 4.5574503, MAE: 1.3588288\n",
      "Epoch 325: MSE: 3.763958, MAE: 1.2291453\n",
      "Epoch 326: MSE: 3.042646, MAE: 1.2949462\n",
      "Epoch 327: MSE: 3.9114833, MAE: 1.4259015\n",
      "Epoch 328: MSE: 3.7773793, MAE: 1.3921838\n",
      "Epoch 329: MSE: 3.1156998, MAE: 1.35345\n",
      "Epoch 330: MSE: 4.848836, MAE: 1.5186143\n",
      "Epoch 331: MSE: 4.0323753, MAE: 1.3754522\n",
      "Epoch 332: MSE: 4.3156896, MAE: 1.3974702\n",
      "Epoch 333: MSE: 3.48262, MAE: 1.3153596\n",
      "Epoch 334: MSE: 5.146364, MAE: 1.3919339\n",
      "Epoch 335: MSE: 3.901793, MAE: 1.2761936\n",
      "Epoch 336: MSE: 3.1310353, MAE: 1.2613852\n",
      "Epoch 337: MSE: 4.6347165, MAE: 1.403144\n",
      "Epoch 338: MSE: 3.823981, MAE: 1.3176814\n",
      "Epoch 339: MSE: 3.0888166, MAE: 1.3061968\n",
      "Epoch 340: MSE: 3.8620403, MAE: 1.3554084\n",
      "Epoch 341: MSE: 3.5039096, MAE: 1.3710021\n",
      "Epoch 342: MSE: 4.182926, MAE: 1.2895343\n",
      "Epoch 343: MSE: 3.8215578, MAE: 1.4303175\n",
      "Epoch 344: MSE: 4.6872196, MAE: 1.3500497\n",
      "Epoch 345: MSE: 3.6761808, MAE: 1.2607802\n",
      "Epoch 346: MSE: 3.1778314, MAE: 1.3246522\n",
      "Epoch 347: MSE: 5.004182, MAE: 1.3981678\n",
      "Epoch 348: MSE: 3.70893, MAE: 1.3099434\n",
      "Epoch 349: MSE: 3.837477, MAE: 1.3524053\n",
      "Epoch 350: MSE: 3.653974, MAE: 1.2936927\n",
      "Epoch 351: MSE: 4.2954736, MAE: 1.471116\n",
      "Epoch 352: MSE: 3.534474, MAE: 1.2444425\n",
      "Epoch 353: MSE: 4.012644, MAE: 1.3221227\n",
      "Epoch 354: MSE: 3.5095675, MAE: 1.2883241\n",
      "Epoch 355: MSE: 3.9813042, MAE: 1.517802\n",
      "Epoch 356: MSE: 3.4723234, MAE: 1.2621865\n",
      "Epoch 357: MSE: 3.7494688, MAE: 1.3213526\n",
      "Epoch 358: MSE: 4.28382, MAE: 1.4003913\n",
      "Epoch 359: MSE: 3.3087285, MAE: 1.210174\n",
      "Epoch 360: MSE: 3.586315, MAE: 1.3275657\n",
      "Epoch 361: MSE: 3.836066, MAE: 1.4284971\n",
      "Epoch 362: MSE: 3.0562708, MAE: 1.3526145\n",
      "Epoch 363: MSE: 4.534141, MAE: 1.240664\n",
      "Epoch 364: MSE: 3.103406, MAE: 1.2543795\n",
      "Epoch 365: MSE: 3.4829793, MAE: 1.3944225\n",
      "Epoch 366: MSE: 4.394546, MAE: 1.353527\n",
      "Epoch 367: MSE: 3.2991776, MAE: 1.2500125\n",
      "Epoch 368: MSE: 3.3292341, MAE: 1.3854547\n",
      "Epoch 369: MSE: 4.807503, MAE: 1.3755722\n",
      "Epoch 370: MSE: 3.5513616, MAE: 1.2703297\n",
      "Epoch 371: MSE: 3.6972606, MAE: 1.3848014\n",
      "Epoch 372: MSE: 3.9903183, MAE: 1.2434363\n",
      "Epoch 373: MSE: 3.481008, MAE: 1.3413227\n",
      "Epoch 374: MSE: 4.3769665, MAE: 1.3737003\n",
      "Epoch 375: MSE: 3.101011, MAE: 1.1829331\n",
      "Epoch 376: MSE: 3.6794052, MAE: 1.4644824\n",
      "Epoch 377: MSE: 3.7969804, MAE: 1.3288587\n",
      "Epoch 378: MSE: 3.2034202, MAE: 1.2364745\n",
      "Epoch 379: MSE: 3.87738, MAE: 1.333349\n",
      "Epoch 380: MSE: 3.4719887, MAE: 1.362905\n",
      "Epoch 381: MSE: 4.0205965, MAE: 1.344034\n",
      "Epoch 382: MSE: 2.9158092, MAE: 1.2396559\n",
      "Epoch 383: MSE: 3.7113485, MAE: 1.3294107\n",
      "Epoch 384: MSE: 3.297719, MAE: 1.2856959\n",
      "Epoch 385: MSE: 3.4021606, MAE: 1.3297985\n",
      "Epoch 386: MSE: 3.9761696, MAE: 1.3935895\n",
      "Epoch 387: MSE: 2.8390203, MAE: 1.3334559\n",
      "Epoch 388: MSE: 3.3558962, MAE: 1.4031705\n",
      "Epoch 389: MSE: 3.2557378, MAE: 1.2398375\n",
      "Epoch 390: MSE: 3.9237466, MAE: 1.4446743\n",
      "Epoch 391: MSE: 3.7150388, MAE: 1.304187\n",
      "Epoch 392: MSE: 2.602545, MAE: 1.21684\n",
      "Epoch 393: MSE: 4.152278, MAE: 1.3691827\n",
      "Epoch 394: MSE: 3.0190556, MAE: 1.2184275\n",
      "Epoch 395: MSE: 3.743194, MAE: 1.2148061\n",
      "Epoch 396: MSE: 3.4040287, MAE: 1.304121\n",
      "Epoch 397: MSE: 2.946987, MAE: 1.3067876\n",
      "Epoch 398: MSE: 4.1013813, MAE: 1.3964716\n",
      "Epoch 399: MSE: 3.7199411, MAE: 1.3480445\n",
      "Epoch 400: MSE: 3.3619509, MAE: 1.298662\n",
      "Epoch 401: MSE: 3.1510603, MAE: 1.2344445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 402: MSE: 3.4507406, MAE: 1.254597\n",
      "Epoch 403: MSE: 3.2094576, MAE: 1.3705623\n",
      "Epoch 404: MSE: 3.9017057, MAE: 1.4432244\n",
      "Epoch 405: MSE: 3.6647542, MAE: 1.1924517\n",
      "Epoch 406: MSE: 2.4816108, MAE: 1.1199019\n",
      "Epoch 407: MSE: 4.2928243, MAE: 1.2900232\n",
      "Epoch 408: MSE: 3.2563086, MAE: 1.3340185\n",
      "Epoch 409: MSE: 3.9291458, MAE: 1.2399777\n",
      "Epoch 410: MSE: 2.70482, MAE: 1.2080882\n",
      "Epoch 411: MSE: 2.909846, MAE: 1.3731022\n",
      "Epoch 412: MSE: 2.656191, MAE: 1.4094914\n",
      "Epoch 413: MSE: 3.0881805, MAE: 1.4242656\n",
      "Epoch 414: MSE: 3.4344919, MAE: 1.3106711\n",
      "Epoch 415: MSE: 3.8755643, MAE: 1.2790871\n",
      "Epoch 416: MSE: 2.5888186, MAE: 1.1590711\n",
      "Epoch 417: MSE: 4.547571, MAE: 1.3669559\n",
      "Epoch 418: MSE: 3.2188756, MAE: 1.2354851\n",
      "Epoch 419: MSE: 2.9972012, MAE: 1.2204171\n",
      "Epoch 420: MSE: 3.1958475, MAE: 1.3693506\n",
      "Epoch 421: MSE: 3.6022315, MAE: 1.1951505\n",
      "Epoch 422: MSE: 4.4722285, MAE: 1.3136648\n",
      "Epoch 423: MSE: 3.380418, MAE: 1.2344615\n",
      "Epoch 424: MSE: 2.7478828, MAE: 1.1847932\n",
      "Epoch 425: MSE: 3.5860574, MAE: 1.4262645\n",
      "Epoch 426: MSE: 2.5882087, MAE: 1.126368\n",
      "Epoch 427: MSE: 3.0749626, MAE: 1.4332103\n",
      "Epoch 428: MSE: 4.739276, MAE: 1.2250879\n",
      "Epoch 429: MSE: 2.6676264, MAE: 1.1209562\n",
      "Epoch 430: MSE: 3.3331425, MAE: 1.3013122\n",
      "Epoch 431: MSE: 3.829217, MAE: 1.2834226\n",
      "Epoch 432: MSE: 3.0499425, MAE: 1.2483131\n",
      "Epoch 433: MSE: 2.8377585, MAE: 1.2618817\n",
      "Epoch 434: MSE: 2.8583198, MAE: 1.280126\n",
      "Epoch 435: MSE: 4.8344483, MAE: 1.3798432\n",
      "Epoch 436: MSE: 2.7210112, MAE: 1.1020402\n",
      "Epoch 437: MSE: 4.0930166, MAE: 1.3788681\n",
      "Epoch 438: MSE: 2.8925307, MAE: 1.1443177\n",
      "Epoch 439: MSE: 2.9437585, MAE: 1.3013362\n",
      "Epoch 440: MSE: 3.2753694, MAE: 1.1550469\n",
      "Epoch 441: MSE: 3.1986144, MAE: 1.3663545\n",
      "Epoch 442: MSE: 3.101821, MAE: 1.2300638\n",
      "Epoch 443: MSE: 3.4148927, MAE: 1.3435796\n",
      "Epoch 444: MSE: 2.7835715, MAE: 1.3151101\n",
      "Epoch 445: MSE: 3.5210843, MAE: 1.2220985\n",
      "Epoch 446: MSE: 2.6536348, MAE: 1.2158574\n",
      "Epoch 447: MSE: 3.519948, MAE: 1.3573009\n",
      "Epoch 448: MSE: 3.2318063, MAE: 1.3073399\n",
      "Epoch 449: MSE: 3.4578934, MAE: 1.2077513\n",
      "Epoch 450: MSE: 2.886478, MAE: 1.1398276\n",
      "Epoch 451: MSE: 3.4675112, MAE: 1.3100021\n",
      "Epoch 452: MSE: 2.7816863, MAE: 1.1950585\n",
      "Epoch 453: MSE: 2.743856, MAE: 1.3313615\n",
      "Epoch 454: MSE: 4.0212665, MAE: 1.4536813\n",
      "Epoch 455: MSE: 2.8196847, MAE: 1.1462175\n",
      "Epoch 456: MSE: 2.560758, MAE: 1.22036\n",
      "Epoch 457: MSE: 4.394008, MAE: 1.2975622\n",
      "Epoch 458: MSE: 2.8205314, MAE: 1.1838465\n",
      "Epoch 459: MSE: 2.8169312, MAE: 1.2200545\n",
      "Epoch 460: MSE: 3.147413, MAE: 1.3556058\n",
      "Epoch 461: MSE: 3.454409, MAE: 1.2490777\n",
      "Epoch 462: MSE: 2.877285, MAE: 1.2706921\n",
      "Epoch 463: MSE: 3.2350478, MAE: 1.3557872\n",
      "Epoch 464: MSE: 3.6696372, MAE: 1.190798\n",
      "Epoch 465: MSE: 3.1880193, MAE: 1.2704974\n",
      "Epoch 466: MSE: 3.685633, MAE: 1.1603004\n",
      "Epoch 467: MSE: 2.495502, MAE: 1.1439646\n",
      "Epoch 468: MSE: 2.4847486, MAE: 1.2991275\n",
      "Epoch 469: MSE: 3.7054431, MAE: 1.367505\n",
      "Epoch 470: MSE: 3.2300951, MAE: 1.329068\n",
      "Epoch 471: MSE: 3.5712678, MAE: 1.1846689\n",
      "Epoch 472: MSE: 2.5322423, MAE: 1.2412648\n",
      "Epoch 473: MSE: 3.5145063, MAE: 1.378072\n",
      "Epoch 474: MSE: 2.5965002, MAE: 1.263075\n",
      "Epoch 475: MSE: 2.8867924, MAE: 1.3113688\n",
      "Epoch 476: MSE: 3.0764704, MAE: 1.2467141\n",
      "Epoch 477: MSE: 3.2551627, MAE: 1.3501483\n",
      "Epoch 478: MSE: 3.1138983, MAE: 1.3348933\n",
      "Epoch 479: MSE: 2.479687, MAE: 1.1030803\n",
      "Epoch 480: MSE: 2.5854921, MAE: 1.3974738\n",
      "Epoch 481: MSE: 3.0668516, MAE: 1.3555535\n",
      "Epoch 482: MSE: 2.6832883, MAE: 1.118149\n",
      "Epoch 483: MSE: 2.902187, MAE: 1.155452\n",
      "Epoch 484: MSE: 3.51627, MAE: 1.2557902\n",
      "Epoch 485: MSE: 2.5668857, MAE: 1.2517318\n",
      "Epoch 486: MSE: 3.1397047, MAE: 1.3046162\n",
      "Epoch 487: MSE: 2.5090017, MAE: 1.2457262\n",
      "Epoch 488: MSE: 3.6188161, MAE: 1.2514739\n",
      "Epoch 489: MSE: 2.9050975, MAE: 1.1546233\n",
      "Epoch 490: MSE: 2.987389, MAE: 1.2123381\n",
      "Epoch 491: MSE: 3.020701, MAE: 1.2378881\n",
      "Epoch 492: MSE: 3.261387, MAE: 1.2605673\n",
      "Epoch 493: MSE: 2.2591074, MAE: 1.1740031\n",
      "Epoch 494: MSE: 4.4425573, MAE: 1.3470224\n",
      "Epoch 495: MSE: 2.1438608, MAE: 1.0001656\n",
      "Epoch 496: MSE: 2.7795844, MAE: 1.276427\n",
      "Epoch 497: MSE: 3.1884038, MAE: 1.3711438\n",
      "Epoch 498: MSE: 2.9160285, MAE: 1.1612499\n",
      "Epoch 499: MSE: 2.8360505, MAE: 1.1776378\n",
      "Epoch 500: MSE: 3.5579355, MAE: 1.2152667\n"
     ]
    }
   ],
   "source": [
    "for epoch in 1...epochCount {\n",
    "    var epochLoss: Float = 0\n",
    "    var epochMAE: Float = 0\n",
    "    var batchCount: Int = 0\n",
    "    var batchArray = Array(repeating: false, count: numberOfBatch)\n",
    "    for batch in 0..<numberOfBatch {\n",
    "        var r = batch\n",
    "        if shuffle {\n",
    "            while true {\n",
    "                r = Int.random(in: 0..<numberOfBatch)\n",
    "                if !batchArray[r] {\n",
    "                    batchArray[r] = true\n",
    "                    break\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        let batchStart = r * batchSize\n",
    "        let batchEnd = min(numTrainRecords, batchStart + batchSize)\n",
    "        let (loss, grad) = model.valueWithGradient { (model: RegressionModel) -> Tensor<Float> in\n",
    "            let multiInput = MultiInputs(numerical: [XNumericalTrain[batchStart..<batchEnd]],\n",
    "                                         categorical: [XCategoricalTrain[0][batchStart..<batchEnd],\n",
    "                                                       XCategoricalTrain[1][batchStart..<batchEnd]])\n",
    "            let logits = model(multiInput)\n",
    "            return meanSquaredError(predicted: logits, expected: YTrain[batchStart..<batchEnd])\n",
    "        }\n",
    "        optimizer.update(&model, along: grad)\n",
    "        \n",
    "        let multiInput = MultiInputs(numerical: [XNumericalTrain[batchStart..<batchEnd]],\n",
    "                                     categorical: [XCategoricalTrain[0][batchStart..<batchEnd],\n",
    "                                                   XCategoricalTrain[1][batchStart..<batchEnd]])\n",
    "        let logits = model(multiInput)\n",
    "        epochMAE += mae(predictions: logits, truths: YTrain[batchStart..<batchEnd])\n",
    "        epochLoss += loss.scalarized()\n",
    "        batchCount += 1\n",
    "    }\n",
    "    epochMAE /= Float(batchCount)\n",
    "    epochLoss /= Float(batchCount)\n",
    "\n",
    "    print(\"Epoch \\(epoch): MSE: \\(epochLoss), MAE: \\(epochMAE)\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.44058493, MAE: 0.045618672\r\n"
     ]
    }
   ],
   "source": [
    "Context.local.learningPhase = .inference\n",
    "\n",
    "let multiInputTest = MultiInputs(numerical: [XNumericalTest],\n",
    "                                 categorical: [XCategoricalTest[0],\n",
    "                                               XCategoricalTest[1]])\n",
    "\n",
    "let prediction = model(multiInputTest)\n",
    "\n",
    "let predictionMse = meanSquaredError(predicted: prediction, expected: YTest).scalarized()/Float(numTestRecords)\n",
    "let predictionMae = mae(predictions: prediction, truths: YTest)/Float(numTestRecords)\n",
    "\n",
    "print(\"MSE: \\(predictionMse), MAE: \\(predictionMae)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fkf29HLlohIP"
   },
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2] [9, 5]\r\n"
     ]
    }
   ],
   "source": [
    "print(model.embedding1.embeddings.shape, model.embedding2.embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "let coremlModel = Model(version: 4,\n",
    "                        shortDescription: \"Regression\",\n",
    "                        author: \"Jacopo Mangiavacchi\",\n",
    "                        license: \"MIT\",\n",
    "                        userDefined: [\"SwiftCoremltoolsVersion\" : \"0.0.6\"]) {\n",
    "    Input(name: \"numericalInput\", shape: [11])\n",
    "    Input(name: \"categoricalInput1\", shape: [2])\n",
    "    Input(name: \"categoricalInput2\", shape: [9])\n",
    "    Output(name: \"output\", shape: [1])\n",
    "    NeuralNetwork {\n",
    "        Embedding(name: \"embedding1\",\n",
    "                     input: [\"categoricalInput1\"],\n",
    "                     output: [\"outEmbedding1\"],\n",
    "                     weight: model.embedding1.embeddings.transposed().flattened().scalars,\n",
    "                     inputDim: 2,\n",
    "                     outputChannels: 2)\n",
    "        Permute(name: \"permute1\",\n",
    "                     input: [\"outEmbedding1\"],\n",
    "                     output: [\"outPermute1\"],\n",
    "                     axis: [2, 1, 0, 3])\n",
    "        Flatten(name: \"flatten1\",\n",
    "                     input: [\"outPermute1\"],\n",
    "                     output: [\"outFlatten1\"],\n",
    "                     mode: .last)\n",
    "        Embedding(name: \"embedding2\",\n",
    "                     input: [\"categoricalInput2\"],\n",
    "                     output: [\"outEmbedding2\"],\n",
    "                     weight: model.embedding2.embeddings.transposed().flattened().scalars,\n",
    "                     inputDim: 9,\n",
    "                     outputChannels: 5)\n",
    "        Permute(name: \"permute2\",\n",
    "                     input: [\"outEmbedding2\"],\n",
    "                     output: [\"outPermute2\"],\n",
    "                     axis: [2, 1, 0, 3])\n",
    "        Flatten(name: \"flatten2\",\n",
    "                     input: [\"outPermute2\"],\n",
    "                     output: [\"outFlatten2\"],\n",
    "                     mode: .last)\n",
    "        Concat(name: \"concat\",\n",
    "                     input: [\"numericInput\", \"outFlatten1\", \"outFlatten2\"],\n",
    "                     output: [\"outConcat\"])\n",
    "        InnerProduct(name: \"dense1\",\n",
    "                     input: [\"outConcat\"],\n",
    "                     output: [\"outDense1\"],\n",
    "                     weight: model.allInputConcatLayer.weight.transposed().flattened().scalars,\n",
    "                     bias: model.allInputConcatLayer.bias.flattened().scalars,\n",
    "                     inputChannels: 60,\n",
    "                     outputChannels: 128)\n",
    "        ReLu(name: \"Relu1\",\n",
    "             input: [\"outDense1\"],\n",
    "             output: [\"outRelu1\"])\n",
    "        InnerProduct(name: \"dense2\",\n",
    "                     input: [\"outRelu1\"],\n",
    "                     output: [\"outDense2\"],\n",
    "                     weight: model.hiddenLayer.weight.transposed().flattened().scalars,\n",
    "                     bias: model.hiddenLayer.bias.flattened().scalars,\n",
    "                     inputChannels: 128,\n",
    "                     outputChannels: 32)\n",
    "        ReLu(name: \"Relu2\",\n",
    "             input: [\"outDense2\"],\n",
    "             output: [\"outRelu2\"])\n",
    "        InnerProduct(name: \"dense3\",\n",
    "                     input: [\"outRelu2\"],\n",
    "                     output: [\"output\"],\n",
    "                     weight: model.outputLayer.weight.transposed().flattened().scalars,\n",
    "                     bias: model.outputLayer.bias.flattened().scalars,\n",
    "                     inputChannels: 32,\n",
    "                     outputChannels: 1)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "let coreMLData = coremlModel.coreMLData\n",
    "try! coreMLData!.write(to: URL(fileURLWithPath: \"./s4tf_house_simplified_trained_model.mlmodel\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "S4TF_House.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
