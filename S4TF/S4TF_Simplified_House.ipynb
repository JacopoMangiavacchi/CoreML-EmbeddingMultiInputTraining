{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(url: \"https://github.com/JacopoMangiavacchi/SwiftCoreMLTools.git\", from: \"0.0.6\")\n",
      "\t\tSwiftCoreMLTools\n",
      "\t.package(url: \"https://github.com/dduan/Just.git\", from: \"0.8.0\")\n",
      "\t\tJust\n",
      "With SwiftPM flags: ['-c', 'release']\n",
      "Working in: /tmp/tmpwcc3ynqv/swift-install\n",
      "Fetching https://github.com/JacopoMangiavacchi/SwiftCoreMLTools.git\n",
      "Fetching https://github.com/dduan/Just.git\n",
      "Fetching https://github.com/apple/swift-protobuf.git\n",
      "Cloning https://github.com/JacopoMangiavacchi/SwiftCoreMLTools.git\n",
      "Resolving https://github.com/JacopoMangiavacchi/SwiftCoreMLTools.git at 0.0.6\n",
      "Cloning https://github.com/apple/swift-protobuf.git\n",
      "Resolving https://github.com/apple/swift-protobuf.git at 1.8.0\n",
      "Cloning https://github.com/dduan/Just.git\n",
      "Resolving https://github.com/dduan/Just.git at 0.8.0\n",
      "[1/3] Compiling Just Just.swift\n",
      "[2/3] Compiling SwiftProtobuf AnyMessageStorage.swift\n",
      "[3/4] Compiling SwiftCoreMLTools Activations.swift\n",
      "[4/5] Compiling jupyterInstalledPackages jupyterInstalledPackages.swift\n",
      "[5/5] Linking libjupyterInstalledPackages.so\n",
      "Initializing Swift...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install-swiftpm-flags -c release\n",
    "%install '.package(url: \"https://github.com/JacopoMangiavacchi/SwiftCoreMLTools.git\", from: \"0.0.6\")' SwiftCoreMLTools\n",
    "%install '.package(url: \"https://github.com/dduan/Just.git\", from: \"0.8.0\")' Just"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TO4FT9bUohHx"
   },
   "outputs": [],
   "source": [
    "import Foundation\n",
    "import SwiftCoreMLTools\n",
    "import TensorFlow\n",
    "import Just"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Download\n",
    "\n",
    "Boston house prices dataset\n",
    "---------------------------\n",
    "\n",
    "**Data Set Characteristics:**  \n",
    "\n",
    "    :Number of Instances: 506 \n",
    "\n",
    "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
    "\n",
    "    :Attribute Information (in order):\n",
    "        - CRIM     per capita crime rate by town\n",
    "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "        - INDUS    proportion of non-retail business acres per town\n",
    "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "        - NOX      nitric oxides concentration (parts per 10 million)\n",
    "        - RM       average number of rooms per dwelling\n",
    "        - AGE      proportion of owner-occupied units built prior to 1940\n",
    "        - DIS      weighted distances to five Boston employment centres\n",
    "        - RAD      index of accessibility to radial highways\n",
    "        - TAX      full-value property-tax rate per ten thousand dollars\n",
    "        - PTRATIO  pupil-teacher ratio by town\n",
    "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "        - LSTAT    % lower status of the population\n",
    "        - MEDV     Median value of owner-occupied homes in a thousand dollar\n",
    "\n",
    "    :Missing Attribute Values: None\n",
    "\n",
    "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
    "\n",
    "This is a copy of UCI ML housing dataset.\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// if let cts = Just.get(URL(string: \"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\")!).content {\n",
    "//     try! cts.write(to: URL(fileURLWithPath:\"../data/housing.csv\"))\n",
    "// }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "let data = try String(contentsOfFile:\"./data/housing.csv\", encoding: String.Encoding.utf8)\n",
    "let dataRecords: [[Float]] = data.split(separator: \"\\n\").map{ String($0).split(separator: \" \").compactMap{ Float(String($0)) } }\n",
    "\n",
    "let numRecords = dataRecords.count\n",
    "let numColumns = dataRecords[0].count\n",
    "\n",
    "var index = Set<Int>()\n",
    "\n",
    "while index.count < numRecords {\n",
    "    index.insert(Int.random(in: 0..<numRecords))\n",
    "}\n",
    "\n",
    "let randomDataRecords = index.map{ dataRecords[$0] }\n",
    "\n",
    "let dataFeatures = randomDataRecords.map{ Array($0[0..<numColumns-1]) }\n",
    "let dataLabels = randomDataRecords.map{ Array($0[(numColumns-1)...]) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Numerical Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "let categoricalColumns = [3, 8]\n",
    "let numericalColumns = [0, 1, 2, 4, 5, 6, 7, 9, 10, 11, 12]\n",
    "let numCategoricalFeatures = categoricalColumns.count\n",
    "let numNumericalFeatures = numericalColumns.count\n",
    "let numLabels = 1\n",
    "\n",
    "assert(numColumns == numCategoricalFeatures + numNumericalFeatures + 1)\n",
    "\n",
    "// Get Categorical Features\n",
    "let allCategoriesValues = dataFeatures.map{ row in categoricalColumns.map{ Int32(row[$0]) } }\n",
    "                                .reduce(into: Array(repeating: [Int32](), count: 2)){ total, value in\n",
    "                                    total[0].append(value[0])\n",
    "                                    total[1].append(value[1]) }\n",
    "                                .map{ Set($0).sorted() }\n",
    "\n",
    "let categoricalFeatures = dataFeatures.map{ row in categoricalColumns.map{ Int32(row[$0]) } }\n",
    "\n",
    "// Get Numerical Features\n",
    "let numericalFeatures = dataFeatures.map{ row in numericalColumns.map{ row[$0] } }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorize Categorical Features with Ordinal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "var categoricalValues = Array(repeating: Set<Int32>(), count: 2)\n",
    "\n",
    "for record in categoricalFeatures {\n",
    "    categoricalValues[0].insert(record[0])\n",
    "    categoricalValues[1].insert(record[1])\n",
    "}\n",
    "\n",
    "let sortedCategoricalValues = [categoricalValues[0].sorted(), categoricalValues[1].sorted()]\n",
    "\n",
    "let ordinalCategoricalFeatures = categoricalFeatures.map{ [Int32(sortedCategoricalValues[0].firstIndex(of:$0[0])!), \n",
    "                                                           Int32(sortedCategoricalValues[1].firstIndex(of:$0[1])!)] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "let trainPercentage:Float = 0.8\n",
    "let numTrainRecords = Int(ceil(Float(numRecords) * trainPercentage))\n",
    "let numTestRecords = numRecords - numTrainRecords\n",
    "\n",
    "func matrixTranspose<T>(_ matrix: [[T]]) -> [[T]] {\n",
    "    if matrix.isEmpty {return matrix}\n",
    "    var result = [[T]]()\n",
    "    for index in 0..<matrix.first!.count {\n",
    "        result.append(matrix.map{$0[index]})\n",
    "    }\n",
    "    return result\n",
    "}\n",
    "\n",
    "let xCategoricalAllTrain = matrixTranspose(Array(ordinalCategoricalFeatures[0..<numTrainRecords]))\n",
    "let xCategoricalAllTest = matrixTranspose(Array(ordinalCategoricalFeatures[numTrainRecords...]))\n",
    "let xNumericalAllTrain = Array(Array(numericalFeatures[0..<numTrainRecords]).joined())\n",
    "let xNumericalAllTest = Array(Array(numericalFeatures[numTrainRecords...]).joined())\n",
    "let yAllTrain = Array(Array(dataLabels[0..<numTrainRecords]).joined())\n",
    "let yAllTest = Array(Array(dataLabels[numTrainRecords...]).joined())\n",
    "\n",
    "let XCategoricalTrain = xCategoricalAllTrain.enumerated().map{ (offset, element) in \n",
    "    Tensor<Int32>(element).reshaped(to: TensorShape([numTrainRecords, 1]))\n",
    "}\n",
    "let XCategoricalTest = xCategoricalAllTest.enumerated().map{ (offset, element) in \n",
    "    Tensor<Int32>(element).reshaped(to: TensorShape([numTestRecords, 1]))\n",
    "}\n",
    "\n",
    "let XNumericalTrainDeNorm = Tensor<Float>(xNumericalAllTrain).reshaped(to: TensorShape([numTrainRecords, numNumericalFeatures]))\n",
    "let XNumericalTestDeNorm = Tensor<Float>(xNumericalAllTest).reshaped(to: TensorShape([numTestRecords, numNumericalFeatures]))\n",
    "let YTrain = Tensor<Float>(yAllTrain).reshaped(to: TensorShape([numTrainRecords, numLabels]))\n",
    "let YTest = Tensor<Float>(yAllTest).reshaped(to: TensorShape([numTestRecords, numLabels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.7323196, 11.682716, 11.056519, 0.5521525, 6.2791896,  67.98276, 3.8403397,  407.3679,\r\n",
      "  18.457317, 354.19437, 12.740963]] [[  8.805707,  23.690788,   6.854839,  0.1145683, 0.70437694,  28.508282,  2.1304362,  168.30038,\r\n",
      "   2.1645133,   95.57281,   7.256609]]\r\n"
     ]
    }
   ],
   "source": [
    "let mean = XNumericalTrainDeNorm.mean(alongAxes: 0)\n",
    "let std = XNumericalTrainDeNorm.standardDeviation(alongAxes: 0)\n",
    "\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "let XNumericalTrain = (XNumericalTrainDeNorm - mean)/std\n",
    "let XNumericalTest = (XNumericalTestDeNorm - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shapes [405, 11] [405, 1] [405, 1] [405, 1]\r\n",
      "Testing shapes  [101, 11] [101, 1] [101, 1] [101, 1]\r\n"
     ]
    }
   ],
   "source": [
    "print(\"Training shapes \\(XNumericalTrain.shape) \\(XCategoricalTrain[0].shape) \\(XCategoricalTrain[1].shape) \\(YTrain.shape)\")\n",
    "print(\"Testing shapes  \\(XNumericalTest.shape) \\(XCategoricalTest[0].shape) \\(XCategoricalTest[1].shape) \\(YTest.shape)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HDJZCCgqohIC"
   },
   "outputs": [],
   "source": [
    "struct MultiInputs<N: Differentiable, C>: Differentiable {\n",
    "  var numerical: N\n",
    "  \n",
    "  @noDerivative\n",
    "  var categorical: C\n",
    "\n",
    "  @differentiable\n",
    "  init(numerical: N, categorical: C) {\n",
    "    self.numerical = numerical\n",
    "    self.categorical = categorical\n",
    "  }\n",
    "}\n",
    "\n",
    "struct RegressionModel: Module {\n",
    "    var embedding1 = TensorFlow.Embedding<Float>(vocabularySize: 2, embeddingSize: 2)\n",
    "    var embedding2 = TensorFlow.Embedding<Float>(vocabularySize: 9, embeddingSize: 5)\n",
    "    var allInputConcatLayer = Dense<Float>(inputSize: (11 + 2 + 5), outputSize: 64, activation: relu)\n",
    "    var hiddenLayer = Dense<Float>(inputSize: 64, outputSize: 32, activation: relu)\n",
    "    var outputLayer = Dense<Float>(inputSize: 32, outputSize: 1)\n",
    "    \n",
    "    @differentiable\n",
    "    func callAsFunction(_ input: MultiInputs<[Tensor<Float>], [Tensor<Int32>]>) -> Tensor<Float> {\n",
    "        let embeddingOutput1 = embedding1(input.categorical[0])\n",
    "        let embeddingOutput1Reshaped = embeddingOutput1.reshaped(to: \n",
    "            TensorShape([embeddingOutput1.shape[0], embeddingOutput1.shape[2]]))\n",
    "        let embeddingOutput2 = embedding2(input.categorical[1])\n",
    "        let embeddingOutput2Reshaped = embeddingOutput2.reshaped(to: \n",
    "            TensorShape([embeddingOutput2.shape[0], embeddingOutput2.shape[2]]))\n",
    "        let allConcat = Tensor<Float>(concatenating: [input.numerical[0], embeddingOutput1Reshaped, embeddingOutput2Reshaped], alongAxis: 1)\n",
    "        return allConcat.sequenced(through: allInputConcatLayer, hiddenLayer, outputLayer)\n",
    "    }\n",
    "}\n",
    "\n",
    "var model = RegressionModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JK0Vj7bSohIF"
   },
   "outputs": [],
   "source": [
    "let optimizer = RMSProp(for: model, learningRate: 0.001)\n",
    "Context.local.learningPhase = .training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gY8C7yHJohIH"
   },
   "outputs": [],
   "source": [
    "let epochCount = 500\n",
    "let batchSize = 32\n",
    "let numberOfBatch = Int(ceil(Double(numTrainRecords) / Double(batchSize)))\n",
    "let shuffle = true\n",
    "\n",
    "func mae(predictions: Tensor<Float>, truths: Tensor<Float>) -> Float {\n",
    "    return abs(Tensor<Float>(predictions - truths)).mean().scalarized()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "L9bU9HsdohIK",
    "outputId": "692b81c5-3286-4e7c-9246-eb56e6a3eaee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: MSE: 509.42477, MAE: 20.298983\n",
      "Epoch 2: MSE: 415.45248, MAE: 17.864117\n",
      "Epoch 3: MSE: 310.03607, MAE: 15.104606\n",
      "Epoch 4: MSE: 205.26285, MAE: 11.9016\n",
      "Epoch 5: MSE: 120.53741, MAE: 8.609443\n",
      "Epoch 6: MSE: 65.79524, MAE: 6.1997004\n",
      "Epoch 7: MSE: 42.678642, MAE: 4.695429\n",
      "Epoch 8: MSE: 33.683193, MAE: 4.101423\n",
      "Epoch 9: MSE: 28.45089, MAE: 3.7345552\n",
      "Epoch 10: MSE: 24.791328, MAE: 3.4785464\n",
      "Epoch 11: MSE: 22.563753, MAE: 3.2739127\n",
      "Epoch 12: MSE: 20.997671, MAE: 3.1062355\n",
      "Epoch 13: MSE: 19.714146, MAE: 3.003347\n",
      "Epoch 14: MSE: 18.874678, MAE: 2.9497476\n",
      "Epoch 15: MSE: 18.064705, MAE: 2.863904\n",
      "Epoch 16: MSE: 17.435883, MAE: 2.8042974\n",
      "Epoch 17: MSE: 16.894167, MAE: 2.7306285\n",
      "Epoch 18: MSE: 16.364544, MAE: 2.6772122\n",
      "Epoch 19: MSE: 15.788252, MAE: 2.636347\n",
      "Epoch 20: MSE: 15.3288, MAE: 2.5955412\n",
      "Epoch 21: MSE: 14.848641, MAE: 2.561652\n",
      "Epoch 22: MSE: 14.527006, MAE: 2.5185835\n",
      "Epoch 23: MSE: 14.070323, MAE: 2.4688697\n",
      "Epoch 24: MSE: 13.697621, MAE: 2.4265943\n",
      "Epoch 25: MSE: 13.397248, MAE: 2.395078\n",
      "Epoch 26: MSE: 13.069504, MAE: 2.3698518\n",
      "Epoch 27: MSE: 12.745473, MAE: 2.3459978\n",
      "Epoch 28: MSE: 12.407391, MAE: 2.311855\n",
      "Epoch 29: MSE: 12.186558, MAE: 2.279217\n",
      "Epoch 30: MSE: 11.960414, MAE: 2.2411466\n",
      "Epoch 31: MSE: 11.845382, MAE: 2.2318177\n",
      "Epoch 32: MSE: 11.520936, MAE: 2.1987126\n",
      "Epoch 33: MSE: 11.263927, MAE: 2.183203\n",
      "Epoch 34: MSE: 11.142995, MAE: 2.1592062\n",
      "Epoch 35: MSE: 10.881547, MAE: 2.1491482\n",
      "Epoch 36: MSE: 10.724138, MAE: 2.1204753\n",
      "Epoch 37: MSE: 10.57786, MAE: 2.116908\n",
      "Epoch 38: MSE: 10.399504, MAE: 2.0751703\n",
      "Epoch 39: MSE: 10.346002, MAE: 2.0770533\n",
      "Epoch 40: MSE: 10.051164, MAE: 2.0558848\n",
      "Epoch 41: MSE: 9.819116, MAE: 2.0341651\n",
      "Epoch 42: MSE: 9.793755, MAE: 2.0470083\n",
      "Epoch 43: MSE: 9.76888, MAE: 2.0269632\n",
      "Epoch 44: MSE: 9.630369, MAE: 2.005935\n",
      "Epoch 45: MSE: 9.428131, MAE: 1.9984385\n",
      "Epoch 46: MSE: 9.437506, MAE: 1.981193\n",
      "Epoch 47: MSE: 9.296289, MAE: 1.9773504\n",
      "Epoch 48: MSE: 9.0879, MAE: 1.9735101\n",
      "Epoch 49: MSE: 9.218263, MAE: 1.9608178\n",
      "Epoch 50: MSE: 9.106922, MAE: 1.9637433\n",
      "Epoch 51: MSE: 8.905324, MAE: 1.9447709\n",
      "Epoch 52: MSE: 8.87297, MAE: 1.9423598\n",
      "Epoch 53: MSE: 8.640311, MAE: 1.9237574\n",
      "Epoch 54: MSE: 8.769775, MAE: 1.9293123\n",
      "Epoch 55: MSE: 8.683402, MAE: 1.9167151\n",
      "Epoch 56: MSE: 8.610429, MAE: 1.9080669\n",
      "Epoch 57: MSE: 8.445128, MAE: 1.9058056\n",
      "Epoch 58: MSE: 8.504189, MAE: 1.8991648\n",
      "Epoch 59: MSE: 8.314216, MAE: 1.8962867\n",
      "Epoch 60: MSE: 8.285081, MAE: 1.8773258\n",
      "Epoch 61: MSE: 8.191014, MAE: 1.8769033\n",
      "Epoch 62: MSE: 8.124662, MAE: 1.8681241\n",
      "Epoch 63: MSE: 8.102263, MAE: 1.8692348\n",
      "Epoch 64: MSE: 8.010276, MAE: 1.8616983\n",
      "Epoch 65: MSE: 8.031309, MAE: 1.8552787\n",
      "Epoch 66: MSE: 7.926685, MAE: 1.8543867\n",
      "Epoch 67: MSE: 7.764285, MAE: 1.8499783\n",
      "Epoch 68: MSE: 7.7605124, MAE: 1.8514034\n",
      "Epoch 69: MSE: 7.7620893, MAE: 1.8362719\n",
      "Epoch 70: MSE: 7.6899686, MAE: 1.8481903\n",
      "Epoch 71: MSE: 7.6493826, MAE: 1.8266698\n",
      "Epoch 72: MSE: 7.5762177, MAE: 1.8235506\n",
      "Epoch 73: MSE: 7.6241207, MAE: 1.8201082\n",
      "Epoch 74: MSE: 7.537064, MAE: 1.809121\n",
      "Epoch 75: MSE: 7.4950185, MAE: 1.8018868\n",
      "Epoch 76: MSE: 7.5644197, MAE: 1.7944686\n",
      "Epoch 77: MSE: 7.230367, MAE: 1.7917016\n",
      "Epoch 78: MSE: 7.3836055, MAE: 1.7851149\n",
      "Epoch 79: MSE: 7.3458934, MAE: 1.7721238\n",
      "Epoch 80: MSE: 7.107068, MAE: 1.7887619\n",
      "Epoch 81: MSE: 7.1255817, MAE: 1.7648597\n",
      "Epoch 82: MSE: 7.166753, MAE: 1.78406\n",
      "Epoch 83: MSE: 7.064186, MAE: 1.7662563\n",
      "Epoch 84: MSE: 7.029252, MAE: 1.7584548\n",
      "Epoch 85: MSE: 7.036257, MAE: 1.7565854\n",
      "Epoch 86: MSE: 6.9532003, MAE: 1.7495787\n",
      "Epoch 87: MSE: 7.0355115, MAE: 1.748709\n",
      "Epoch 88: MSE: 6.961533, MAE: 1.745186\n",
      "Epoch 89: MSE: 6.864907, MAE: 1.7389376\n",
      "Epoch 90: MSE: 6.7238345, MAE: 1.7357959\n",
      "Epoch 91: MSE: 6.8027377, MAE: 1.7253093\n",
      "Epoch 92: MSE: 6.714656, MAE: 1.7298769\n",
      "Epoch 93: MSE: 6.586029, MAE: 1.7245795\n",
      "Epoch 94: MSE: 6.7829256, MAE: 1.729279\n",
      "Epoch 95: MSE: 6.4594584, MAE: 1.7154725\n",
      "Epoch 96: MSE: 6.705421, MAE: 1.7145722\n",
      "Epoch 97: MSE: 6.545088, MAE: 1.7087778\n",
      "Epoch 98: MSE: 6.5738125, MAE: 1.6994039\n",
      "Epoch 99: MSE: 6.512718, MAE: 1.7021537\n",
      "Epoch 100: MSE: 6.451935, MAE: 1.7043366\n",
      "Epoch 101: MSE: 6.4567876, MAE: 1.693061\n",
      "Epoch 102: MSE: 6.4367695, MAE: 1.6886508\n",
      "Epoch 103: MSE: 6.3370547, MAE: 1.6839191\n",
      "Epoch 104: MSE: 6.254255, MAE: 1.6755716\n",
      "Epoch 105: MSE: 6.183922, MAE: 1.67508\n",
      "Epoch 106: MSE: 6.219952, MAE: 1.6681869\n",
      "Epoch 107: MSE: 6.0715375, MAE: 1.6506644\n",
      "Epoch 108: MSE: 6.263237, MAE: 1.6646583\n",
      "Epoch 109: MSE: 6.212643, MAE: 1.6660606\n",
      "Epoch 110: MSE: 6.0914173, MAE: 1.659902\n",
      "Epoch 111: MSE: 6.017834, MAE: 1.6431738\n",
      "Epoch 112: MSE: 6.1421556, MAE: 1.6578946\n",
      "Epoch 113: MSE: 5.8771744, MAE: 1.6297188\n",
      "Epoch 114: MSE: 6.0352526, MAE: 1.6441321\n",
      "Epoch 115: MSE: 6.0947657, MAE: 1.6448009\n",
      "Epoch 116: MSE: 5.9315195, MAE: 1.6455451\n",
      "Epoch 117: MSE: 5.8046546, MAE: 1.62128\n",
      "Epoch 118: MSE: 5.8455863, MAE: 1.6314552\n",
      "Epoch 119: MSE: 5.8807397, MAE: 1.6185613\n",
      "Epoch 120: MSE: 5.745446, MAE: 1.6116025\n",
      "Epoch 121: MSE: 5.8701396, MAE: 1.6138809\n",
      "Epoch 122: MSE: 5.699353, MAE: 1.6063279\n",
      "Epoch 123: MSE: 5.7002764, MAE: 1.6076745\n",
      "Epoch 124: MSE: 5.794955, MAE: 1.606783\n",
      "Epoch 125: MSE: 5.529113, MAE: 1.5846677\n",
      "Epoch 126: MSE: 5.7919383, MAE: 1.6005033\n",
      "Epoch 127: MSE: 5.585397, MAE: 1.5858823\n",
      "Epoch 128: MSE: 5.501707, MAE: 1.5842942\n",
      "Epoch 129: MSE: 5.5713086, MAE: 1.5773252\n",
      "Epoch 130: MSE: 5.533137, MAE: 1.5713189\n",
      "Epoch 131: MSE: 5.5032296, MAE: 1.5723546\n",
      "Epoch 132: MSE: 5.484264, MAE: 1.5634757\n",
      "Epoch 133: MSE: 5.3850284, MAE: 1.562304\n",
      "Epoch 134: MSE: 5.438483, MAE: 1.5737431\n",
      "Epoch 135: MSE: 5.305714, MAE: 1.5634563\n",
      "Epoch 136: MSE: 5.371136, MAE: 1.5658963\n",
      "Epoch 137: MSE: 5.396617, MAE: 1.5400379\n",
      "Epoch 138: MSE: 5.3363266, MAE: 1.5598044\n",
      "Epoch 139: MSE: 5.33929, MAE: 1.5427953\n",
      "Epoch 140: MSE: 5.2371845, MAE: 1.546885\n",
      "Epoch 141: MSE: 5.2228694, MAE: 1.5386767\n",
      "Epoch 142: MSE: 5.27876, MAE: 1.5412184\n",
      "Epoch 143: MSE: 5.1831784, MAE: 1.5235442\n",
      "Epoch 144: MSE: 5.1071877, MAE: 1.5274574\n",
      "Epoch 145: MSE: 5.1904683, MAE: 1.5184194\n",
      "Epoch 146: MSE: 5.1241856, MAE: 1.5068034\n",
      "Epoch 147: MSE: 5.134662, MAE: 1.5293598\n",
      "Epoch 148: MSE: 4.942986, MAE: 1.5171936\n",
      "Epoch 149: MSE: 5.1602097, MAE: 1.5051217\n",
      "Epoch 150: MSE: 4.9926476, MAE: 1.4998778\n",
      "Epoch 151: MSE: 4.960808, MAE: 1.5078021\n",
      "Epoch 152: MSE: 5.115618, MAE: 1.4968625\n",
      "Epoch 153: MSE: 4.937528, MAE: 1.4988658\n",
      "Epoch 154: MSE: 5.0099382, MAE: 1.4931183\n",
      "Epoch 155: MSE: 4.9522476, MAE: 1.485565\n",
      "Epoch 156: MSE: 4.9660716, MAE: 1.497884\n",
      "Epoch 157: MSE: 4.878377, MAE: 1.4820902\n",
      "Epoch 158: MSE: 4.803934, MAE: 1.4696363\n",
      "Epoch 159: MSE: 4.856269, MAE: 1.4649093\n",
      "Epoch 160: MSE: 4.825384, MAE: 1.4687282\n",
      "Epoch 161: MSE: 4.7361336, MAE: 1.454452\n",
      "Epoch 162: MSE: 4.793873, MAE: 1.4659634\n",
      "Epoch 163: MSE: 4.737525, MAE: 1.4585855\n",
      "Epoch 164: MSE: 4.749488, MAE: 1.4544874\n",
      "Epoch 165: MSE: 4.7646704, MAE: 1.4572457\n",
      "Epoch 166: MSE: 4.5799866, MAE: 1.4544421\n",
      "Epoch 167: MSE: 4.647856, MAE: 1.4531269\n",
      "Epoch 168: MSE: 4.6034803, MAE: 1.4563757\n",
      "Epoch 169: MSE: 4.660213, MAE: 1.4491757\n",
      "Epoch 170: MSE: 4.5830355, MAE: 1.4384669\n",
      "Epoch 171: MSE: 4.574148, MAE: 1.4321963\n",
      "Epoch 172: MSE: 4.595953, MAE: 1.4231095\n",
      "Epoch 173: MSE: 4.51832, MAE: 1.4332948\n",
      "Epoch 174: MSE: 4.5236993, MAE: 1.4235368\n",
      "Epoch 175: MSE: 4.5235143, MAE: 1.4242839\n",
      "Epoch 176: MSE: 4.5160365, MAE: 1.4167582\n",
      "Epoch 177: MSE: 4.4725447, MAE: 1.4066399\n",
      "Epoch 178: MSE: 4.5055447, MAE: 1.4090613\n",
      "Epoch 179: MSE: 4.3077145, MAE: 1.3981413\n",
      "Epoch 180: MSE: 4.3728833, MAE: 1.4146298\n",
      "Epoch 181: MSE: 4.428767, MAE: 1.398807\n",
      "Epoch 182: MSE: 4.3775086, MAE: 1.3923341\n",
      "Epoch 183: MSE: 4.3719387, MAE: 1.3938901\n",
      "Epoch 184: MSE: 4.3129373, MAE: 1.395259\n",
      "Epoch 185: MSE: 4.161652, MAE: 1.3819152\n",
      "Epoch 186: MSE: 4.257331, MAE: 1.3788897\n",
      "Epoch 187: MSE: 4.2429547, MAE: 1.384015\n",
      "Epoch 188: MSE: 4.237986, MAE: 1.3904874\n",
      "Epoch 189: MSE: 4.1062937, MAE: 1.3592516\n",
      "Epoch 190: MSE: 4.16696, MAE: 1.3655986\n",
      "Epoch 191: MSE: 4.318042, MAE: 1.3654679\n",
      "Epoch 192: MSE: 4.152802, MAE: 1.377281\n",
      "Epoch 193: MSE: 4.105421, MAE: 1.3548608\n",
      "Epoch 194: MSE: 4.1947713, MAE: 1.3633602\n",
      "Epoch 195: MSE: 4.1424055, MAE: 1.3586996\n",
      "Epoch 196: MSE: 4.050276, MAE: 1.3349583\n",
      "Epoch 197: MSE: 4.06985, MAE: 1.3501891\n",
      "Epoch 198: MSE: 4.0952015, MAE: 1.3513656\n",
      "Epoch 199: MSE: 3.9026213, MAE: 1.3368089\n",
      "Epoch 200: MSE: 3.9456341, MAE: 1.3404452\n",
      "Epoch 201: MSE: 4.0288095, MAE: 1.325762\n",
      "Epoch 202: MSE: 3.946784, MAE: 1.3279538\n",
      "Epoch 203: MSE: 3.965803, MAE: 1.3297362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204: MSE: 3.9368362, MAE: 1.3268192\n",
      "Epoch 205: MSE: 3.9429255, MAE: 1.3200129\n",
      "Epoch 206: MSE: 3.9325767, MAE: 1.3187277\n",
      "Epoch 207: MSE: 3.9360974, MAE: 1.3254979\n",
      "Epoch 208: MSE: 3.8871424, MAE: 1.3131541\n",
      "Epoch 209: MSE: 3.9413524, MAE: 1.3049474\n",
      "Epoch 210: MSE: 3.890732, MAE: 1.3031197\n",
      "Epoch 211: MSE: 3.8245835, MAE: 1.3058529\n",
      "Epoch 212: MSE: 3.7643776, MAE: 1.2978442\n",
      "Epoch 213: MSE: 3.866338, MAE: 1.3014371\n",
      "Epoch 214: MSE: 3.7299643, MAE: 1.283544\n",
      "Epoch 215: MSE: 3.8389654, MAE: 1.300832\n",
      "Epoch 216: MSE: 3.741732, MAE: 1.2800835\n",
      "Epoch 217: MSE: 3.7434015, MAE: 1.2786516\n",
      "Epoch 218: MSE: 3.7460153, MAE: 1.2725298\n",
      "Epoch 219: MSE: 3.616992, MAE: 1.258021\n",
      "Epoch 220: MSE: 3.6546946, MAE: 1.2885996\n",
      "Epoch 221: MSE: 3.8217356, MAE: 1.2744024\n",
      "Epoch 222: MSE: 3.6022692, MAE: 1.2644615\n",
      "Epoch 223: MSE: 3.653287, MAE: 1.2596035\n",
      "Epoch 224: MSE: 3.6572492, MAE: 1.2642125\n",
      "Epoch 225: MSE: 3.582483, MAE: 1.2623336\n",
      "Epoch 226: MSE: 3.524511, MAE: 1.2478155\n",
      "Epoch 227: MSE: 3.6919184, MAE: 1.2662277\n",
      "Epoch 228: MSE: 3.6680303, MAE: 1.2528154\n",
      "Epoch 229: MSE: 3.609355, MAE: 1.2500056\n",
      "Epoch 230: MSE: 3.51405, MAE: 1.2446729\n",
      "Epoch 231: MSE: 3.4755328, MAE: 1.2393812\n",
      "Epoch 232: MSE: 3.4278364, MAE: 1.2282506\n",
      "Epoch 233: MSE: 3.5165493, MAE: 1.2290673\n",
      "Epoch 234: MSE: 3.5315077, MAE: 1.2288158\n",
      "Epoch 235: MSE: 3.493922, MAE: 1.2355498\n",
      "Epoch 236: MSE: 3.5423636, MAE: 1.228465\n",
      "Epoch 237: MSE: 3.3326404, MAE: 1.2119478\n",
      "Epoch 238: MSE: 3.4488626, MAE: 1.2309343\n",
      "Epoch 239: MSE: 3.3783674, MAE: 1.2242414\n",
      "Epoch 240: MSE: 3.389605, MAE: 1.2118299\n",
      "Epoch 241: MSE: 3.4093106, MAE: 1.2242459\n",
      "Epoch 242: MSE: 3.4615908, MAE: 1.2050836\n",
      "Epoch 243: MSE: 3.3563237, MAE: 1.2016518\n",
      "Epoch 244: MSE: 3.3579786, MAE: 1.2158458\n",
      "Epoch 245: MSE: 3.4033084, MAE: 1.2090471\n",
      "Epoch 246: MSE: 3.3422542, MAE: 1.2123438\n",
      "Epoch 247: MSE: 3.3281255, MAE: 1.1967018\n",
      "Epoch 248: MSE: 3.2219338, MAE: 1.1793933\n",
      "Epoch 249: MSE: 3.303772, MAE: 1.1993029\n",
      "Epoch 250: MSE: 3.2789485, MAE: 1.1877829\n",
      "Epoch 251: MSE: 3.206051, MAE: 1.1871016\n",
      "Epoch 252: MSE: 3.3597414, MAE: 1.1859059\n",
      "Epoch 253: MSE: 3.2091422, MAE: 1.183388\n",
      "Epoch 254: MSE: 3.1893601, MAE: 1.1698006\n",
      "Epoch 255: MSE: 3.2546158, MAE: 1.1863784\n",
      "Epoch 256: MSE: 3.2916794, MAE: 1.1787025\n",
      "Epoch 257: MSE: 3.0956063, MAE: 1.1588707\n",
      "Epoch 258: MSE: 3.2390835, MAE: 1.1784408\n",
      "Epoch 259: MSE: 3.2330835, MAE: 1.184174\n",
      "Epoch 260: MSE: 3.163344, MAE: 1.1557515\n",
      "Epoch 261: MSE: 3.0860317, MAE: 1.1607796\n",
      "Epoch 262: MSE: 3.1433053, MAE: 1.1583883\n",
      "Epoch 263: MSE: 3.1697106, MAE: 1.1552558\n",
      "Epoch 264: MSE: 3.0950952, MAE: 1.1594144\n",
      "Epoch 265: MSE: 3.1389706, MAE: 1.1488236\n",
      "Epoch 266: MSE: 3.2065263, MAE: 1.1593337\n",
      "Epoch 267: MSE: 3.0573282, MAE: 1.1473662\n",
      "Epoch 268: MSE: 3.168796, MAE: 1.1665063\n",
      "Epoch 269: MSE: 3.0918086, MAE: 1.145165\n",
      "Epoch 270: MSE: 3.0738924, MAE: 1.155465\n",
      "Epoch 271: MSE: 3.0280547, MAE: 1.1329877\n",
      "Epoch 272: MSE: 3.0325494, MAE: 1.1400464\n",
      "Epoch 273: MSE: 3.1461122, MAE: 1.1458749\n",
      "Epoch 274: MSE: 3.0182354, MAE: 1.129382\n",
      "Epoch 275: MSE: 3.0159552, MAE: 1.1324302\n",
      "Epoch 276: MSE: 2.9327228, MAE: 1.129393\n",
      "Epoch 277: MSE: 3.0908575, MAE: 1.1211766\n",
      "Epoch 278: MSE: 3.0017166, MAE: 1.1284698\n",
      "Epoch 279: MSE: 3.0510957, MAE: 1.1428702\n",
      "Epoch 280: MSE: 2.9983041, MAE: 1.1271837\n",
      "Epoch 281: MSE: 2.9630854, MAE: 1.1170532\n",
      "Epoch 282: MSE: 2.9335914, MAE: 1.1105317\n",
      "Epoch 283: MSE: 2.833928, MAE: 1.088679\n",
      "Epoch 284: MSE: 2.998519, MAE: 1.116225\n",
      "Epoch 285: MSE: 2.9352992, MAE: 1.1048154\n",
      "Epoch 286: MSE: 2.8459392, MAE: 1.100624\n",
      "Epoch 287: MSE: 2.9790251, MAE: 1.1205313\n",
      "Epoch 288: MSE: 2.9238145, MAE: 1.12541\n",
      "Epoch 289: MSE: 2.8816714, MAE: 1.1073239\n",
      "Epoch 290: MSE: 2.7448366, MAE: 1.0836424\n",
      "Epoch 291: MSE: 2.8428233, MAE: 1.0938174\n",
      "Epoch 292: MSE: 2.8274252, MAE: 1.0997\n",
      "Epoch 293: MSE: 2.8587663, MAE: 1.1092671\n",
      "Epoch 294: MSE: 2.8456259, MAE: 1.0769469\n",
      "Epoch 295: MSE: 2.8861673, MAE: 1.0923182\n",
      "Epoch 296: MSE: 2.8090189, MAE: 1.092537\n",
      "Epoch 297: MSE: 2.8398874, MAE: 1.075186\n",
      "Epoch 298: MSE: 2.8207781, MAE: 1.0900502\n",
      "Epoch 299: MSE: 2.7921333, MAE: 1.0824813\n",
      "Epoch 300: MSE: 2.7537444, MAE: 1.0746458\n",
      "Epoch 301: MSE: 2.7794352, MAE: 1.0830587\n",
      "Epoch 302: MSE: 2.8457108, MAE: 1.0840402\n",
      "Epoch 303: MSE: 2.7875135, MAE: 1.0800171\n",
      "Epoch 304: MSE: 2.7306352, MAE: 1.0755122\n",
      "Epoch 305: MSE: 2.8753886, MAE: 1.0773898\n",
      "Epoch 306: MSE: 2.7506957, MAE: 1.0724369\n",
      "Epoch 307: MSE: 2.6934254, MAE: 1.0704907\n",
      "Epoch 308: MSE: 2.7615206, MAE: 1.0793545\n",
      "Epoch 309: MSE: 2.6836255, MAE: 1.0707133\n",
      "Epoch 310: MSE: 2.722205, MAE: 1.0645335\n",
      "Epoch 311: MSE: 2.8266945, MAE: 1.0690936\n",
      "Epoch 312: MSE: 2.7298884, MAE: 1.0551414\n",
      "Epoch 313: MSE: 2.6385057, MAE: 1.0634451\n",
      "Epoch 314: MSE: 2.7565587, MAE: 1.0638032\n",
      "Epoch 315: MSE: 2.680805, MAE: 1.0641501\n",
      "Epoch 316: MSE: 2.630189, MAE: 1.0499177\n",
      "Epoch 317: MSE: 2.6512427, MAE: 1.0610937\n",
      "Epoch 318: MSE: 2.672417, MAE: 1.0497862\n",
      "Epoch 319: MSE: 2.673318, MAE: 1.0441474\n",
      "Epoch 320: MSE: 2.5342252, MAE: 1.0363375\n",
      "Epoch 321: MSE: 2.7234256, MAE: 1.0677712\n",
      "Epoch 322: MSE: 2.6369154, MAE: 1.0601804\n",
      "Epoch 323: MSE: 2.564051, MAE: 1.0440228\n",
      "Epoch 324: MSE: 2.5575175, MAE: 1.0425994\n",
      "Epoch 325: MSE: 2.7091424, MAE: 1.0564702\n",
      "Epoch 326: MSE: 2.499271, MAE: 1.0303547\n",
      "Epoch 327: MSE: 2.5369153, MAE: 1.0279248\n",
      "Epoch 328: MSE: 2.6473098, MAE: 1.0720205\n",
      "Epoch 329: MSE: 2.646576, MAE: 1.0363717\n",
      "Epoch 330: MSE: 2.7011983, MAE: 1.0448966\n",
      "Epoch 331: MSE: 2.5291002, MAE: 1.0356272\n",
      "Epoch 332: MSE: 2.559977, MAE: 1.0310439\n",
      "Epoch 333: MSE: 2.4980867, MAE: 1.0407455\n",
      "Epoch 334: MSE: 2.6604755, MAE: 1.0275565\n",
      "Epoch 335: MSE: 2.575029, MAE: 1.0284469\n",
      "Epoch 336: MSE: 2.5335655, MAE: 1.0261023\n",
      "Epoch 337: MSE: 2.4918852, MAE: 1.0123509\n",
      "Epoch 338: MSE: 2.5342982, MAE: 1.0353409\n",
      "Epoch 339: MSE: 2.5547204, MAE: 1.0280087\n",
      "Epoch 340: MSE: 2.6006713, MAE: 1.0274937\n",
      "Epoch 341: MSE: 2.487902, MAE: 1.0246856\n",
      "Epoch 342: MSE: 2.384951, MAE: 1.0101485\n",
      "Epoch 343: MSE: 2.565856, MAE: 1.0152686\n",
      "Epoch 344: MSE: 2.3911219, MAE: 1.0055935\n",
      "Epoch 345: MSE: 2.336032, MAE: 0.99559724\n",
      "Epoch 346: MSE: 2.5151117, MAE: 1.02752\n",
      "Epoch 347: MSE: 2.4756494, MAE: 0.9913517\n",
      "Epoch 348: MSE: 2.3922305, MAE: 1.0001386\n",
      "Epoch 349: MSE: 2.397341, MAE: 1.007592\n",
      "Epoch 350: MSE: 2.3617036, MAE: 1.0271039\n",
      "Epoch 351: MSE: 2.4987628, MAE: 1.0039328\n",
      "Epoch 352: MSE: 2.4388623, MAE: 1.0029321\n",
      "Epoch 353: MSE: 2.4275362, MAE: 1.0034758\n",
      "Epoch 354: MSE: 2.323934, MAE: 0.99967873\n",
      "Epoch 355: MSE: 2.414182, MAE: 1.0115118\n",
      "Epoch 356: MSE: 2.4194784, MAE: 0.9948617\n",
      "Epoch 357: MSE: 2.33772, MAE: 0.99641776\n",
      "Epoch 358: MSE: 2.3319373, MAE: 0.9998944\n",
      "Epoch 359: MSE: 2.3401775, MAE: 0.9918515\n",
      "Epoch 360: MSE: 2.319797, MAE: 0.98166996\n",
      "Epoch 361: MSE: 2.2873387, MAE: 1.000572\n",
      "Epoch 362: MSE: 2.4604938, MAE: 1.0015701\n",
      "Epoch 363: MSE: 2.415328, MAE: 0.9759109\n",
      "Epoch 364: MSE: 2.3927376, MAE: 0.9816513\n",
      "Epoch 365: MSE: 2.346771, MAE: 0.9754092\n",
      "Epoch 366: MSE: 2.3560987, MAE: 0.9868093\n",
      "Epoch 367: MSE: 2.4000494, MAE: 0.99479485\n",
      "Epoch 368: MSE: 2.2885203, MAE: 0.9889874\n",
      "Epoch 369: MSE: 2.2835298, MAE: 0.97966796\n",
      "Epoch 370: MSE: 2.3836203, MAE: 0.9842051\n",
      "Epoch 371: MSE: 2.3626795, MAE: 0.99381053\n",
      "Epoch 372: MSE: 2.3355067, MAE: 0.9680246\n",
      "Epoch 373: MSE: 2.3322473, MAE: 0.9705206\n",
      "Epoch 374: MSE: 2.3490367, MAE: 0.98098826\n",
      "Epoch 375: MSE: 2.269504, MAE: 0.9751863\n",
      "Epoch 376: MSE: 2.354509, MAE: 0.9968234\n",
      "Epoch 377: MSE: 2.2285993, MAE: 0.9664307\n",
      "Epoch 378: MSE: 2.2907984, MAE: 0.98344445\n",
      "Epoch 379: MSE: 2.2593112, MAE: 0.9875771\n",
      "Epoch 380: MSE: 2.3442175, MAE: 0.99598604\n",
      "Epoch 381: MSE: 2.2673936, MAE: 0.9717562\n",
      "Epoch 382: MSE: 2.2223732, MAE: 0.96602374\n",
      "Epoch 383: MSE: 2.304619, MAE: 0.96398896\n",
      "Epoch 384: MSE: 2.2189276, MAE: 0.9469148\n",
      "Epoch 385: MSE: 2.2055335, MAE: 0.9648068\n",
      "Epoch 386: MSE: 2.2274404, MAE: 0.9648173\n",
      "Epoch 387: MSE: 2.2909286, MAE: 0.98689336\n",
      "Epoch 388: MSE: 2.2644348, MAE: 0.9649003\n",
      "Epoch 389: MSE: 2.159563, MAE: 0.9421945\n",
      "Epoch 390: MSE: 2.2754436, MAE: 0.9592416\n",
      "Epoch 391: MSE: 2.1836953, MAE: 0.9604985\n",
      "Epoch 392: MSE: 2.2700427, MAE: 0.9631983\n",
      "Epoch 393: MSE: 2.1993358, MAE: 0.96295387\n",
      "Epoch 394: MSE: 2.1992955, MAE: 0.953093\n",
      "Epoch 395: MSE: 2.2265527, MAE: 0.9577144\n",
      "Epoch 396: MSE: 2.2258246, MAE: 0.94841135\n",
      "Epoch 397: MSE: 2.1179514, MAE: 0.9494435\n",
      "Epoch 398: MSE: 2.2456656, MAE: 0.963462\n",
      "Epoch 399: MSE: 2.2704332, MAE: 0.97024906\n",
      "Epoch 400: MSE: 2.1027162, MAE: 0.9329777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 401: MSE: 2.174545, MAE: 0.95603955\n",
      "Epoch 402: MSE: 2.1745489, MAE: 0.9498168\n",
      "Epoch 403: MSE: 2.12718, MAE: 0.9386047\n",
      "Epoch 404: MSE: 2.2721417, MAE: 0.9544712\n",
      "Epoch 405: MSE: 2.1209373, MAE: 0.93142253\n",
      "Epoch 406: MSE: 2.1258137, MAE: 0.9257672\n",
      "Epoch 407: MSE: 2.112322, MAE: 0.9288239\n",
      "Epoch 408: MSE: 2.195156, MAE: 0.9769999\n",
      "Epoch 409: MSE: 2.2485585, MAE: 0.9282168\n",
      "Epoch 410: MSE: 2.083832, MAE: 0.94158196\n",
      "Epoch 411: MSE: 2.113854, MAE: 0.93981904\n",
      "Epoch 412: MSE: 1.9812143, MAE: 0.9247312\n",
      "Epoch 413: MSE: 2.1703284, MAE: 0.9639997\n",
      "Epoch 414: MSE: 2.0970042, MAE: 0.9147361\n",
      "Epoch 415: MSE: 2.179111, MAE: 0.94063705\n",
      "Epoch 416: MSE: 2.2293305, MAE: 0.94249755\n",
      "Epoch 417: MSE: 2.0280595, MAE: 0.9107259\n",
      "Epoch 418: MSE: 2.1353292, MAE: 0.9448258\n",
      "Epoch 419: MSE: 2.1035933, MAE: 0.9340265\n",
      "Epoch 420: MSE: 2.1437876, MAE: 0.94043\n",
      "Epoch 421: MSE: 2.0260217, MAE: 0.9310862\n",
      "Epoch 422: MSE: 2.1398954, MAE: 0.9434151\n",
      "Epoch 423: MSE: 2.072306, MAE: 0.9121071\n",
      "Epoch 424: MSE: 2.0328891, MAE: 0.9029232\n",
      "Epoch 425: MSE: 2.0686696, MAE: 0.9230313\n",
      "Epoch 426: MSE: 2.0767226, MAE: 0.90748316\n",
      "Epoch 427: MSE: 2.0370085, MAE: 0.91430557\n",
      "Epoch 428: MSE: 2.0417018, MAE: 0.9134379\n",
      "Epoch 429: MSE: 2.0066578, MAE: 0.9040397\n",
      "Epoch 430: MSE: 2.1212025, MAE: 0.92909724\n",
      "Epoch 431: MSE: 2.001665, MAE: 0.89611477\n",
      "Epoch 432: MSE: 2.0334158, MAE: 0.9076944\n",
      "Epoch 433: MSE: 2.0548205, MAE: 0.9003619\n",
      "Epoch 434: MSE: 2.124715, MAE: 0.915796\n",
      "Epoch 435: MSE: 2.0554762, MAE: 0.9297893\n",
      "Epoch 436: MSE: 2.0772173, MAE: 0.9203708\n",
      "Epoch 437: MSE: 2.0293653, MAE: 0.9245012\n",
      "Epoch 438: MSE: 1.9907539, MAE: 0.9167168\n",
      "Epoch 439: MSE: 1.9468553, MAE: 0.8933624\n",
      "Epoch 440: MSE: 2.0647407, MAE: 0.9167343\n",
      "Epoch 441: MSE: 1.9260197, MAE: 0.88196725\n",
      "Epoch 442: MSE: 1.9864815, MAE: 0.9125655\n",
      "Epoch 443: MSE: 2.0711057, MAE: 0.9338691\n",
      "Epoch 444: MSE: 1.9072031, MAE: 0.8942464\n",
      "Epoch 445: MSE: 1.9744506, MAE: 0.913619\n",
      "Epoch 446: MSE: 1.8551666, MAE: 0.8783429\n",
      "Epoch 447: MSE: 2.0285397, MAE: 0.91018516\n",
      "Epoch 448: MSE: 2.0503848, MAE: 0.91641957\n",
      "Epoch 449: MSE: 1.9148635, MAE: 0.88477606\n",
      "Epoch 450: MSE: 1.9911994, MAE: 0.9060316\n",
      "Epoch 451: MSE: 1.8916109, MAE: 0.8895216\n",
      "Epoch 452: MSE: 1.9030914, MAE: 0.8965848\n",
      "Epoch 453: MSE: 1.9385896, MAE: 0.8922161\n",
      "Epoch 454: MSE: 1.9263341, MAE: 0.8793357\n",
      "Epoch 455: MSE: 1.8907158, MAE: 0.8952386\n",
      "Epoch 456: MSE: 1.8874705, MAE: 0.8864384\n",
      "Epoch 457: MSE: 1.9532453, MAE: 0.9126962\n",
      "Epoch 458: MSE: 1.8248713, MAE: 0.86292756\n",
      "Epoch 459: MSE: 1.9368688, MAE: 0.91502124\n",
      "Epoch 460: MSE: 1.8535671, MAE: 0.89543587\n",
      "Epoch 461: MSE: 2.0033314, MAE: 0.90725774\n",
      "Epoch 462: MSE: 1.9191467, MAE: 0.8959912\n",
      "Epoch 463: MSE: 1.8898699, MAE: 0.87419456\n",
      "Epoch 464: MSE: 1.8081131, MAE: 0.85560095\n",
      "Epoch 465: MSE: 2.033767, MAE: 0.89004654\n",
      "Epoch 466: MSE: 1.8326392, MAE: 0.8583466\n",
      "Epoch 467: MSE: 1.87401, MAE: 0.8694381\n",
      "Epoch 468: MSE: 1.8433036, MAE: 0.8742476\n",
      "Epoch 469: MSE: 1.7942003, MAE: 0.88122284\n",
      "Epoch 470: MSE: 1.8609302, MAE: 0.86278975\n",
      "Epoch 471: MSE: 1.8981656, MAE: 0.86425614\n",
      "Epoch 472: MSE: 1.8905604, MAE: 0.86272234\n",
      "Epoch 473: MSE: 1.7323079, MAE: 0.8515584\n",
      "Epoch 474: MSE: 1.8867168, MAE: 0.8788362\n",
      "Epoch 475: MSE: 1.8213959, MAE: 0.8719035\n",
      "Epoch 476: MSE: 1.9524937, MAE: 0.8762777\n",
      "Epoch 477: MSE: 1.7662532, MAE: 0.85444534\n",
      "Epoch 478: MSE: 1.9119192, MAE: 0.87425435\n",
      "Epoch 479: MSE: 1.8640995, MAE: 0.8674637\n",
      "Epoch 480: MSE: 1.8868461, MAE: 0.8870881\n",
      "Epoch 481: MSE: 1.8054981, MAE: 0.84909683\n",
      "Epoch 482: MSE: 1.7718055, MAE: 0.86493266\n",
      "Epoch 483: MSE: 1.7900236, MAE: 0.90055436\n",
      "Epoch 484: MSE: 1.8004568, MAE: 0.8443417\n",
      "Epoch 485: MSE: 1.8365661, MAE: 0.8559459\n",
      "Epoch 486: MSE: 1.771078, MAE: 0.8591639\n",
      "Epoch 487: MSE: 1.7426947, MAE: 0.8689371\n",
      "Epoch 488: MSE: 1.8941389, MAE: 0.86193126\n",
      "Epoch 489: MSE: 1.7057623, MAE: 0.83735526\n",
      "Epoch 490: MSE: 1.8190814, MAE: 0.8659192\n",
      "Epoch 491: MSE: 1.8165704, MAE: 0.85512024\n",
      "Epoch 492: MSE: 1.7611052, MAE: 0.8613742\n",
      "Epoch 493: MSE: 1.7326525, MAE: 0.8402713\n",
      "Epoch 494: MSE: 1.7363772, MAE: 0.8391044\n",
      "Epoch 495: MSE: 1.6434224, MAE: 0.8445872\n",
      "Epoch 496: MSE: 1.7465267, MAE: 0.86910504\n",
      "Epoch 497: MSE: 1.7835879, MAE: 0.84619117\n",
      "Epoch 498: MSE: 1.6565478, MAE: 0.8433218\n",
      "Epoch 499: MSE: 1.816103, MAE: 0.8475772\n",
      "Epoch 500: MSE: 1.735246, MAE: 0.8338108\n"
     ]
    }
   ],
   "source": [
    "for epoch in 1...epochCount {\n",
    "    var epochLoss: Float = 0\n",
    "    var epochMAE: Float = 0\n",
    "    var batchCount: Int = 0\n",
    "    var batchArray = Array(repeating: false, count: numberOfBatch)\n",
    "    for batch in 0..<numberOfBatch {\n",
    "        var r = batch\n",
    "        if shuffle {\n",
    "            while true {\n",
    "                r = Int.random(in: 0..<numberOfBatch)\n",
    "                if !batchArray[r] {\n",
    "                    batchArray[r] = true\n",
    "                    break\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        let batchStart = r * batchSize\n",
    "        let batchEnd = min(numTrainRecords, batchStart + batchSize)\n",
    "        let (loss, grad) = model.valueWithGradient { (model: RegressionModel) -> Tensor<Float> in\n",
    "            let multiInput = MultiInputs(numerical: [XNumericalTrain[batchStart..<batchEnd]],\n",
    "                                         categorical: [XCategoricalTrain[0][batchStart..<batchEnd],\n",
    "                                                       XCategoricalTrain[1][batchStart..<batchEnd]])\n",
    "            let logits = model(multiInput)\n",
    "            return meanSquaredError(predicted: logits, expected: YTrain[batchStart..<batchEnd])\n",
    "        }\n",
    "        optimizer.update(&model, along: grad)\n",
    "        \n",
    "        let multiInput = MultiInputs(numerical: [XNumericalTrain[batchStart..<batchEnd]],\n",
    "                                     categorical: [XCategoricalTrain[0][batchStart..<batchEnd],\n",
    "                                                   XCategoricalTrain[1][batchStart..<batchEnd]])\n",
    "        let logits = model(multiInput)\n",
    "        epochMAE += mae(predictions: logits, truths: YTrain[batchStart..<batchEnd])\n",
    "        epochLoss += loss.scalarized()\n",
    "        batchCount += 1\n",
    "    }\n",
    "    epochMAE /= Float(batchCount)\n",
    "    epochLoss /= Float(batchCount)\n",
    "\n",
    "    print(\"Epoch \\(epoch): MSE: \\(epochLoss), MAE: \\(epochMAE)\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.09736897, MAE: 0.02195308\r\n"
     ]
    }
   ],
   "source": [
    "Context.local.learningPhase = .inference\n",
    "\n",
    "let multiInputTest = MultiInputs(numerical: [XNumericalTest],\n",
    "                                 categorical: [XCategoricalTest[0],\n",
    "                                               XCategoricalTest[1]])\n",
    "\n",
    "let prediction = model(multiInputTest)\n",
    "\n",
    "let predictionMse = meanSquaredError(predicted: prediction, expected: YTest).scalarized()/Float(numTestRecords)\n",
    "let predictionMae = mae(predictions: prediction, truths: YTest)/Float(numTestRecords)\n",
    "\n",
    "print(\"MSE: \\(predictionMse), MAE: \\(predictionMae)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fkf29HLlohIP"
   },
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2] [9, 5]\r\n"
     ]
    }
   ],
   "source": [
    "print(model.embedding1.embeddings.shape, model.embedding2.embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "let coremlModel = Model(version: 4,\n",
    "                        shortDescription: \"Regression\",\n",
    "                        author: \"Jacopo Mangiavacchi\",\n",
    "                        license: \"MIT\",\n",
    "                        userDefined: [\"SwiftCoremltoolsVersion\" : \"0.0.6\"]) {\n",
    "    Input(name: \"numericalInput\", shape: [11])\n",
    "    Input(name: \"categoricalInput1\", shape: [1])\n",
    "    Input(name: \"categoricalInput2\", shape: [1])\n",
    "    Output(name: \"output\", shape: [1])\n",
    "    NeuralNetwork {\n",
    "        Embedding(name: \"embedding1\",\n",
    "                     input: [\"categoricalInput1\"],\n",
    "                     output: [\"outEmbedding1\"],\n",
    "                     weight: model.embedding1.embeddings.transposed().flattened().scalars,\n",
    "                     inputDim: 2,\n",
    "                     outputChannels: 2)\n",
    "        Permute(name: \"permute1\",\n",
    "                     input: [\"outEmbedding1\"],\n",
    "                     output: [\"outPermute1\"],\n",
    "                     axis: [2, 1, 0, 3])\n",
    "        Flatten(name: \"flatten1\",\n",
    "                     input: [\"outPermute1\"],\n",
    "                     output: [\"outFlatten1\"],\n",
    "                     mode: .last)\n",
    "        Embedding(name: \"embedding2\",\n",
    "                     input: [\"categoricalInput2\"],\n",
    "                     output: [\"outEmbedding2\"],\n",
    "                     weight: model.embedding2.embeddings.transposed().flattened().scalars,\n",
    "                     inputDim: 9,\n",
    "                     outputChannels: 5)\n",
    "        Permute(name: \"permute2\",\n",
    "                     input: [\"outEmbedding2\"],\n",
    "                     output: [\"outPermute2\"],\n",
    "                     axis: [2, 1, 0, 3])\n",
    "        Flatten(name: \"flatten2\",\n",
    "                     input: [\"outPermute2\"],\n",
    "                     output: [\"outFlatten2\"],\n",
    "                     mode: .last)\n",
    "        Concat(name: \"concat\",\n",
    "                     input: [\"numericalInput\", \"outFlatten1\", \"outFlatten2\"],\n",
    "                     output: [\"outConcat\"])\n",
    "        InnerProduct(name: \"dense1\",\n",
    "                     input: [\"outConcat\"],\n",
    "                     output: [\"outDense1\"],\n",
    "                     weight: model.allInputConcatLayer.weight.transposed().flattened().scalars,\n",
    "                     bias: model.allInputConcatLayer.bias.flattened().scalars,\n",
    "                     inputChannels: 11 + 2 + 5,\n",
    "                     outputChannels: 64)\n",
    "        ReLu(name: \"Relu1\",\n",
    "             input: [\"outDense1\"],\n",
    "             output: [\"outRelu1\"])\n",
    "        InnerProduct(name: \"dense2\",\n",
    "                     input: [\"outRelu1\"],\n",
    "                     output: [\"outDense2\"],\n",
    "                     weight: model.hiddenLayer.weight.transposed().flattened().scalars,\n",
    "                     bias: model.hiddenLayer.bias.flattened().scalars,\n",
    "                     inputChannels: 64,\n",
    "                     outputChannels: 32)\n",
    "        ReLu(name: \"Relu2\",\n",
    "             input: [\"outDense2\"],\n",
    "             output: [\"outRelu2\"])\n",
    "        InnerProduct(name: \"dense3\",\n",
    "                     input: [\"outRelu2\"],\n",
    "                     output: [\"output\"],\n",
    "                     weight: model.outputLayer.weight.transposed().flattened().scalars,\n",
    "                     bias: model.outputLayer.bias.flattened().scalars,\n",
    "                     inputChannels: 32,\n",
    "                     outputChannels: 1)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "let coreMLData = coremlModel.coreMLData\n",
    "try! coreMLData!.write(to: URL(fileURLWithPath: \"./s4tf_house_simplified_trained_model.mlmodel\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.56604546, -0.49313328,   1.0275197,   1.2293758,  0.27231246,   1.0809927, -0.99258536,\r\n",
      "   1.5367292,   0.8051156,  0.39535978,   0.6034551] [0] [8] => [13.1]\r\n",
      "[-0.39648032, -0.49313328, -0.53633916, -0.51630783,   -0.278529, -0.85177904,   0.7395483,\r\n",
      "  -0.7151967,  0.52791685,  0.44683868, 0.006757561] [0] [4] => [22.2]\r\n",
      "[-0.35303918, -0.49313328, -0.70848036, -0.39410976,   0.8515478,  0.34085673,  -0.2667246,\r\n",
      " -0.59636164, -0.48847827,   0.3787233, -0.38736588] [1] [7] => [27.5]\r\n"
     ]
    }
   ],
   "source": [
    "print(XNumericalTest[0], XCategoricalTest[0][0], XCategoricalTest[1][0], \"=>\", YTest[0])\n",
    "print(XNumericalTest[17], XCategoricalTest[0][17], XCategoricalTest[1][17], \"=>\", YTest[17])\n",
    "print(XNumericalTest[87], XCategoricalTest[0][87], XCategoricalTest[1][87], \"=>\", YTest[87])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "S4TF_House.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
