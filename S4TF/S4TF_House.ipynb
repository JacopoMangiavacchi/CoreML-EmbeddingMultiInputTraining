{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "// %install-swiftpm-flags -c release\n",
    "// %install '.package(url: \"https://github.com/JacopoMangiavacchi/SwiftCoreMLTools.git\", from: \"0.0.5\")' SwiftCoreMLTools\n",
    "// %install '.package(url: \"https://github.com/dduan/Just.git\", from: \"0.8.0\")' Just"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TO4FT9bUohHx"
   },
   "outputs": [],
   "source": [
    "import Foundation\n",
    "import TensorFlow\n",
    "// import SwiftCoreMLTools\n",
    "// import Just"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Download\n",
    "\n",
    "Boston house prices dataset\n",
    "---------------------------\n",
    "\n",
    "**Data Set Characteristics:**  \n",
    "\n",
    "    :Number of Instances: 506 \n",
    "\n",
    "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
    "\n",
    "    :Attribute Information (in order):\n",
    "        - CRIM     per capita crime rate by town\n",
    "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "        - INDUS    proportion of non-retail business acres per town\n",
    "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "        - NOX      nitric oxides concentration (parts per 10 million)\n",
    "        - RM       average number of rooms per dwelling\n",
    "        - AGE      proportion of owner-occupied units built prior to 1940\n",
    "        - DIS      weighted distances to five Boston employment centres\n",
    "        - RAD      index of accessibility to radial highways\n",
    "        - TAX      full-value property-tax rate per ten thousand dollars\n",
    "        - PTRATIO  pupil-teacher ratio by town\n",
    "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "        - LSTAT    % lower status of the population\n",
    "        - MEDV     Median value of owner-occupied homes in a thousand dollar\n",
    "\n",
    "    :Missing Attribute Values: None\n",
    "\n",
    "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
    "\n",
    "This is a copy of UCI ML housing dataset.\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// if let cts = Just.get(URL(string: \"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\")!).content {\n",
    "//     try! cts.write(to: URL(fileURLWithPath:\"../data/housing.csv\"))\n",
    "// }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "let data = try String(contentsOfFile:\"../data/housing.csv\", encoding: String.Encoding.utf8)\n",
    "let dataRecords: [[Float]] = data.split(separator: \"\\n\").map{ String($0).split(separator: \" \").compactMap{ Float(String($0)) } }\n",
    "\n",
    "let numRecords = dataRecords.count\n",
    "let numColumns = dataRecords[0].count\n",
    "\n",
    "let dataFeatures = dataRecords.map{ Array($0[0..<numColumns-1]) }\n",
    "let dataLabels = dataRecords.map{ Array($0[(numColumns-1)...]) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Numerical Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "let categoricalColumns = [3, 8]\n",
    "let numericalColumns = [0, 1, 2, 4, 5, 6, 7, 9, 10, 11, 12]\n",
    "let numCategoricalFeatures = categoricalColumns.count\n",
    "let numNumericalFeatures = numericalColumns.count\n",
    "let numLabels = 1\n",
    "\n",
    "assert(numColumns == numCategoricalFeatures + numNumericalFeatures + 1)\n",
    "\n",
    "// Get Categorical Features\n",
    "let allCategoriesValues = dataFeatures.map{ row in categoricalColumns.map{ Int32(row[$0]) } }\n",
    "                                .reduce(into: Array(repeating: [Int32](), count: 2)){ total, value in\n",
    "                                    total[0].append(value[0])\n",
    "                                    total[1].append(value[1]) }\n",
    "                                .map{ Set($0).sorted() }\n",
    "\n",
    "let embeddingSizes = allCategoriesValues.map{ $0.count }\n",
    "\n",
    "let categoricalFeatures = dataFeatures.map{ row in categoricalColumns.map{ Int32(row[$0]) } }\n",
    "let oneHotCategoricalFeatures:[[[Int32]]] = categoricalFeatures.map{ catArray in\n",
    "    var oneHotArray = [[Int32]]()\n",
    "    \n",
    "    for i in 0..<catArray.count {\n",
    "        var oneHot = Array(repeating: Int32(0), count: allCategoriesValues[i].count)\n",
    "        if let pos = allCategoriesValues[i].firstIndex(where: { $0 == catArray[i] }){\n",
    "            oneHot[pos] = 1\n",
    "        }\n",
    "        oneHotArray.append(oneHot)\n",
    "    }\n",
    "    \n",
    "    return oneHotArray\n",
    "}\n",
    "\n",
    "// Get Numerical Features\n",
    "let numericalFeatures = dataFeatures.map{ row in numericalColumns.map{ row[$0] } }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "let trainPercentage:Float = 0.8\n",
    "let numTrainRecords = Int(ceil(Float(numRecords) * trainPercentage))\n",
    "let numTestRecords = numRecords - numTrainRecords\n",
    "\n",
    "func matrixTranspose<T>(_ matrix: [[T]]) -> [[T]] {\n",
    "    if matrix.isEmpty {return matrix}\n",
    "    var result = [[T]]()\n",
    "    for index in 0..<matrix.first!.count {\n",
    "        result.append(matrix.map{$0[index]})\n",
    "    }\n",
    "    return result\n",
    "}\n",
    "\n",
    "let xCategoricalAllTrain = matrixTranspose(Array(oneHotCategoricalFeatures[0..<numTrainRecords])).map{ Array($0.joined()) }\n",
    "let xCategoricalAllTest = matrixTranspose(Array(oneHotCategoricalFeatures[numTrainRecords...])).map{ Array($0.joined()) }\n",
    "let xNumericalAllTrain = Array(Array(numericalFeatures[0..<numTrainRecords]).joined())\n",
    "let xNumericalAllTest = Array(Array(numericalFeatures[numTrainRecords...]).joined())\n",
    "let yAllTrain = Array(Array(dataLabels[0..<numTrainRecords]).joined())\n",
    "let yAllTest = Array(Array(dataLabels[numTrainRecords...]).joined())\n",
    "\n",
    "let XCategoricalTrain = xCategoricalAllTrain.enumerated().map{ (offset, element) in \n",
    "    Tensor<Int32>(element).reshaped(to: TensorShape([numTrainRecords, embeddingSizes[offset]]))\n",
    "}\n",
    "let XCategoricalTest = xCategoricalAllTest.enumerated().map{ (offset, element) in \n",
    "    Tensor<Int32>(element).reshaped(to: TensorShape([numTestRecords, embeddingSizes[offset]]))\n",
    "}\n",
    "let XNumericalTrainDeNorm = Tensor<Float>(xNumericalAllTrain).reshaped(to: TensorShape([numTrainRecords, numNumericalFeatures]))\n",
    "let XNumericalTestDeNorm = Tensor<Float>(xNumericalAllTest).reshaped(to: TensorShape([numTestRecords, numNumericalFeatures]))\n",
    "let YTrain = Tensor<Float>(yAllTrain).reshaped(to: TensorShape([numTrainRecords, numLabels]))\n",
    "let YTest = Tensor<Float>(yAllTest).reshaped(to: TensorShape([numTestRecords, numLabels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.0137098,  14.197531,   9.523555, 0.53213036,  6.3311296,   64.47929,  4.1678762,  353.68396,\r\n",
      "    18.03163,  379.84735,  11.394517]] [[ 6.5076075,  25.258776,   6.534038, 0.11449408,  0.7311985,  29.000755,  2.1797554,  132.14561,\r\n",
      "    2.217345,  40.494495,   6.852825]]\r\n"
     ]
    }
   ],
   "source": [
    "let mean = XNumericalTrainDeNorm.mean(alongAxes: 0)\n",
    "let std = XNumericalTrainDeNorm.standardDeviation(alongAxes: 0)\n",
    "\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "let XNumericalTrain = (XNumericalTrainDeNorm - mean)/std\n",
    "let XNumericalTest = (XNumericalTestDeNorm - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shapes [405, 11] [405, 2] [405, 9] [405, 1]\r\n",
      "Testing shapes  [101, 11] [101, 2] [101, 9] [101, 1]\r\n"
     ]
    }
   ],
   "source": [
    "print(\"Training shapes \\(XNumericalTrain.shape) \\(XCategoricalTrain[0].shape) \\(XCategoricalTrain[1].shape) \\(YTrain.shape)\")\n",
    "print(\"Testing shapes  \\(XNumericalTest.shape) \\(XCategoricalTest[0].shape) \\(XCategoricalTest[1].shape) \\(YTest.shape)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HDJZCCgqohIC"
   },
   "outputs": [],
   "source": [
    "struct MultiInputs<N: Differentiable, C>: Differentiable {\n",
    "  var numerical: N\n",
    "  \n",
    "  @noDerivative\n",
    "  var categorical: C\n",
    "\n",
    "  @differentiable\n",
    "  init(numerical: N, categorical: C) {\n",
    "    self.numerical = numerical\n",
    "    self.categorical = categorical\n",
    "  }\n",
    "}\n",
    "\n",
    "struct RegressionModel: Module {\n",
    "    var numericalLayer = Dense<Float>(inputSize: 11, outputSize: 32, activation: relu)\n",
    "    var embedding1 = Embedding<Float>(vocabularySize: 2, embeddingSize: 2)\n",
    "    var embedding2 = Embedding<Float>(vocabularySize: 9, embeddingSize: 5)\n",
    "    var embeddingLayer = Dense<Float>(inputSize: (4 + 45), outputSize: 64, activation: relu)\n",
    "    var allInputConcatLayer = Dense<Float>(inputSize: (32 + 64), outputSize: 128, activation: relu)\n",
    "    var hiddenLayer = Dense<Float>(inputSize: 128, outputSize: 32, activation: relu)\n",
    "    var outputLayer = Dense<Float>(inputSize: 32, outputSize: 1)\n",
    "    \n",
    "    @differentiable\n",
    "    func callAsFunction(_ input: MultiInputs<[Tensor<Float>], [Tensor<Int32>]>) -> Tensor<Float> {\n",
    "        let numericalInput = numericalLayer(input.numerical[0])\n",
    "        let embeddingOutput1 = embedding1(input.categorical[0])\n",
    "        let embeddingOutput1Reshaped = embeddingOutput1.reshaped(to: \n",
    "            TensorShape([embeddingOutput1.shape[0], embeddingOutput1.shape[1] * embeddingOutput1.shape[2]]))\n",
    "        let embeddingOutput2 = embedding2(input.categorical[1])\n",
    "        let embeddingOutput2Reshaped = embeddingOutput2.reshaped(to: \n",
    "            TensorShape([embeddingOutput2.shape[0], embeddingOutput2.shape[1] * embeddingOutput2.shape[2]]))\n",
    "        let embeddingConcat = Tensor<Float>(concatenating: [embeddingOutput1Reshaped, embeddingOutput2Reshaped], alongAxis: 1)\n",
    "        let embeddingInput = embeddingLayer(embeddingConcat)\n",
    "        let allConcat = Tensor<Float>(concatenating: [numericalInput, embeddingInput], alongAxis: 1)\n",
    "        return allConcat.sequenced(through: allInputConcatLayer, hiddenLayer, outputLayer)\n",
    "    }\n",
    "}\n",
    "\n",
    "var model = RegressionModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JK0Vj7bSohIF"
   },
   "outputs": [],
   "source": [
    "let optimizer = RMSProp(for: model, learningRate: 0.001)\n",
    "Context.local.learningPhase = .training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gY8C7yHJohIH"
   },
   "outputs": [],
   "source": [
    "let epochCount = 500\n",
    "let batchSize = 32\n",
    "let numberOfBatch = Int(ceil(Double(numTrainRecords) / Double(batchSize)))\n",
    "let shuffle = true\n",
    "\n",
    "func mae(predictions: Tensor<Float>, truths: Tensor<Float>) -> Float {\n",
    "    return abs(Tensor<Float>(predictions - truths)).mean().scalarized()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "L9bU9HsdohIK",
    "outputId": "692b81c5-3286-4e7c-9246-eb56e6a3eaee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: MSE: 538.7566, MAE: 20.769682\n",
      "Epoch 2: MSE: 326.60144, MAE: 14.487716\n",
      "Epoch 3: MSE: 174.02321, MAE: 9.4455385\n",
      "Epoch 4: MSE: 113.11984, MAE: 7.26101\n",
      "Epoch 5: MSE: 97.41988, MAE: 6.8143406\n",
      "Epoch 6: MSE: 85.93188, MAE: 6.2576895\n",
      "Epoch 7: MSE: 75.830185, MAE: 5.9648123\n",
      "Epoch 8: MSE: 69.65385, MAE: 5.545283\n",
      "Epoch 9: MSE: 60.797016, MAE: 5.333738\n",
      "Epoch 10: MSE: 56.28574, MAE: 4.696577\n",
      "Epoch 11: MSE: 51.698994, MAE: 4.681757\n",
      "Epoch 12: MSE: 48.156887, MAE: 4.2115726\n",
      "Epoch 13: MSE: 41.7582, MAE: 4.0844197\n",
      "Epoch 14: MSE: 41.818134, MAE: 3.9483097\n",
      "Epoch 15: MSE: 34.868656, MAE: 3.7961638\n",
      "Epoch 16: MSE: 38.032547, MAE: 3.6246533\n",
      "Epoch 17: MSE: 33.77977, MAE: 3.5818517\n",
      "Epoch 18: MSE: 33.526505, MAE: 3.4123552\n",
      "Epoch 19: MSE: 32.165363, MAE: 3.4164927\n",
      "Epoch 20: MSE: 30.53839, MAE: 3.3854785\n",
      "Epoch 21: MSE: 29.151405, MAE: 3.3084984\n",
      "Epoch 22: MSE: 28.262146, MAE: 3.274151\n",
      "Epoch 23: MSE: 28.250547, MAE: 3.2525787\n",
      "Epoch 24: MSE: 28.393291, MAE: 3.1756794\n",
      "Epoch 25: MSE: 25.969215, MAE: 3.1627727\n",
      "Epoch 26: MSE: 26.09594, MAE: 3.074617\n",
      "Epoch 27: MSE: 25.069754, MAE: 3.0481443\n",
      "Epoch 28: MSE: 25.44034, MAE: 2.992625\n",
      "Epoch 29: MSE: 24.199108, MAE: 2.977876\n",
      "Epoch 30: MSE: 23.925371, MAE: 2.942467\n",
      "Epoch 31: MSE: 23.336754, MAE: 2.930997\n",
      "Epoch 32: MSE: 22.30515, MAE: 2.8774836\n",
      "Epoch 33: MSE: 21.645847, MAE: 2.8594213\n",
      "Epoch 34: MSE: 22.466248, MAE: 2.8502355\n",
      "Epoch 35: MSE: 21.455631, MAE: 2.809802\n",
      "Epoch 36: MSE: 21.064604, MAE: 2.7767777\n",
      "Epoch 37: MSE: 20.089415, MAE: 2.7549803\n",
      "Epoch 38: MSE: 19.84013, MAE: 2.7048798\n",
      "Epoch 39: MSE: 19.320189, MAE: 2.7047217\n",
      "Epoch 40: MSE: 20.446228, MAE: 2.7154427\n",
      "Epoch 41: MSE: 19.468395, MAE: 2.6632054\n",
      "Epoch 42: MSE: 18.808197, MAE: 2.6354356\n",
      "Epoch 43: MSE: 18.460117, MAE: 2.5829\n",
      "Epoch 44: MSE: 18.060534, MAE: 2.595992\n",
      "Epoch 45: MSE: 17.95243, MAE: 2.5873973\n",
      "Epoch 46: MSE: 17.259514, MAE: 2.5299745\n",
      "Epoch 47: MSE: 17.326277, MAE: 2.499554\n",
      "Epoch 48: MSE: 16.735466, MAE: 2.5026803\n",
      "Epoch 49: MSE: 16.433037, MAE: 2.4619396\n",
      "Epoch 50: MSE: 17.146254, MAE: 2.4875245\n",
      "Epoch 51: MSE: 15.275005, MAE: 2.4664857\n",
      "Epoch 52: MSE: 16.272303, MAE: 2.4125478\n",
      "Epoch 53: MSE: 15.177828, MAE: 2.4125803\n",
      "Epoch 54: MSE: 16.29115, MAE: 2.396292\n",
      "Epoch 55: MSE: 15.036739, MAE: 2.392095\n",
      "Epoch 56: MSE: 14.9059105, MAE: 2.3815484\n",
      "Epoch 57: MSE: 15.162154, MAE: 2.3383255\n",
      "Epoch 58: MSE: 14.292756, MAE: 2.2964334\n",
      "Epoch 59: MSE: 14.9935875, MAE: 2.3141775\n",
      "Epoch 60: MSE: 13.811569, MAE: 2.2761197\n",
      "Epoch 61: MSE: 13.342767, MAE: 2.263696\n",
      "Epoch 62: MSE: 13.753306, MAE: 2.225714\n",
      "Epoch 63: MSE: 13.926255, MAE: 2.2129653\n",
      "Epoch 64: MSE: 12.796091, MAE: 2.2287352\n",
      "Epoch 65: MSE: 13.296824, MAE: 2.22924\n",
      "Epoch 66: MSE: 12.764781, MAE: 2.231663\n",
      "Epoch 67: MSE: 13.017424, MAE: 2.192233\n",
      "Epoch 68: MSE: 12.268152, MAE: 2.172278\n",
      "Epoch 69: MSE: 11.934272, MAE: 2.1720438\n",
      "Epoch 70: MSE: 12.271413, MAE: 2.1657376\n",
      "Epoch 71: MSE: 11.70517, MAE: 2.146765\n",
      "Epoch 72: MSE: 12.131205, MAE: 2.1559892\n",
      "Epoch 73: MSE: 11.776513, MAE: 2.1265702\n",
      "Epoch 74: MSE: 12.489756, MAE: 2.1164336\n",
      "Epoch 75: MSE: 11.277969, MAE: 2.0935879\n",
      "Epoch 76: MSE: 11.526997, MAE: 2.0988855\n",
      "Epoch 77: MSE: 11.744348, MAE: 2.1072695\n",
      "Epoch 78: MSE: 10.876719, MAE: 2.0618205\n",
      "Epoch 79: MSE: 10.79361, MAE: 2.0666525\n",
      "Epoch 80: MSE: 10.28865, MAE: 2.026344\n",
      "Epoch 81: MSE: 11.6125965, MAE: 2.0506978\n",
      "Epoch 82: MSE: 10.495143, MAE: 2.0183816\n",
      "Epoch 83: MSE: 10.277904, MAE: 2.0195422\n",
      "Epoch 84: MSE: 11.270874, MAE: 2.0001929\n",
      "Epoch 85: MSE: 9.667031, MAE: 1.9906691\n",
      "Epoch 86: MSE: 9.782771, MAE: 1.9944772\n",
      "Epoch 87: MSE: 11.117102, MAE: 1.9752669\n",
      "Epoch 88: MSE: 10.249479, MAE: 1.9840453\n",
      "Epoch 89: MSE: 9.148469, MAE: 1.98796\n",
      "Epoch 90: MSE: 9.649858, MAE: 2.0095608\n",
      "Epoch 91: MSE: 9.836597, MAE: 1.9432635\n",
      "Epoch 92: MSE: 8.79476, MAE: 1.9275246\n",
      "Epoch 93: MSE: 9.791759, MAE: 1.9868877\n",
      "Epoch 94: MSE: 9.864187, MAE: 1.9287059\n",
      "Epoch 95: MSE: 8.915299, MAE: 1.9306966\n",
      "Epoch 96: MSE: 9.3018, MAE: 1.9580424\n",
      "Epoch 97: MSE: 9.419364, MAE: 1.9133378\n",
      "Epoch 98: MSE: 9.517579, MAE: 1.9167404\n",
      "Epoch 99: MSE: 8.034102, MAE: 1.8822967\n",
      "Epoch 100: MSE: 9.374749, MAE: 1.9313102\n",
      "Epoch 101: MSE: 8.6529455, MAE: 1.9035351\n",
      "Epoch 102: MSE: 9.342947, MAE: 1.9158158\n",
      "Epoch 103: MSE: 8.627085, MAE: 1.9054053\n",
      "Epoch 104: MSE: 8.506879, MAE: 1.8797408\n",
      "Epoch 105: MSE: 8.957329, MAE: 1.8888105\n",
      "Epoch 106: MSE: 8.883938, MAE: 1.873686\n",
      "Epoch 107: MSE: 8.714622, MAE: 1.8463458\n",
      "Epoch 108: MSE: 8.767485, MAE: 1.8643754\n",
      "Epoch 109: MSE: 8.326049, MAE: 1.8649006\n",
      "Epoch 110: MSE: 7.7376757, MAE: 1.8968787\n",
      "Epoch 111: MSE: 9.345905, MAE: 1.8642578\n",
      "Epoch 112: MSE: 8.80279, MAE: 1.884774\n",
      "Epoch 113: MSE: 8.378462, MAE: 1.824084\n",
      "Epoch 114: MSE: 7.9298277, MAE: 1.893533\n",
      "Epoch 115: MSE: 8.824669, MAE: 1.8492204\n",
      "Epoch 116: MSE: 6.840442, MAE: 1.8295969\n",
      "Epoch 117: MSE: 8.8611, MAE: 1.8570788\n",
      "Epoch 118: MSE: 8.366329, MAE: 1.8758067\n",
      "Epoch 119: MSE: 7.840521, MAE: 1.8072034\n",
      "Epoch 120: MSE: 7.617752, MAE: 1.8114015\n",
      "Epoch 121: MSE: 7.5568314, MAE: 1.8330786\n",
      "Epoch 122: MSE: 8.215224, MAE: 1.8137912\n",
      "Epoch 123: MSE: 7.068445, MAE: 1.7549402\n",
      "Epoch 124: MSE: 7.281884, MAE: 1.8294162\n",
      "Epoch 125: MSE: 7.8715916, MAE: 1.8194885\n",
      "Epoch 126: MSE: 7.4450226, MAE: 1.8007696\n",
      "Epoch 127: MSE: 7.0397406, MAE: 1.7739036\n",
      "Epoch 128: MSE: 7.9281416, MAE: 1.7984138\n",
      "Epoch 129: MSE: 6.926663, MAE: 1.7551143\n",
      "Epoch 130: MSE: 6.0824594, MAE: 1.7505379\n",
      "Epoch 131: MSE: 8.578741, MAE: 1.7671239\n",
      "Epoch 132: MSE: 7.6712923, MAE: 1.7478586\n",
      "Epoch 133: MSE: 6.5639377, MAE: 1.7405783\n",
      "Epoch 134: MSE: 7.483179, MAE: 1.7816257\n",
      "Epoch 135: MSE: 7.293261, MAE: 1.7485032\n",
      "Epoch 136: MSE: 7.840154, MAE: 1.7672766\n",
      "Epoch 137: MSE: 6.658551, MAE: 1.6921022\n",
      "Epoch 138: MSE: 6.6341653, MAE: 1.7108718\n",
      "Epoch 139: MSE: 6.510439, MAE: 1.7837168\n",
      "Epoch 140: MSE: 7.3183646, MAE: 1.6939329\n",
      "Epoch 141: MSE: 6.8437476, MAE: 1.7920743\n",
      "Epoch 142: MSE: 6.127177, MAE: 1.7299851\n",
      "Epoch 143: MSE: 6.951546, MAE: 1.7624682\n",
      "Epoch 144: MSE: 6.9596353, MAE: 1.7056364\n",
      "Epoch 145: MSE: 6.972974, MAE: 1.6778806\n",
      "Epoch 146: MSE: 6.3298693, MAE: 1.7261949\n",
      "Epoch 147: MSE: 6.753394, MAE: 1.7290432\n",
      "Epoch 148: MSE: 6.6982145, MAE: 1.6475106\n",
      "Epoch 149: MSE: 6.2019095, MAE: 1.6643007\n",
      "Epoch 150: MSE: 6.063805, MAE: 1.7316566\n",
      "Epoch 151: MSE: 5.8723483, MAE: 1.7184201\n",
      "Epoch 152: MSE: 6.529775, MAE: 1.7936617\n",
      "Epoch 153: MSE: 6.2762322, MAE: 1.6479691\n",
      "Epoch 154: MSE: 6.048729, MAE: 1.6905217\n",
      "Epoch 155: MSE: 7.1757736, MAE: 1.7080929\n",
      "Epoch 156: MSE: 6.2799673, MAE: 1.6309173\n",
      "Epoch 157: MSE: 6.969671, MAE: 1.6705977\n",
      "Epoch 158: MSE: 5.5358357, MAE: 1.6282105\n",
      "Epoch 159: MSE: 5.961027, MAE: 1.6539435\n",
      "Epoch 160: MSE: 6.384141, MAE: 1.6585176\n",
      "Epoch 161: MSE: 6.4146724, MAE: 1.6168243\n",
      "Epoch 162: MSE: 6.0690784, MAE: 1.6101004\n",
      "Epoch 163: MSE: 6.14964, MAE: 1.6030837\n",
      "Epoch 164: MSE: 6.2824144, MAE: 1.6661758\n",
      "Epoch 165: MSE: 5.4275136, MAE: 1.6274472\n",
      "Epoch 166: MSE: 5.854917, MAE: 1.677901\n",
      "Epoch 167: MSE: 5.9220166, MAE: 1.5977317\n",
      "Epoch 168: MSE: 6.260483, MAE: 1.6253402\n",
      "Epoch 169: MSE: 5.448295, MAE: 1.5876203\n",
      "Epoch 170: MSE: 6.164477, MAE: 1.6265837\n",
      "Epoch 171: MSE: 5.8493176, MAE: 1.5918943\n",
      "Epoch 172: MSE: 7.030685, MAE: 1.5799135\n",
      "Epoch 173: MSE: 5.14013, MAE: 1.5448029\n",
      "Epoch 174: MSE: 5.3684945, MAE: 1.6218545\n",
      "Epoch 175: MSE: 6.058048, MAE: 1.6158\n",
      "Epoch 176: MSE: 5.00749, MAE: 1.6141404\n",
      "Epoch 177: MSE: 5.899426, MAE: 1.6470212\n",
      "Epoch 178: MSE: 5.2230506, MAE: 1.5833598\n",
      "Epoch 179: MSE: 6.069419, MAE: 1.6169899\n",
      "Epoch 180: MSE: 6.1255984, MAE: 1.4998513\n",
      "Epoch 181: MSE: 5.53557, MAE: 1.633789\n",
      "Epoch 182: MSE: 4.8691397, MAE: 1.5360653\n",
      "Epoch 183: MSE: 4.5290937, MAE: 1.5725402\n",
      "Epoch 184: MSE: 6.0245037, MAE: 1.591884\n",
      "Epoch 185: MSE: 5.812952, MAE: 1.54453\n",
      "Epoch 186: MSE: 4.9469266, MAE: 1.5540516\n",
      "Epoch 187: MSE: 5.277366, MAE: 1.6563888\n",
      "Epoch 188: MSE: 5.4595413, MAE: 1.5461113\n",
      "Epoch 189: MSE: 5.4635625, MAE: 1.5416762\n",
      "Epoch 190: MSE: 5.753668, MAE: 1.5384768\n",
      "Epoch 191: MSE: 5.9787464, MAE: 1.5430776\n",
      "Epoch 192: MSE: 4.4379487, MAE: 1.5385276\n",
      "Epoch 193: MSE: 5.4893103, MAE: 1.5895678\n",
      "Epoch 194: MSE: 5.418734, MAE: 1.5902691\n",
      "Epoch 195: MSE: 5.769163, MAE: 1.534656\n",
      "Epoch 196: MSE: 5.089992, MAE: 1.5133336\n",
      "Epoch 197: MSE: 5.4804263, MAE: 1.5846244\n",
      "Epoch 198: MSE: 5.5065722, MAE: 1.4939749\n",
      "Epoch 199: MSE: 4.6320844, MAE: 1.4801091\n",
      "Epoch 200: MSE: 4.876631, MAE: 1.4978745\n",
      "Epoch 201: MSE: 5.7732806, MAE: 1.546663\n",
      "Epoch 202: MSE: 5.230092, MAE: 1.538287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203: MSE: 4.4322333, MAE: 1.4914743\n",
      "Epoch 204: MSE: 5.102226, MAE: 1.5785955\n",
      "Epoch 205: MSE: 5.5810084, MAE: 1.5072623\n",
      "Epoch 206: MSE: 5.068066, MAE: 1.5116196\n",
      "Epoch 207: MSE: 5.0839076, MAE: 1.4730271\n",
      "Epoch 208: MSE: 4.3763723, MAE: 1.4861579\n",
      "Epoch 209: MSE: 4.1561007, MAE: 1.5250263\n",
      "Epoch 210: MSE: 5.526101, MAE: 1.4812418\n",
      "Epoch 211: MSE: 5.6118298, MAE: 1.5378153\n",
      "Epoch 212: MSE: 4.247137, MAE: 1.4208779\n",
      "Epoch 213: MSE: 5.320103, MAE: 1.5671417\n",
      "Epoch 214: MSE: 4.6118116, MAE: 1.4267943\n",
      "Epoch 215: MSE: 4.415875, MAE: 1.5514921\n",
      "Epoch 216: MSE: 5.1508203, MAE: 1.4871061\n",
      "Epoch 217: MSE: 4.1682534, MAE: 1.4328892\n",
      "Epoch 218: MSE: 5.8022723, MAE: 1.4710673\n",
      "Epoch 219: MSE: 4.4486613, MAE: 1.4278293\n",
      "Epoch 220: MSE: 4.655449, MAE: 1.4594887\n",
      "Epoch 221: MSE: 5.071363, MAE: 1.4734396\n",
      "Epoch 222: MSE: 4.117449, MAE: 1.4228147\n",
      "Epoch 223: MSE: 4.5146456, MAE: 1.569769\n",
      "Epoch 224: MSE: 4.3864665, MAE: 1.4835556\n",
      "Epoch 225: MSE: 4.025299, MAE: 1.4285905\n",
      "Epoch 226: MSE: 4.1691127, MAE: 1.5233592\n",
      "Epoch 227: MSE: 4.874047, MAE: 1.5232394\n",
      "Epoch 228: MSE: 4.1677127, MAE: 1.4128082\n",
      "Epoch 229: MSE: 3.5204089, MAE: 1.4215382\n",
      "Epoch 230: MSE: 4.8742385, MAE: 1.5088888\n",
      "Epoch 231: MSE: 4.547977, MAE: 1.4088287\n",
      "Epoch 232: MSE: 4.6879373, MAE: 1.388989\n",
      "Epoch 233: MSE: 4.2270617, MAE: 1.3744272\n",
      "Epoch 234: MSE: 4.502773, MAE: 1.414295\n",
      "Epoch 235: MSE: 4.44647, MAE: 1.3632995\n",
      "Epoch 236: MSE: 4.006386, MAE: 1.4133778\n",
      "Epoch 237: MSE: 3.8742266, MAE: 1.4312685\n",
      "Epoch 238: MSE: 4.8650556, MAE: 1.4826791\n",
      "Epoch 239: MSE: 4.501023, MAE: 1.420369\n",
      "Epoch 240: MSE: 4.436549, MAE: 1.4619598\n",
      "Epoch 241: MSE: 4.40086, MAE: 1.3569524\n",
      "Epoch 242: MSE: 3.9539554, MAE: 1.4511158\n",
      "Epoch 243: MSE: 4.6792483, MAE: 1.3910147\n",
      "Epoch 244: MSE: 3.6200006, MAE: 1.3755869\n",
      "Epoch 245: MSE: 4.669799, MAE: 1.3806152\n",
      "Epoch 246: MSE: 3.5090027, MAE: 1.3692888\n",
      "Epoch 247: MSE: 4.7243586, MAE: 1.4750311\n",
      "Epoch 248: MSE: 4.04465, MAE: 1.2804686\n",
      "Epoch 249: MSE: 3.697839, MAE: 1.3411216\n",
      "Epoch 250: MSE: 3.8263965, MAE: 1.4237225\n",
      "Epoch 251: MSE: 3.8221278, MAE: 1.4338464\n",
      "Epoch 252: MSE: 3.8750353, MAE: 1.4820633\n",
      "Epoch 253: MSE: 3.7208128, MAE: 1.3555232\n",
      "Epoch 254: MSE: 4.6052833, MAE: 1.4048283\n",
      "Epoch 255: MSE: 4.085868, MAE: 1.3091178\n",
      "Epoch 256: MSE: 3.9165416, MAE: 1.2874573\n",
      "Epoch 257: MSE: 3.9566317, MAE: 1.3771437\n",
      "Epoch 258: MSE: 4.379785, MAE: 1.3403062\n",
      "Epoch 259: MSE: 3.6276474, MAE: 1.3506138\n",
      "Epoch 260: MSE: 3.7520788, MAE: 1.3493708\n",
      "Epoch 261: MSE: 4.2309785, MAE: 1.4285783\n",
      "Epoch 262: MSE: 3.3445382, MAE: 1.2605659\n",
      "Epoch 263: MSE: 4.471019, MAE: 1.4590466\n",
      "Epoch 264: MSE: 3.666355, MAE: 1.3283293\n",
      "Epoch 265: MSE: 4.3902407, MAE: 1.3602409\n",
      "Epoch 266: MSE: 3.4239757, MAE: 1.250758\n",
      "Epoch 267: MSE: 4.387266, MAE: 1.4106306\n",
      "Epoch 268: MSE: 3.8954341, MAE: 1.3017049\n",
      "Epoch 269: MSE: 3.8592596, MAE: 1.347978\n",
      "Epoch 270: MSE: 4.0596647, MAE: 1.3408856\n",
      "Epoch 271: MSE: 3.5322468, MAE: 1.3133115\n",
      "Epoch 272: MSE: 3.8570833, MAE: 1.418407\n",
      "Epoch 273: MSE: 4.2659755, MAE: 1.3727149\n",
      "Epoch 274: MSE: 3.3272302, MAE: 1.2484144\n",
      "Epoch 275: MSE: 3.0609953, MAE: 1.2291263\n",
      "Epoch 276: MSE: 4.2050657, MAE: 1.3474519\n",
      "Epoch 277: MSE: 3.648145, MAE: 1.3543885\n",
      "Epoch 278: MSE: 4.063881, MAE: 1.3939525\n",
      "Epoch 279: MSE: 4.0867176, MAE: 1.2892767\n",
      "Epoch 280: MSE: 3.5567942, MAE: 1.2794831\n",
      "Epoch 281: MSE: 3.49964, MAE: 1.3776553\n",
      "Epoch 282: MSE: 4.2129474, MAE: 1.3262527\n",
      "Epoch 283: MSE: 3.2001958, MAE: 1.2840629\n",
      "Epoch 284: MSE: 4.261526, MAE: 1.3303164\n",
      "Epoch 285: MSE: 3.549758, MAE: 1.2918977\n",
      "Epoch 286: MSE: 3.9794207, MAE: 1.237934\n",
      "Epoch 287: MSE: 3.2012415, MAE: 1.3228657\n",
      "Epoch 288: MSE: 3.6917102, MAE: 1.4069833\n",
      "Epoch 289: MSE: 3.4044313, MAE: 1.380886\n",
      "Epoch 290: MSE: 3.6961787, MAE: 1.326266\n",
      "Epoch 291: MSE: 4.3155084, MAE: 1.3175765\n",
      "Epoch 292: MSE: 3.1193008, MAE: 1.2310809\n",
      "Epoch 293: MSE: 3.1053007, MAE: 1.2983645\n",
      "Epoch 294: MSE: 4.0939903, MAE: 1.3707353\n",
      "Epoch 295: MSE: 3.1181962, MAE: 1.2558792\n",
      "Epoch 296: MSE: 3.3100863, MAE: 1.380598\n",
      "Epoch 297: MSE: 3.4360118, MAE: 1.2870713\n",
      "Epoch 298: MSE: 3.079677, MAE: 1.2539632\n",
      "Epoch 299: MSE: 3.676578, MAE: 1.3887509\n",
      "Epoch 300: MSE: 3.0711913, MAE: 1.3594521\n",
      "Epoch 301: MSE: 3.967767, MAE: 1.2316158\n",
      "Epoch 302: MSE: 3.7150939, MAE: 1.2850716\n",
      "Epoch 303: MSE: 3.3696115, MAE: 1.2968976\n",
      "Epoch 304: MSE: 3.334619, MAE: 1.2532507\n",
      "Epoch 305: MSE: 3.7254474, MAE: 1.3765652\n",
      "Epoch 306: MSE: 3.488752, MAE: 1.2955534\n",
      "Epoch 307: MSE: 3.2780206, MAE: 1.2592866\n",
      "Epoch 308: MSE: 3.711047, MAE: 1.2777383\n",
      "Epoch 309: MSE: 3.4516993, MAE: 1.2792083\n",
      "Epoch 310: MSE: 2.8578322, MAE: 1.2092248\n",
      "Epoch 311: MSE: 2.8677444, MAE: 1.4237984\n",
      "Epoch 312: MSE: 4.0130367, MAE: 1.4249222\n",
      "Epoch 313: MSE: 3.7320695, MAE: 1.2064148\n",
      "Epoch 314: MSE: 2.7543318, MAE: 1.2198296\n",
      "Epoch 315: MSE: 3.2117336, MAE: 1.3388788\n",
      "Epoch 316: MSE: 3.4009726, MAE: 1.3759042\n",
      "Epoch 317: MSE: 2.803087, MAE: 1.2249871\n",
      "Epoch 318: MSE: 3.0256922, MAE: 1.313022\n",
      "Epoch 319: MSE: 3.9502368, MAE: 1.3143079\n",
      "Epoch 320: MSE: 3.3172762, MAE: 1.2287407\n",
      "Epoch 321: MSE: 3.3985896, MAE: 1.2402738\n",
      "Epoch 322: MSE: 2.696576, MAE: 1.2262483\n",
      "Epoch 323: MSE: 3.5264838, MAE: 1.3382113\n",
      "Epoch 324: MSE: 3.1702383, MAE: 1.1882777\n",
      "Epoch 325: MSE: 3.442357, MAE: 1.2557429\n",
      "Epoch 326: MSE: 3.0636647, MAE: 1.2596116\n",
      "Epoch 327: MSE: 3.1654298, MAE: 1.2427427\n",
      "Epoch 328: MSE: 3.0555391, MAE: 1.3467889\n",
      "Epoch 329: MSE: 3.7637281, MAE: 1.2656543\n",
      "Epoch 330: MSE: 2.5143917, MAE: 1.137465\n",
      "Epoch 331: MSE: 2.5634248, MAE: 1.3391936\n",
      "Epoch 332: MSE: 3.487503, MAE: 1.342093\n",
      "Epoch 333: MSE: 2.7024841, MAE: 1.175307\n",
      "Epoch 334: MSE: 4.028697, MAE: 1.3313104\n",
      "Epoch 335: MSE: 2.9274945, MAE: 1.1636968\n",
      "Epoch 336: MSE: 2.8724375, MAE: 1.2590779\n",
      "Epoch 337: MSE: 2.7683637, MAE: 1.2169087\n",
      "Epoch 338: MSE: 3.4795444, MAE: 1.259824\n",
      "Epoch 339: MSE: 3.3119166, MAE: 1.2931558\n",
      "Epoch 340: MSE: 2.934264, MAE: 1.3102635\n",
      "Epoch 341: MSE: 3.1316006, MAE: 1.170467\n",
      "Epoch 342: MSE: 3.034908, MAE: 1.2790852\n",
      "Epoch 343: MSE: 3.2600403, MAE: 1.2573279\n",
      "Epoch 344: MSE: 2.6771927, MAE: 1.2276073\n",
      "Epoch 345: MSE: 3.1144457, MAE: 1.2462049\n",
      "Epoch 346: MSE: 2.8584723, MAE: 1.2698516\n",
      "Epoch 347: MSE: 3.1320062, MAE: 1.2406464\n",
      "Epoch 348: MSE: 3.0304384, MAE: 1.2207192\n",
      "Epoch 349: MSE: 3.1884725, MAE: 1.3285617\n",
      "Epoch 350: MSE: 3.2236202, MAE: 1.2627811\n",
      "Epoch 351: MSE: 2.7475855, MAE: 1.2099502\n",
      "Epoch 352: MSE: 2.8226943, MAE: 1.2443128\n",
      "Epoch 353: MSE: 2.6364424, MAE: 1.1857965\n",
      "Epoch 354: MSE: 2.8002093, MAE: 1.2876271\n",
      "Epoch 355: MSE: 3.0020983, MAE: 1.3184853\n",
      "Epoch 356: MSE: 3.0555987, MAE: 1.2384776\n",
      "Epoch 357: MSE: 2.7693303, MAE: 1.150301\n",
      "Epoch 358: MSE: 2.845629, MAE: 1.3252803\n",
      "Epoch 359: MSE: 2.9418132, MAE: 1.2885693\n",
      "Epoch 360: MSE: 2.9710195, MAE: 1.1378037\n",
      "Epoch 361: MSE: 3.171078, MAE: 1.2750767\n",
      "Epoch 362: MSE: 2.5500467, MAE: 1.2539608\n",
      "Epoch 363: MSE: 3.384431, MAE: 1.2579827\n",
      "Epoch 364: MSE: 2.4767451, MAE: 1.1496762\n",
      "Epoch 365: MSE: 3.211099, MAE: 1.3959516\n",
      "Epoch 366: MSE: 2.575198, MAE: 1.1306596\n",
      "Epoch 367: MSE: 2.9064527, MAE: 1.2836622\n",
      "Epoch 368: MSE: 2.467421, MAE: 1.2584817\n",
      "Epoch 369: MSE: 3.3108814, MAE: 1.254559\n",
      "Epoch 370: MSE: 2.5365844, MAE: 1.1679331\n",
      "Epoch 371: MSE: 2.7474875, MAE: 1.3240974\n",
      "Epoch 372: MSE: 2.458171, MAE: 1.2240065\n",
      "Epoch 373: MSE: 3.2981796, MAE: 1.3082273\n",
      "Epoch 374: MSE: 2.250795, MAE: 1.1548126\n",
      "Epoch 375: MSE: 2.6511922, MAE: 1.2555369\n",
      "Epoch 376: MSE: 3.0141966, MAE: 1.3784611\n",
      "Epoch 377: MSE: 2.8602693, MAE: 0.977397\n",
      "Epoch 378: MSE: 2.2770238, MAE: 1.1152062\n",
      "Epoch 379: MSE: 3.603898, MAE: 1.4005876\n",
      "Epoch 380: MSE: 2.332661, MAE: 1.1487563\n",
      "Epoch 381: MSE: 2.855801, MAE: 1.2459865\n",
      "Epoch 382: MSE: 2.4916315, MAE: 1.15382\n",
      "Epoch 383: MSE: 2.7710605, MAE: 1.1925988\n",
      "Epoch 384: MSE: 2.1472328, MAE: 1.2698106\n",
      "Epoch 385: MSE: 3.5630274, MAE: 1.3265983\n",
      "Epoch 386: MSE: 2.1960511, MAE: 1.0044988\n",
      "Epoch 387: MSE: 2.960217, MAE: 1.3365502\n",
      "Epoch 388: MSE: 2.5618186, MAE: 1.1258944\n",
      "Epoch 389: MSE: 2.687093, MAE: 1.1880565\n",
      "Epoch 390: MSE: 3.3109822, MAE: 1.2365988\n",
      "Epoch 391: MSE: 2.5597713, MAE: 1.1268198\n",
      "Epoch 392: MSE: 2.3547494, MAE: 1.0401047\n",
      "Epoch 393: MSE: 2.6712515, MAE: 1.4419005\n",
      "Epoch 394: MSE: 2.5211089, MAE: 1.1508648\n",
      "Epoch 395: MSE: 2.7579515, MAE: 1.1704873\n",
      "Epoch 396: MSE: 2.788779, MAE: 1.2975824\n",
      "Epoch 397: MSE: 2.4234178, MAE: 1.1880594\n",
      "Epoch 398: MSE: 2.5059924, MAE: 1.2365754\n",
      "Epoch 399: MSE: 2.225283, MAE: 1.2622502\n",
      "Epoch 400: MSE: 3.3243098, MAE: 1.390894\n",
      "Epoch 401: MSE: 2.379587, MAE: 1.1086986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 402: MSE: 2.8664737, MAE: 1.3277223\n",
      "Epoch 403: MSE: 2.6558275, MAE: 1.0866882\n",
      "Epoch 404: MSE: 2.427057, MAE: 1.2064211\n",
      "Epoch 405: MSE: 2.6761448, MAE: 1.102355\n",
      "Epoch 406: MSE: 2.384178, MAE: 1.3494707\n",
      "Epoch 407: MSE: 2.708345, MAE: 1.1365261\n",
      "Epoch 408: MSE: 2.148976, MAE: 1.1808168\n",
      "Epoch 409: MSE: 2.9321685, MAE: 1.3322326\n",
      "Epoch 410: MSE: 2.278824, MAE: 1.273396\n",
      "Epoch 411: MSE: 2.9815178, MAE: 1.224273\n",
      "Epoch 412: MSE: 2.2886305, MAE: 1.1418269\n",
      "Epoch 413: MSE: 2.818647, MAE: 1.2507799\n",
      "Epoch 414: MSE: 2.4442058, MAE: 1.1488692\n",
      "Epoch 415: MSE: 2.364881, MAE: 1.3078539\n",
      "Epoch 416: MSE: 3.3966343, MAE: 1.3626841\n",
      "Epoch 417: MSE: 1.9701341, MAE: 0.9753904\n",
      "Epoch 418: MSE: 2.2934823, MAE: 1.2918925\n",
      "Epoch 419: MSE: 2.8187218, MAE: 1.3370166\n",
      "Epoch 420: MSE: 2.7583895, MAE: 1.1940745\n",
      "Epoch 421: MSE: 2.4790819, MAE: 1.0520953\n",
      "Epoch 422: MSE: 2.2844384, MAE: 1.2423292\n",
      "Epoch 423: MSE: 2.4127598, MAE: 1.2542771\n",
      "Epoch 424: MSE: 2.373144, MAE: 1.2433466\n",
      "Epoch 425: MSE: 2.2326548, MAE: 0.94958335\n",
      "Epoch 426: MSE: 2.331006, MAE: 1.2687272\n",
      "Epoch 427: MSE: 2.8084733, MAE: 1.3239193\n",
      "Epoch 428: MSE: 2.289466, MAE: 1.0472548\n",
      "Epoch 429: MSE: 2.1562188, MAE: 1.2303861\n",
      "Epoch 430: MSE: 2.337797, MAE: 1.306382\n",
      "Epoch 431: MSE: 2.5785909, MAE: 1.2064413\n",
      "Epoch 432: MSE: 2.5574214, MAE: 1.1711646\n",
      "Epoch 433: MSE: 2.06311, MAE: 1.1155725\n",
      "Epoch 434: MSE: 3.074262, MAE: 1.3300683\n",
      "Epoch 435: MSE: 1.9607404, MAE: 0.9733331\n",
      "Epoch 436: MSE: 2.5680346, MAE: 1.3577505\n",
      "Epoch 437: MSE: 2.1510122, MAE: 1.1563509\n",
      "Epoch 438: MSE: 2.592865, MAE: 1.1445236\n",
      "Epoch 439: MSE: 2.2407453, MAE: 1.1163408\n",
      "Epoch 440: MSE: 1.8576901, MAE: 1.2458551\n",
      "Epoch 441: MSE: 3.0902967, MAE: 1.1880559\n",
      "Epoch 442: MSE: 1.9994658, MAE: 1.0818503\n",
      "Epoch 443: MSE: 2.689488, MAE: 1.3511745\n",
      "Epoch 444: MSE: 2.0291438, MAE: 1.1285976\n",
      "Epoch 445: MSE: 2.2142353, MAE: 1.2387536\n",
      "Epoch 446: MSE: 2.3277133, MAE: 1.3134834\n",
      "Epoch 447: MSE: 3.2818332, MAE: 1.2372615\n",
      "Epoch 448: MSE: 2.193546, MAE: 1.1010324\n",
      "Epoch 449: MSE: 1.6716307, MAE: 1.0829813\n",
      "Epoch 450: MSE: 3.2950227, MAE: 1.229326\n",
      "Epoch 451: MSE: 1.994833, MAE: 1.0575262\n",
      "Epoch 452: MSE: 2.3841865, MAE: 1.2125682\n",
      "Epoch 453: MSE: 2.1976206, MAE: 1.2558689\n",
      "Epoch 454: MSE: 2.9982975, MAE: 1.0931636\n",
      "Epoch 455: MSE: 2.1148202, MAE: 1.191312\n",
      "Epoch 456: MSE: 2.353065, MAE: 1.2435417\n",
      "Epoch 457: MSE: 2.5376577, MAE: 1.1289513\n",
      "Epoch 458: MSE: 2.3937418, MAE: 1.2357249\n",
      "Epoch 459: MSE: 2.4089012, MAE: 1.0792192\n",
      "Epoch 460: MSE: 2.2031639, MAE: 1.1701219\n",
      "Epoch 461: MSE: 2.5546584, MAE: 1.3413138\n",
      "Epoch 462: MSE: 2.7798972, MAE: 1.214177\n",
      "Epoch 463: MSE: 2.0125675, MAE: 1.0217268\n",
      "Epoch 464: MSE: 2.4518652, MAE: 1.2416937\n",
      "Epoch 465: MSE: 2.0673718, MAE: 1.159156\n",
      "Epoch 466: MSE: 2.0447414, MAE: 1.0642128\n",
      "Epoch 467: MSE: 2.3659914, MAE: 1.3793509\n",
      "Epoch 468: MSE: 2.4461951, MAE: 1.2487538\n",
      "Epoch 469: MSE: 2.0900786, MAE: 1.1253884\n",
      "Epoch 470: MSE: 2.727843, MAE: 1.2516261\n",
      "Epoch 471: MSE: 2.2502532, MAE: 1.1339452\n",
      "Epoch 472: MSE: 1.8552437, MAE: 1.0915734\n",
      "Epoch 473: MSE: 2.5728288, MAE: 1.1573467\n",
      "Epoch 474: MSE: 2.1418164, MAE: 1.1850924\n",
      "Epoch 475: MSE: 2.3223162, MAE: 1.2620361\n",
      "Epoch 476: MSE: 2.5905027, MAE: 1.2724149\n",
      "Epoch 477: MSE: 2.0422602, MAE: 1.0796535\n",
      "Epoch 478: MSE: 1.8433131, MAE: 1.0424184\n",
      "Epoch 479: MSE: 2.0438006, MAE: 1.1911077\n",
      "Epoch 480: MSE: 2.740158, MAE: 1.1513758\n",
      "Epoch 481: MSE: 2.2291179, MAE: 1.2721596\n",
      "Epoch 482: MSE: 1.6630563, MAE: 1.020135\n",
      "Epoch 483: MSE: 2.961907, MAE: 1.3054149\n",
      "Epoch 484: MSE: 2.324798, MAE: 1.1442084\n",
      "Epoch 485: MSE: 1.7195435, MAE: 1.0172362\n",
      "Epoch 486: MSE: 2.5619504, MAE: 1.1878339\n",
      "Epoch 487: MSE: 2.770875, MAE: 1.3227804\n",
      "Epoch 488: MSE: 1.6091087, MAE: 0.87132573\n",
      "Epoch 489: MSE: 2.6171663, MAE: 1.3811057\n",
      "Epoch 490: MSE: 1.6727164, MAE: 0.9029334\n",
      "Epoch 491: MSE: 2.8333833, MAE: 1.5248083\n",
      "Epoch 492: MSE: 2.4941328, MAE: 1.0044955\n",
      "Epoch 493: MSE: 1.8135763, MAE: 1.0465429\n",
      "Epoch 494: MSE: 2.1874382, MAE: 1.1757872\n",
      "Epoch 495: MSE: 2.7981782, MAE: 1.2715566\n",
      "Epoch 496: MSE: 2.338319, MAE: 1.0475922\n",
      "Epoch 497: MSE: 2.2057297, MAE: 1.1175641\n",
      "Epoch 498: MSE: 1.685358, MAE: 0.96694344\n",
      "Epoch 499: MSE: 2.4299467, MAE: 1.4175217\n",
      "Epoch 500: MSE: 2.0229146, MAE: 1.1723006\n"
     ]
    }
   ],
   "source": [
    "for epoch in 1...epochCount {\n",
    "    var epochLoss: Float = 0\n",
    "    var epochMAE: Float = 0\n",
    "    var batchCount: Int = 0\n",
    "    var batchArray = Array(repeating: false, count: numberOfBatch)\n",
    "    for batch in 0..<numberOfBatch {\n",
    "        var r = batch\n",
    "        if shuffle {\n",
    "            while true {\n",
    "                r = Int.random(in: 0..<numberOfBatch)\n",
    "                if !batchArray[r] {\n",
    "                    batchArray[r] = true\n",
    "                    break\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        let batchStart = r * batchSize\n",
    "        let batchEnd = min(numTrainRecords, batchStart + batchSize)\n",
    "        let (loss, grad) = model.valueWithGradient { (model: RegressionModel) -> Tensor<Float> in\n",
    "            let multiInput = MultiInputs(numerical: [XNumericalTrain[batchStart..<batchEnd]],\n",
    "                                         categorical: [XCategoricalTrain[0][batchStart..<batchEnd],\n",
    "                                                       XCategoricalTrain[1][batchStart..<batchEnd]])\n",
    "            let logits = model(multiInput)\n",
    "            return meanSquaredError(predicted: logits, expected: YTrain[batchStart..<batchEnd])\n",
    "        }\n",
    "        optimizer.update(&model, along: grad)\n",
    "        \n",
    "        let multiInput = MultiInputs(numerical: [XNumericalTrain[batchStart..<batchEnd]],\n",
    "                                     categorical: [XCategoricalTrain[0][batchStart..<batchEnd],\n",
    "                                                   XCategoricalTrain[1][batchStart..<batchEnd]])\n",
    "        let logits = model(multiInput)\n",
    "        epochMAE += mae(predictions: logits, truths: YTrain[batchStart..<batchEnd])\n",
    "        epochLoss += loss.scalarized()\n",
    "        batchCount += 1\n",
    "    }\n",
    "    epochMAE /= Float(batchCount)\n",
    "    epochLoss /= Float(batchCount)\n",
    "\n",
    "    print(\"Epoch \\(epoch): MSE: \\(epochLoss), MAE: \\(epochMAE)\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.44593185, MAE: 0.046968672\r\n"
     ]
    }
   ],
   "source": [
    "Context.local.learningPhase = .inference\n",
    "\n",
    "let multiInputTest = MultiInputs(numerical: [XNumericalTest],\n",
    "                                 categorical: [XCategoricalTest[0],\n",
    "                                               XCategoricalTest[1]])\n",
    "\n",
    "let prediction = model(multiInputTest)\n",
    "\n",
    "let predictionMse = meanSquaredError(predicted: prediction, expected: YTest).scalarized()/Float(numTestRecords)\n",
    "let predictionMae = mae(predictions: prediction, truths: YTest)/Float(numTestRecords)\n",
    "\n",
    "print(\"MSE: \\(predictionMse), MAE: \\(predictionMae)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fkf29HLlohIP"
   },
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "// print(model.layer1.weight.shape, model.layer2.weight.shape, model.layer3.weight.shape)\n",
    "// print(model.layer1.bias.shape, model.layer2.bias.shape, model.layer3.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "// let coremlModel = Model(version: 4,\n",
    "//                         shortDescription: \"Regression\",\n",
    "//                         author: \"Jacopo Mangiavacchi\",\n",
    "//                         license: \"MIT\",\n",
    "//                         userDefined: [\"SwiftCoremltoolsVersion\" : \"0.0.3\"]) {\n",
    "//     Input(name: \"input\", shape: [13])\n",
    "//     Output(name: \"output\", shape: [1])\n",
    "//     NeuralNetwork {\n",
    "//         InnerProduct(name: \"dense1\",\n",
    "//                      input: [\"input\"],\n",
    "//                      output: [\"outDense1\"],\n",
    "//                      weight: model.layer1.weight.transposed().flattened().scalars,\n",
    "//                      bias: model.layer1.bias.flattened().scalars,\n",
    "//                      inputChannels: 13,\n",
    "//                      outputChannels: 64)\n",
    "//         ReLu(name: \"Relu1\",\n",
    "//              input: [\"outDense1\"],\n",
    "//              output: [\"outRelu1\"])\n",
    "//         InnerProduct(name: \"dense2\",\n",
    "//                      input: [\"outRelu1\"],\n",
    "//                      output: [\"outDense2\"],\n",
    "//                      weight: model.layer2.weight.transposed().flattened().scalars,\n",
    "//                      bias: model.layer2.bias.flattened().scalars,\n",
    "//                      inputChannels: 64,\n",
    "//                      outputChannels: 32)\n",
    "//         ReLu(name: \"Relu2\",\n",
    "//              input: [\"outDense2\"],\n",
    "//              output: [\"outRelu2\"])\n",
    "//         InnerProduct(name: \"dense3\",\n",
    "//                      input: [\"outRelu2\"],\n",
    "//                      output: [\"output\"],\n",
    "//                      weight: model.layer3.weight.transposed().flattened().scalars,\n",
    "//                      bias: model.layer3.bias.flattened().scalars,\n",
    "//                      inputChannels: 32,\n",
    "//                      outputChannels: 1)\n",
    "//     }\n",
    "// }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "// let coreMLData = coremlModel.coreMLData\n",
    "// try! coreMLData!.write(to: URL(fileURLWithPath: \"../model/s4tf_train_model.mlmodel\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "S4TF_House.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
